{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join('/home/ring/sumo-svn/', 'tools'))\n",
    "import traci\n",
    "import traci.constants as tc\n",
    "import numpy as np\n",
    "from sumolib import checkBinary\n",
    "import datetime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import optparse\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkA1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkA1, self).__init__()\n",
    "        # [50, 60, 100, 0, .... , 20, 8]\n",
    "        self.fc_layer1 = nn.Linear(10, 15) ##CHANGE!! 8\n",
    "        self.fc_layer2 = nn.Linear(15, 20)\n",
    "        self.fc_layer3 = nn.Linear(20, 2) ##CHANGE!! 2\n",
    "    def forward(self, x):\n",
    "        # x is input\n",
    "        # Layer 1\n",
    "        x = F.relu(self.fc_layer1(x))\n",
    "        # Layer 2\n",
    "        x = F.relu(self.fc_layer2(x))\n",
    "        # Layer 3\n",
    "        x = self.fc_layer3(x)\n",
    "        return x\n",
    "model1 = NetworkA1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkA2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkA2, self).__init__()\n",
    "        # [50, 60, 100, 0, .... , 20, 8]\n",
    "        self.fc_layer1 = nn.Linear(11, 15) ##CHANGE!! 6\n",
    "        self.fc_layer2 = nn.Linear(15, 20)\n",
    "        self.fc_layer3 = nn.Linear(20, 3) ##CHANGE!! 4\n",
    "    def forward(self, x):\n",
    "        # x is input\n",
    "        # Layer 1\n",
    "        x = F.relu(self.fc_layer1(x))\n",
    "        # Layer 2\n",
    "        x = F.relu(self.fc_layer2(x))\n",
    "        # Layer 3\n",
    "        x = self.fc_layer3(x)\n",
    "        return x\n",
    "model2 = NetworkA2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the environment\n",
    "def reset():\n",
    "    sumoBinary = checkBinary('sumo') #-gui\n",
    "    traci.start([sumoBinary, \"-c\", \"map_grid_lock_add2TL_copy.sumocfg\",\n",
    "                             \"--tripinfo-output\", \"tripinfo.xml\", '--start','true','--quit-on-end','true','--time-to-teleport','-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = {'virtual' : '270329335' ,'KasemRat': 'cluster_272448137_272555800_272555808_7660045934_7710268409'}\n",
    "detecters = {'QA' : ['156261350#0'], 'QB' : ['-453962283#3'], 'QC' : ['459551209#0', '459551209#3'], 'QD' : ['27702347#0', '27702347#4', '27702347#6']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumolib\n",
    "import lxml.etree as ET\n",
    "def get_edge_id(POI):\n",
    "    root = ET.Element('additionals')\n",
    "    net = sumolib.net.readNet('map_grid_lock_add2TL_copy_newTLS.net.xml')\n",
    "    output = []\n",
    "    for key in POI:\n",
    "        edgeID = net.getEdge(key)\n",
    "        numOfLane = edgeID.getLaneNumber()\n",
    "        for lane_index in range(numOfLane):\n",
    "            output.append(key+'_'+str(lane_index))\n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state():\n",
    "    occupancy = []\n",
    "    for key in ['QA', 'QB', 'QC', 'QD']:\n",
    "        occupancy.append(sum([traci.lanearea.getLastStepOccupancy(e) for e in get_edge_id(detecters[key])])/len(get_edge_id(detecters[key])))\n",
    "    current_phase = [traci.trafficlight.getPhase(id[key]) for key in ['virtual','KasemRat']]\n",
    "    for i in current_phase:\n",
    "        current_phase_binary = [0, 0, 0]\n",
    "        current_phase_binary[int(i//2)] = 1\n",
    "        occupancy+= current_phase_binary\n",
    "    return occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_memory = []\n",
    "def plot_durations():\n",
    "    print('show')\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(reward_memory, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_junction(current_state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(1)\n",
    "    else:\n",
    "        # Ask model what to do?        \n",
    "        answer = policy_net1(torch.tensor(current_state))\n",
    "        action = torch.argmax(answer)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(action, ID):\n",
    "    w = 0.999\n",
    "    act = [5, 1, 3]\n",
    "    print(act[action])\n",
    "    change_to_phase = (act[action]+1)%6\n",
    "    if change_to_phase == traci.trafficlight.setPhase(id[junction_name[ID]]):\n",
    "        traci.trafficlight.setPhase(id[junction_name[ID]], change_to_phase) # NO YELLOW PHASE \n",
    "    else:\n",
    "        traci.trafficlight.setPhase(id[junction_name[ID]], act[action]) # YELLOW PHASE FIRST\n",
    "    for i in range(5):\n",
    "        traci.simulationStep()\n",
    "#     w1 = sum([traci.lanearea.getLastStepOccupancy(e) for e in get_edge_id(reward_w1)])\n",
    "#     w2 = sum([traci.lanearea.getLastStepOccupancy(e) for e in get_edge_id(reward_w1)])\n",
    "#     reward = -w2*(1-w) - w1*(w)\n",
    "    reward = -sum(get_state())\n",
    "    print('reward', reward)\n",
    "    next_state1 = get_state()\n",
    "    junct = get_action_junction( next_state1)\n",
    "    next_state2 = copy.deepcopy( next_state1)\n",
    "    print('current_state2', next_state2)\n",
    "    next_state2.append(int(junct))\n",
    "    return reward, next_state1, next_state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action_phase(current_state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        action = np.random.randint(2)\n",
    "    else:\n",
    "        # Ask model what to do?        \n",
    "        answer = policy_net2(torch.tensor(current_state))\n",
    "        action = torch.argmax(answer)\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial(state1, state2, max_occupancy):\n",
    "    occupancy = copy.deepcopy(state1[0:4])\n",
    "    print(occupancy)\n",
    "    s1 = copy.deepcopy(state1[4:7])\n",
    "    s2 = copy.deepcopy(state1[7:9])\n",
    "    if state1 == None and state2 == None:\n",
    "        if ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s1[1] != 1:\n",
    "            print(occupancy)\n",
    "            print('set0')\n",
    "            state2 = copy.deepcopy(state1)\n",
    "            state2.append(0)\n",
    "            traci.trafficlight.setPhase(id['virtual'], 2)\n",
    "            junct = 0\n",
    "            action = 1\n",
    "        elif ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s2[1] != 1:\n",
    "            traci.trafficlight.setPhase(id['KasemRat'], 2)\n",
    "            state2 = copy.deepcopy(state1)\n",
    "            state2.append(1)\n",
    "            junct = 1\n",
    "            action = 1\n",
    "        elif occupancy[2] >= max_occupancy:\n",
    "            print(occupancy)\n",
    "            print('set2')\n",
    "            traci.trafficlight.setPhase(id['KasemRat'], 2)\n",
    "            state2 = copy.deepcopy(state1)\n",
    "            state2.append(1)\n",
    "            junct = 1\n",
    "            action = 1\n",
    "        else:\n",
    "            traci.trafficlight.setPhase(id['virtual'], 0)\n",
    "            state2 = copy.deepcopy(state1)\n",
    "            state2.append(0)\n",
    "            junct = 1\n",
    "            action = 0\n",
    "    else:\n",
    "        if ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s1[1] != 1:\n",
    "            print(occupancy)\n",
    "            print('set0')\n",
    "            traci.trafficlight.setPhase(id['virtual'], 2)\n",
    "            junct = 0\n",
    "            action = 1\n",
    "        elif ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s2[1] != 1:\n",
    "            traci.trafficlight.setPhase(id['KasemRat'], 2)\n",
    "            junct = 1\n",
    "            action = 1\n",
    "        elif occupancy[2] >= max_occupancy:\n",
    "            print(occupancy)\n",
    "            print('set2')\n",
    "            traci.trafficlight.setPhase(id['KasemRat'], 2)\n",
    "            junct = 1\n",
    "            action = 1\n",
    "        else:\n",
    "            traci.trafficlight.setPhase(id['virtual'], 0)\n",
    "            junct = 1\n",
    "            action = 0\n",
    "    for i in range(5):\n",
    "        traci.simulationStep()\n",
    "    reward = -sum(get_state())\n",
    "    next_state1 = get_state()\n",
    "    next_state2 = copy.deepcopy(next_state1)\n",
    "    if ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s1[1] != 1:\n",
    "        next_state2.append(0)\n",
    "    elif ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s2[1] != 1:\n",
    "        next_state2.append(1)\n",
    "    elif occupancy[2] >= max_occupancy:\n",
    "        next_state2.append(1)\n",
    "    else:\n",
    "        next_state2.append(0)\n",
    "    \n",
    "    return reward, state1, state2, action, junct, next_state1, next_state2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model1():\n",
    "    if len(memory1) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory1.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.bool)\n",
    "#     print('non_final_mask', non_final_mask)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "#     print('non_final_next_states',  non_final_next_states)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "#     print('state_batch', state_batch)\n",
    "#     print('reward_batch', reward_batch)\n",
    "#     print('action_batch', action_batch)\n",
    "    state_action_values = policy_net1(state_batch).gather(1, action_batch)\n",
    "#     print(state_action_values)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    next_state_values[non_final_mask] = target_net1(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "#     print('state_action_values', state_action_values)\n",
    "#     print('expected_state_action_values ', expected_state_action_values )\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer1.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net1.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_final_mask = torch.tensor([True, True, True, True, True, True, True, True, True, True])\n",
    "# non_final_next_states = torch.tensor([[ 7.1332,  6.4395, 22.7967,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 16.0987, 18.4670,  4.4428,  0.0000,  0.0000,  1.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 14.0199, 18.6225,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 16.0987, 16.0655, 23.2155,  1.0000,  0.0000,  0.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  1.0000],\n",
    "#         [ 9.9864, 15.5202, 20.6725, 27.4631,  1.0000,  0.0000,  0.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  1.0000],\n",
    "#         [ 9.9864, 16.2855, 21.2403, 34.1072,  1.0000,  0.0000,  0.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  1.0000],\n",
    "#         [ 1.8350,  0.0000, 17.5268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 16.0987, 21.6002, 11.9312,  0.0000,  0.0000,  1.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  0.0000],\n",
    "#         [ 0.0000,  0.0000, 16.1663,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 16.0987, 14.5397, 37.0644,  1.0000,  0.0000,  0.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  1.0000]])\n",
    "\n",
    "# state_batch = torch.tensor([[ 1.8350,  0.0000, 17.5268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 14.0199, 18.6225,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 7.1332,  6.4395, 22.7967,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 16.0987, 21.6002, 11.9312,  0.0000,  0.0000,  1.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 16.0987, 16.0655, 23.2155,  1.0000,  0.0000,  0.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  1.0000],\n",
    "#         [ 9.9864, 15.5202, 20.6725, 27.4631,  1.0000,  0.0000,  0.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  1.0000],\n",
    "#         [ 0.0000,  0.0000, 16.1663,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 16.0987, 18.4670,  4.4428,  0.0000,  0.0000,  1.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 0.0000,  0.0000,  8.8183,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
    "#           0.0000,  0.0000,  0.0000],\n",
    "#         [ 9.9864, 16.2855, 21.2403, 34.1072,  1.0000,  0.0000,  0.0000,  0.0000,\n",
    "#           1.0000,  0.0000,  1.0000]])\n",
    "# action_batch = torch.tensor([[0],\n",
    "#         [2],\n",
    "#         [0],\n",
    "#         [0],\n",
    "#         [0],\n",
    "#         [0],\n",
    "#         [0],\n",
    "#         [2],\n",
    "#         [0],\n",
    "#         [0]])\n",
    "# reward_batch = torch.tensor([[0],\n",
    "#         [2],\n",
    "#         [0],\n",
    "#         [0],\n",
    "#         [0],\n",
    "#         [0],\n",
    "#         [0],\n",
    "#         [2],\n",
    "#         [0],\n",
    "#         [0]])\n",
    "# state_action_values = torch.tensor([[ 0.2508],\n",
    "#         [-0.0413],\n",
    "#         [ 0.1092],\n",
    "#         [-0.0687],\n",
    "#         [-0.0691],\n",
    "#         [-0.0643],\n",
    "#         [ 0.2510],\n",
    "#         [-0.0855],\n",
    "#         [ 0.2228],\n",
    "#         [-0.0836]])\n",
    "# state_action_values = torch.tensor([[ 0.2508],\n",
    "#         [-0.0413],\n",
    "#         [ 0.1092],\n",
    "#         [-0.0687],\n",
    "#         [-0.0691],\n",
    "#         [-0.0643],\n",
    "#         [ 0.2510],\n",
    "#         [-0.0855],\n",
    "#         [ 0.2228],\n",
    "#         [-0.0836]])\n",
    "# expected_state_action_values = torch.tensor([[-38.2710, -38.4463, -38.4064, -38.4315, -38.4271, -38.4445, -38.1435,\n",
    "#          -38.4311, -38.1434, -38.4599],\n",
    "#         [-50.8967, -51.0719, -51.0321, -51.0571, -51.0528, -51.0702, -50.7692,\n",
    "#          -51.0568, -50.7691, -51.0856],\n",
    "#         [-44.5306, -44.7058, -44.6659, -44.6910, -44.6867, -44.7040, -44.4031,\n",
    "#          -44.6907, -44.4029, -44.7194],\n",
    "#         [-67.2678, -67.4431, -67.4032, -67.4283, -67.4240, -67.4413, -67.1403,\n",
    "#          -67.4279, -67.1402, -67.4567],\n",
    "#         [-75.5440, -75.7192, -75.6794, -75.7044, -75.7001, -75.7175, -75.4165,\n",
    "#          -75.7041, -75.4163, -75.7328],\n",
    "#         [-83.5211, -83.6964, -83.6565, -83.6816, -83.6773, -83.6946, -83.3937,\n",
    "#          -83.6813, -83.3935, -83.7100],\n",
    "#         [-21.2635, -21.4388, -21.3989, -21.4240, -21.4197, -21.4370, -21.1361,\n",
    "#          -21.4237, -21.1359, -21.4524],\n",
    "#         [-61.5182, -61.6934, -61.6536, -61.6786, -61.6743, -61.6917, -61.3907,\n",
    "#          -61.6783, -61.3905, -61.7070],\n",
    "#         [-18.0681, -18.2433, -18.2035, -18.2285, -18.2242, -18.2416, -17.9406,\n",
    "#          -18.2282, -17.9404, -18.2570],\n",
    "#         [-79.5910, -79.7663, -79.7264, -79.7515, -79.7471, -79.7645, -79.4635,\n",
    "#          -79.7511, -79.4634, -79.7799]])\n",
    "# state_action_values = policy_net2(state_batch).gather(1, action_batch)\n",
    "# next_state_values = torch.zeros(BATCH_SIZE)\n",
    "# print(next_state_values)\n",
    "# next_state_values[non_final_mask] = target_net2(non_final_next_states).max(1)[0].detach()\n",
    "# print(next_state_values[non_final_mask])\n",
    "# print(state_action_values)\n",
    "#     # Compute the expected Q values\n",
    "# expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "# print(expected_state_action_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model2():\n",
    "    if len(memory2) < BATCH_SIZE:\n",
    "        return\n",
    "#     print(len(memory2))\n",
    "    transitions = memory2.sample(BATCH_SIZE)\n",
    "#     print(type(zip(*transitions)))\n",
    "#     print(type(Transition(*zip(*transitions))))\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.bool)\n",
    "#     print('non_final_mask', non_final_mask)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "#     print('non_final_next_states',  non_final_next_states)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "#     print('state_batch', state_batch)\n",
    "#     print('reward_batch', reward_batch)\n",
    "#     print('action_batch', action_batch)\n",
    "    state_action_values = policy_net2(state_batch).gather(1, action_batch)\n",
    "#     print(state_action_values)\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    next_state_values[non_final_mask] = target_net2(non_final_next_states).max(1)[0].detach()\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "#     print('state_action_values', state_action_values)\n",
    "#     print('expected_state_action_values ', expected_state_action_values )\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # Optimize the model\n",
    "    optimizer2.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net2.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "junction_name = ['virtual','KasemRat']\n",
    "\n",
    "time_step = 5\n",
    "reward = 0 \n",
    "epsilon = 0.1 # ดมกาว 10%\n",
    "GAMMA = 0.9  #GAMMA = [e for e in gamma()] ภีมเปลี่ยนเอาเองนะถ้าอยากเปลี่ยน\n",
    "\n",
    "action = 0\n",
    "current_state1 = None\n",
    "previous_state1 = None\n",
    "\n",
    "memory = []\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "start_training_step = 10\n",
    "current_training_step = 0 \n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "# create your optimizer\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "optimizer2 = optim.SGD(model2.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "policy_net1 = model1\n",
    "target_net1 = model1\n",
    "target_net1.load_state_dict(policy_net1.state_dict())\n",
    "target_net1.eval()\n",
    "optimizer1 = optim.RMSprop(policy_net1.parameters())\n",
    "\n",
    "policy_net2 = model2\n",
    "target_net2 = model2\n",
    "target_net2.load_state_dict(policy_net2.state_dict())\n",
    "target_net2.eval()\n",
    "optimizer2 = optim.RMSprop(policy_net2.parameters())\n",
    "\n",
    "memory1 = ReplayMemory(10000)\n",
    "memory2 = ReplayMemory(10000)\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "FatalTraCIError",
     "evalue": "Not connected.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-58920bab68aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraci\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files (x86)\\Eclipse\\Sumo\\tools\\traci\\main.py\u001b[0m in \u001b[0;36mclose\u001b[1;34m(wait)\u001b[0m\n\u001b[0;32m    211\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;34m\"\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_connections\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mFatalTraCIError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Not connected.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[0m_connections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0m_connections\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_currentLabel\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFatalTraCIError\u001b[0m: Not connected."
     ]
    }
   ],
   "source": [
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 8.81831169220976, 0.0]\n",
      "[0.0, 0.0, 16.067782894979423, 0.0]\n",
      "[0.0, 0.0, 16.166341431316717, 0.0]\n",
      "[0.0, 0.0, 17.899592152070632, 0.0]\n",
      "[1.8350440624129567, 0.0, 17.52676374436763, 0.0]\n",
      "[4.279891304347826, 0.0, 21.235858225237195, 0.0]\n",
      "[7.133152173913045, 6.439473134016308, 22.796659128910978, 0.0]\n",
      "[8.559782608695654, 11.409742513476138, 19.994779610745667, 0.0]\n",
      "[9.986413043478262, 14.019908683823, 18.622492776693996, 0.0]\n",
      "[9.986413043478262, 16.09868283504077, 15.397939185622702, 2.221420843851424]\n",
      "[9.986413043478262, 16.09868283504077, 18.467009217365618, 4.442841687702848]\n",
      "[9.986413043478262, 16.09868283504077, 21.93184201676884, 5.629493407041392]\n",
      "[9.986413043478262, 16.09868283504077, 21.60015148394345, 11.931180900189112]\n",
      "[9.986413043478262, 16.09868283504077, 18.457990401523453, 17.8968228386569]\n",
      "[9.986413043478262, 16.09868283504077, 16.065528299267825, 23.215467077912734]\n",
      "[9.986413043478262, 16.09868283504077, 22.47150963582653, 22.61937378798184]\n",
      "[9.986413043478262, 15.52017944879031, 20.672502977532492, 27.463125477734735]\n",
      "[9.986413043478262, 16.605823037394135, 19.532524950155683, 28.710398190855482]\n",
      "[9.986413043478262, 16.285492663682284, 21.2402551233319, 34.10723998423355]\n",
      "[9.986413043478262, 16.09868283504077, 17.768437876203006, 35.8474578378982]\n",
      "[9.986413043478262, 16.09868283504077, 14.539731426318559, 37.06444620324618]\n",
      "[9.986413043478262, 16.09868283504077, 11.93204989966883, 38.491172755680736]\n",
      "[9.986413043478262, 12.878946268032616, 17.356620454688358, 40.21321734959766]\n",
      "[9.986413043478262, 12.878946268032616, 15.577302285317499, 40.29029952560104]\n",
      "[9.986413043478262, 16.09868283504077, 19.572890794953324, 37.76233909510946]\n",
      "[9.986413043478262, 19.31841940204892, 18.91845054724406, 38.38099773283526]\n",
      "[9.986413043478262, 13.872470931825898, 15.429025404406122, 35.78878251735473]\n",
      "[9.986413043478262, 13.52928912751959, 14.052042578822121, 39.02778830760236]\n",
      "[9.986413043478262, 16.09868283504077, 23.200015928828655, 38.30895521061189]\n",
      "[9.986413043478262, 16.09868283504077, 22.67148532712369, 41.23748667497103]\n",
      "[9.986413043478262, 16.09868283504077, 21.820568701690014, 44.61997193910568]\n",
      "[9.986413043478262, 16.09868283504077, 18.78795921613193, 44.54116649464351]\n",
      "[9.986413043478262, 15.523768076757408, 23.119726451617822, 43.72809278255655]\n",
      "[9.986413043478262, 13.897666185652541, 29.837430189035455, 46.77405558449295]\n",
      "[9.986413043478262, 16.09868283504077, 33.663278435406276, 52.251147930771644]\n",
      "[9.986413043478262, 16.09868283504077, 33.663278435406276, 52.251147930771644]\n",
      "set0\n",
      "[9.986413043478262, 16.09868283504077, 33.85302919369258, 49.50776677186244]\n",
      "[9.986413043478262, 12.878946268032616, 38.72212823464995, 47.450787465501286]\n",
      "[9.986413043478262, 9.65920970102446, 40.24532467561006, 47.8139800447386]\n",
      "[11.413043478260871, 3.219736567008154, 46.69392079556764, 51.27216804537838]\n",
      "[11.413043478260871, 3.219736567008154, 46.69392079556764, 51.27216804537838]\n",
      "set0\n",
      "[11.413043478260871, 0.0, 45.6191936101777, 51.02219367905992]\n",
      "[14.266304347826086, 0.0, 43.86697898888295, 56.104492159968686]\n",
      "[17.1195652173913, 0.0, 40.72900166359246, 53.49938190286346]\n",
      "[17.1195652173913, 0.0, 40.72900166359246, 53.49938190286346]\n",
      "set0\n",
      "[19.972826086956523, 3.219736567008154, 36.97475146211381, 56.268573414920546]\n",
      "[21.517589945263563, 6.439473134016308, 35.06216411850468, 61.4666719946305]\n",
      "[21.517589945263563, 6.439473134016308, 35.06216411850468, 61.4666719946305]\n",
      "set0\n",
      "[24.25271739130435, 9.65920970102446, 29.105909026637807, 63.44387035701558]\n",
      "[24.25271739130435, 12.878946268032616, 24.967517021537084, 62.73125663153382]\n",
      "[24.25271739130435, 12.878946268032616, 24.967517021537084, 62.73125663153382]\n",
      "set0\n",
      "[24.25271739130435, 16.09868283504077, 22.270550638336143, 61.65497651330353]\n",
      "[24.25271739130435, 19.31841940204892, 19.909061850380738, 56.39780975985477]\n",
      "[24.25271739130435, 19.31841940204892, 19.909061850380738, 56.39780975985477]\n",
      "set0\n",
      "[25.220545083426078, 16.643185754422106, 18.81599765604799, 51.08340675611746]\n",
      "[24.25271739130435, 16.09868283504077, 16.67041443753709, 47.85522429120264]\n",
      "[24.25271739130435, 16.09868283504077, 13.95509856542081, 45.096854677325744]\n",
      "[24.25271739130435, 16.09868283504077, 22.018086465384105, 44.258564442779935]\n"
     ]
    }
   ],
   "source": [
    "#episode = 0\n",
    "state1 = None\n",
    "state2 = None\n",
    "rewards = 0\n",
    "reset()\n",
    "traci.simulationStep()\n",
    "state1 = get_state()\n",
    "for seconds in range(100):\n",
    "    reward, current_state1, current_state2, action, junct, next_state1, next_state2 = initial(state1, state2, 50)\n",
    "    if state1 is not None and state2 is not None:\n",
    "        memory1.push(torch.tensor([current_state1]), torch.tensor([[junct]]), torch.tensor([next_state1]), torch.tensor([reward]))\n",
    "        memory2.push(torch.tensor([current_state2]), torch.tensor([[action]]), torch.tensor([next_state2]), torch.tensor([reward]))\n",
    "    state1 = next_state1\n",
    "    state2 = next_state2\n",
    "    optimize_model1()\n",
    "    optimize_model2()\n",
    "    rewards += reward\n",
    "reward_memory.append(rewards)\n",
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def run():\n",
    "for episode in range(100):\n",
    "    count = 0\n",
    "    reset()\n",
    "    traci.simulationStep()\n",
    "    previous_phase = [traci.trafficlight.getPhase(id[key]) for key in ['virtual','KasemRat']]\n",
    "#     print(previous_phase)\n",
    "    for seconds in range(100): \n",
    "#         traci.simulationStep()\n",
    "#             if seconds >= 300: ??\n",
    "#         if seconds%time_step == 0:\n",
    "#             if done(count) >= 50:\n",
    "#                 print('done')\n",
    "#                 break\n",
    "        if current_state1 is None:\n",
    "            current_phase = [traci.trafficlight.getPhase(id[key]) for key in ['virtual','KasemRat']]\n",
    "#             print('current_phase', current_phase)\n",
    "            current_state1 = get_state()\n",
    "#             print('current_state1', current_state1)\n",
    "            junct = get_action_junction(current_state1)\n",
    "            current_state2 = copy.deepcopy(current_state1)\n",
    "#             print('current_state2', current_state2)\n",
    "            current_state2.append(int(junct))\n",
    "#             print('current_state2', current_state2)\n",
    "        else:\n",
    "            current_state1 = next_state1\n",
    "            current_state2 = next_state2\n",
    "        action = get_action_phase(current_state2)\n",
    "#         print('junction', junct)\n",
    "#         print('action', action)\n",
    "        reward, next_state1, next_state2 = take_action(action, junct) \n",
    "        if previous_state1 is not None:\n",
    "            memory1.push(torch.tensor([current_state1]), torch.tensor([[junct]]), torch.tensor([next_state1]), torch.tensor([reward]))\n",
    "            memory2.push(torch.tensor([current_state2]), torch.tensor([[action]]), torch.tensor([next_state2]), torch.tensor([reward]))\n",
    "        optimize_model1()\n",
    "        optimize_model2()\n",
    "        previous_state1 = current_state1\n",
    "        previous_state2 = current_state2\n",
    "        rewards += reward\n",
    "    reward_memory.append(rewards)\n",
    "    plot_durations()\n",
    "\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net1.load_state_dict(policy_net1.state_dict())\n",
    "        target_net2.load_state_dict(policy_net2.state_dict())\n",
    "    traci.close()\n",
    "\n",
    "print('Complete')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
