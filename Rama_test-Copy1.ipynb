{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "sys.path.append(os.path.join('/home/ring/sumo-svn/', 'tools'))\n",
    "import traci\n",
    "import traci.constants as tc\n",
    "import numpy as np\n",
    "from sumolib import checkBinary\n",
    "import datetime\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import optparse\n",
    "import pandas as pd\n",
    "import asyncio\n",
    "from collections import namedtuple\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkA1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetworkA1, self).__init__()\n",
    "        # [50, 60, 100, 0, .... , 20, 8]\n",
    "        self.fc_layer1 = nn.Linear(10, 15) ##CHANGE!! 8\n",
    "        self.fc_layer2 = nn.Linear(15, 20)\n",
    "        self.fc_layer3 = nn.Linear(20, 9) ##CHANGE!! 2\n",
    "        # S1 = [12, 13,24 ,56 , 0 1 0, 0 0 1]\n",
    "        # 6= {0,1,2,3,4,5}\n",
    "        # 7-8 \n",
    "    def forward(self, x):\n",
    "        # x is input\n",
    "        # Layer 1\n",
    "        x = F.relu(self.fc_layer1(x))\n",
    "        # Layer 2\n",
    "        x = F.relu(self.fc_layer2(x))\n",
    "        # Layer 3\n",
    "        x = self.fc_layer3(x)\n",
    "        return x\n",
    "model1 = NetworkA1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset the environment\n",
    "def reset():\n",
    "    sumoBinary = checkBinary('sumo') #-gui\n",
    "    traci.start([sumoBinary, \"-c\", \"map_grid_lock_add2TL_copy.sumocfg\",\n",
    "                             \"--tripinfo-output\", \"tripinfo.xml\", '--start','true','--quit-on-end','true','--time-to-teleport','-1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = {'virtual' : '270329335' ,'KasemRat': 'cluster_272448137_272555800_272555808_7660045934_7710268409'}\n",
    "detecters = {'QA' : ['156261350#0'], 'QB' : ['-453962283#3'], 'QC' : ['459551209#0', '459551209#3'], 'QD' : ['27702347#0', '27702347#4', '27702347#6']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sumolib\n",
    "import lxml.etree as ET\n",
    "def get_edge_id(POI):\n",
    "#     root = ET.Element('additionals')\n",
    "#     net = sumolib.net.readNet('map_grid_lock_add2TL_copy_newTLS.net.xml')\n",
    "#     output = []\n",
    "#     for key in POI:\n",
    "#         edgeID = net.getEdge(key)\n",
    "#         numOfLane = edgeID.getLaneNumber()\n",
    "#         for lane_index in range(numOfLane):\n",
    "#             output.append(key+'_'+str(lane_index))\n",
    "#     print(output)\n",
    "    if POI == ['156261350#0']:\n",
    "        output = ['156261350#0_0']\n",
    "    if POI == ['-453962283#3']:\n",
    "        output = ['-453962283#3_0']\n",
    "    if POI == ['459551209#0', '459551209#3']:\n",
    "        output = ['459551209#0_0', '459551209#0_1', '459551209#0_2', '459551209#0_3', '459551209#3_0', '459551209#3_1', '459551209#3_2', '459551209#3_3']\n",
    "    if POI == ['27702347#0', '27702347#4', '27702347#6']:\n",
    "        output = ['27702347#0_0', '27702347#4_0', '27702347#6_0']\n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state():\n",
    "    occupancy = []\n",
    "    for key in ['QA', 'QB', 'QC', 'QD']:\n",
    "        occupancy.append(sum([traci.lanearea.getLastStepOccupancy(e) for e in get_edge_id(detecters[key])])/len(get_edge_id(detecters[key])))\n",
    "    current_phase = [traci.trafficlight.getPhase(id[key]) for key in ['virtual','KasemRat']]\n",
    "    for i in current_phase:\n",
    "        current_phase_binary = [0, 0, 0]\n",
    "        current_phase_binary[int(i//2)] = 1\n",
    "        occupancy+= current_phase_binary\n",
    "    return occupancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_memory = []\n",
    "def plot_durations():\n",
    "    print('show')\n",
    "    plt.figure(2)\n",
    "    plt.clf()\n",
    "    durations_t = torch.tensor(reward_memory, dtype=torch.float)\n",
    "    plt.title('Training...')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(durations_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(durations_t) >= 100:\n",
    "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(action, ID):\n",
    "    if ID in [0,1]:\n",
    "        action = action-6\n",
    "        w = 0.999\n",
    "        act = [5, 1, 3]\n",
    "        print(act[action])\n",
    "        change_to_phase = (act[action]+1)%6\n",
    "        if change_to_phase == traci.trafficlight.getPhase(id[junction_name[ID]]):\n",
    "            traci.trafficlight.setPhase(id[junction_name[ID]], change_to_phase) # NO YELLOW PHASE \n",
    "        else:\n",
    "            traci.trafficlight.setPhase(id[junction_name[ID]], act[action]) # YELLOW PHASE FIRST\n",
    "    for i in range(5):\n",
    "        traci.simulationStep()\n",
    "#     w1 = sum([traci.lanearea.getLastStepOccupancy(e) for e in get_edge_id(reward_w1)])\n",
    "#     w2 = sum([traci.lanearea.getLastStepOccupancy(e) for e in get_edge_id(reward_w1)])\n",
    "#     reward = -w2*(1-w) - w1*(w)\n",
    "    reward = -sum(get_state())\n",
    "    print('reward', reward)\n",
    "    next_state1 = get_state()\n",
    "    junction, action = get_action(next_state1)\n",
    "    return reward, next_state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(current_state):\n",
    "    if np.random.rand() < epsilon:\n",
    "        junction = np.random.randint(5)\n",
    "        action = np.random.randint(2)\n",
    "    else:\n",
    "        # Ask model what to do?        \n",
    "        answer = policy_net1(torch.tensor(current_state))\n",
    "        junction = torch.argmax(answer[0:6])\n",
    "        action = torch.argmax(answer[6:9])\n",
    "    return junction, action+6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial(state1, max_occupancy):\n",
    "    occupancy = copy.deepcopy(state1[0:4])\n",
    "    print(occupancy)\n",
    "    s1 = copy.deepcopy(state1[4:7])\n",
    "    s2 = copy.deepcopy(state1[7:9])\n",
    "    if state1 == None and state2 == None:\n",
    "        if ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s1[1] != 1:\n",
    "            print(occupancy)\n",
    "            print('set0')\n",
    "            traci.trafficlight.setPhase(id['virtual'], 2)\n",
    "            junction = 0\n",
    "            action = 1\n",
    "        elif ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s2[1] != 1:\n",
    "            traci.trafficlight.setPhase(id['KasemRat'], 2)\n",
    "            junction = 1\n",
    "            action = 1\n",
    "        elif occupancy[2] >= max_occupancy:\n",
    "            print(occupancy)\n",
    "            print('set2')\n",
    "            traci.trafficlight.setPhase(id['KasemRat'], 2)\n",
    "            junction = 1\n",
    "            action = 1\n",
    "        else:\n",
    "            traci.trafficlight.setPhase(id['virtual'], 0)\n",
    "            junction = 1\n",
    "            action = 0\n",
    "    else:\n",
    "        if ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s1[1] != 1:\n",
    "            print(occupancy)\n",
    "            print('set0')\n",
    "            traci.trafficlight.setPhase(id['virtual'], 2)\n",
    "            junct = 0\n",
    "            action = 1\n",
    "        elif ((occupancy[0] >= max_occupancy) or (occupancy[3] >= max_occupancy)) and s2[1] != 1:\n",
    "            traci.trafficlight.setPhase(id['KasemRat'], 2)\n",
    "            junct = 1\n",
    "            action = 1\n",
    "        elif occupancy[2] >= max_occupancy:\n",
    "            print(occupancy)\n",
    "            print('set2')\n",
    "            traci.trafficlight.setPhase(id['KasemRat'], 2)\n",
    "            junct = 1\n",
    "            action = 1\n",
    "        else:\n",
    "            traci.trafficlight.setPhase(id['virtual'], 0)\n",
    "            junct = 1\n",
    "            action = 0\n",
    "    for i in range(5):\n",
    "        traci.simulationStep()\n",
    "    reward = -sum(get_state())\n",
    "    next_state1 = get_state()\n",
    "    \n",
    "    return reward, state1, action, junct, next_state1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model1():\n",
    "    if len(memory1) < BATCH_SIZE:\n",
    "        return\n",
    "\n",
    "    transitions = memory1.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    \n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), dtype=torch.bool)\n",
    "    non_final_mask = non_final_mask.reshape(1,-1).t()\n",
    "    non_final_mask = torch.cat((non_final_mask,non_final_mask),1).detach() \n",
    "    \n",
    "    print('non_final_mask', non_final_mask)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    print('non_final_next_states',  non_final_next_states)\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "    print('state_batch', state_batch)\n",
    "    print('reward_batch', reward_batch)\n",
    "    print('action_batch', action_batch)\n",
    "    print('policy_net1(state_batch)', policy_net1(state_batch))\n",
    "    state_action_values = policy_net1(state_batch).gather(1, action_batch)\n",
    "    print('state_action_values', state_action_values)\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE)\n",
    "    A = target_net1(non_final_next_states).detach().numpy()\n",
    "    print('A', A)\n",
    "    J = torch.tensor(A[:,[0,1,2,3,4,5]]).max(1)[0]\n",
    "    print('J', J)\n",
    "    P = torch.tensor(A[:,[6,7,8]]).max(1)[0]\n",
    "    print('P', P)\n",
    "\n",
    "    next_state_values = torch.cat((J.reshape(1,-1).t(),P.reshape(1,-1).t()),1).detach() \n",
    "    \n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "    print('expected_state_action_values ', expected_state_action_values )\n",
    "\n",
    "    # Compute Huber loss\n",
    "    loss = F.smooth_l1_loss(state_action_values, expected_state_action_values) #.unsqueeze(1)\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer1.zero_grad()\n",
    "    loss.backward()\n",
    "    for param in policy_net1.parameters():\n",
    "        param.grad.data.clamp_(-1, 1)\n",
    "    optimizer1.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-100.2180, -129.0053],\n",
      "        [ -63.2683,  -81.6236],\n",
      "        [ -79.6158, -102.7542],\n",
      "        [ -91.3935, -117.7884],\n",
      "        [ -53.0772,  -68.4507]])\n"
     ]
    }
   ],
   "source": [
    "A = target_net1(torch.tensor([[2.8533, 0.0000, 7.0294, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
    "         0.0000],\n",
    "        [0.0000, 0.0000, 5.3600, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
    "         0.0000],\n",
    "        [0.0000, 0.0000, 7.3356, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
    "         0.0000],\n",
    "        [1.4266, 0.0000, 7.3610, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
    "         0.0000],\n",
    "        [0.0000, 0.0000, 4.1284, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
    "         0.0000]])).detach().numpy()\n",
    "J = torch.tensor(A[:,[0,1,2,3,4,5]]).max(1)[0]\n",
    "P = torch.tensor(A[:,[6,7,8]]).max(1)[0]\n",
    "\n",
    "print(torch.cat((J.reshape(1,-1).t(),P.reshape(1,-1).t()),1))\n",
    "\n",
    "non = torch.tensor([[True],\n",
    "        [True],\n",
    "        [True],\n",
    "        [True],\n",
    "        [True]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "junction_name = ['virtual','KasemRat']\n",
    "\n",
    "time_step = 5\n",
    "reward = 0 \n",
    "epsilon = 0.1 # ดมกาว 10%\n",
    "GAMMA = 0.9  #GAMMA = [e for e in gamma()] ภีมเปลี่ยนเอาเองนะถ้าอยากเปลี่ยน\n",
    "\n",
    "action = 0\n",
    "current_state1 = None\n",
    "previous_state1 = None\n",
    "\n",
    "memory = []\n",
    "BATCH_SIZE =5 \n",
    "\n",
    "start_training_step = 10\n",
    "current_training_step = 0 \n",
    "\n",
    "alpha = 0.1\n",
    "\n",
    "# create your optimizer\n",
    "optimizer1 = optim.SGD(model1.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "policy_net1 = model1\n",
    "target_net1 = model1\n",
    "target_net1.load_state_dict(policy_net1.state_dict())\n",
    "target_net1.eval()\n",
    "optimizer1 = optim.RMSprop(policy_net1.parameters())\n",
    "\n",
    "\n",
    "memory1 = ReplayMemory(10000)\n",
    "\n",
    "TARGET_UPDATE = 10\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #episode = 0\n",
    "# state1 = None\n",
    "# rewards = 0\n",
    "# reset()\n",
    "# traci.simulationStep()\n",
    "# state1 = get_state()\n",
    "# for seconds in range(100):\n",
    "#     reward, current_state1, action, junction, next_state1 = initial(state1, 50)\n",
    "#     if state1 is not None :\n",
    "#         memory1.push(torch.tensor([current_state1]), torch.tensor([[junction, action]]), torch.tensor([next_state1]), torch.tensor([reward]))\n",
    "#     state1 = next_state1\n",
    "#     optimize_model1()\n",
    "#     rewards += reward\n",
    "# reward_memory.append(rewards)\n",
    "# traci.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward -3.5828422639440864\n",
      "reward -3.7268138561220767\n",
      "reward -4.687999665149713\n",
      "reward -4.290023652316296\n",
      "reward -6.76724704198075\n",
      "reward -7.868501523473141\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.5828, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-6.7672],\n",
      "        [-4.6880],\n",
      "        [-7.8685],\n",
      "        [-4.2900],\n",
      "        [-3.7268]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-41.6656, -39.8608, -40.8355, -37.8652, -38.5114, -59.4149, -51.5040,\n",
      "         -48.7878, -49.6561],\n",
      "        [-36.4722, -34.9396, -35.7467, -33.2049, -33.7683, -52.0686, -45.0779,\n",
      "         -42.7638, -43.4767],\n",
      "        [-64.0253, -60.9964, -62.7953, -58.1264, -59.0575, -91.1709, -79.1900,\n",
      "         -74.7876, -76.3663],\n",
      "        [-45.3355, -43.3383, -44.4314, -41.1584, -41.8630, -64.6059, -56.0448,\n",
      "         -53.0445, -54.0226],\n",
      "        [-35.1446, -33.6816, -34.4459, -32.0135, -32.5559, -50.1908, -43.4352,\n",
      "         -41.2239, -41.8971]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-37.8652, -48.7878],\n",
      "        [-33.2049, -42.7638],\n",
      "        [-58.1264, -74.7876],\n",
      "        [-41.1584, -53.0445],\n",
      "        [-32.0135, -41.2239]], grad_fn=<GatherBackward>)\n",
      "A [[ -64.02534   -60.996418  -62.79535   -58.126392  -59.057533  -91.17088\n",
      "   -79.19      -74.787575  -76.36627 ]\n",
      " [ -45.335476  -43.338272  -44.431377  -41.158367  -41.86299   -64.605896\n",
      "   -56.04481   -53.044464  -54.022644]\n",
      " [ -72.58013   -69.06487   -71.22202   -65.962975  -66.94304  -103.392944\n",
      "   -89.7766    -84.76442   -86.58518 ]\n",
      " [ -41.665646  -39.860825  -40.83553   -37.865242  -38.511425  -59.414894\n",
      "   -51.503998  -48.7878    -49.656128]\n",
      " [ -36.472164  -34.93959   -35.74674   -33.20486   -33.76834   -52.06865\n",
      "   -45.077904  -42.763836  -43.476692]]\n",
      "J tensor([-58.1264, -41.1584, -65.9630, -37.8652, -33.2049])\n",
      "P tensor([-74.7876, -53.0445, -84.7644, -48.7878, -42.7638])\n",
      "expected_state_action_values  tensor([[-59.0810, -74.0761],\n",
      "        [-41.7305, -52.4280],\n",
      "        [-67.2352, -84.1565],\n",
      "        [-38.3687, -48.1990],\n",
      "        [-33.6112, -42.2143]])\n",
      "reward -10.700904491424406\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.5828, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-10.7009],\n",
      "        [ -7.8685],\n",
      "        [ -6.7672],\n",
      "        [ -4.6880],\n",
      "        [ -3.7268]])\n",
      "action_batch tensor([[4, 8],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[ -97.1272,  -92.6084,  -95.5067, -115.3725,  -89.5809, -138.1928,\n",
      "         -120.5294, -141.0373, -115.6770],\n",
      "        [ -88.2837,  -84.2438,  -86.7847, -104.8090,  -81.4333, -125.5734,\n",
      "         -109.5539, -128.2026, -105.1490],\n",
      "        [ -57.8565,  -55.3741,  -56.8454,  -68.7362,  -53.4463,  -82.3780,\n",
      "          -71.7659,  -84.1451,  -68.8714],\n",
      "        [ -50.8262,  -48.6920,  -49.9398,  -60.4519,  -47.0091,  -72.4273,\n",
      "          -63.0398,  -73.9856,  -60.5137],\n",
      "        [ -49.0291,  -46.9839,  -48.1746,  -58.3342,  -45.3636,  -69.8836,\n",
      "          -60.8092,  -71.3886,  -58.3773]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -89.5809, -115.6770],\n",
      "        [-104.8090, -128.2026],\n",
      "        [ -68.7362,  -84.1451],\n",
      "        [ -60.4519,  -73.9856],\n",
      "        [ -58.3342,  -71.3886]], grad_fn=<GatherBackward>)\n",
      "A [[-127.89015  -121.76284  -125.80529  -151.96002  -117.917114 -181.94888\n",
      "  -158.7331   -185.62598  -152.36598 ]\n",
      " [ -97.12722   -92.60835   -95.50666  -115.37254   -89.58086  -138.19283\n",
      "  -120.52942  -141.0373   -115.67699 ]\n",
      " [ -88.28374   -84.243805  -86.78467  -104.80899   -81.43326  -125.57343\n",
      "  -109.55388  -128.20264  -105.14898 ]\n",
      " [ -62.824253  -60.095863  -61.72499   -74.59013   -57.99497   -89.409386\n",
      "   -77.93197   -91.3239    -74.77713 ]\n",
      " [ -50.826225  -48.692     -49.93984   -60.451893  -47.00912   -72.42726\n",
      "   -63.039803  -73.98565   -60.513714]]\n",
      "J tensor([-117.9171,  -89.5809,  -81.4333,  -57.9950,  -47.0091])\n",
      "P tensor([-152.3660, -115.6770, -105.1490,  -74.7771,  -60.5137])\n",
      "expected_state_action_values  tensor([[-116.8263, -147.8303],\n",
      "        [ -88.4913, -111.9778],\n",
      "        [ -80.0572, -101.4013],\n",
      "        [ -56.8835,  -71.9874],\n",
      "        [ -46.0350,  -58.1892]])\n",
      "5\n",
      "reward -10.760997101723085\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -6.7672],\n",
      "        [ -4.2900],\n",
      "        [-10.7009],\n",
      "        [ -7.8685],\n",
      "        [-10.7610]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [3, 7],\n",
      "        [4, 8],\n",
      "        [3, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -49.4682,  -47.6947,  -48.6961,  -49.0912,  -60.3007,  -70.4391,\n",
      "          -61.5070,  -62.2365,  -73.6671],\n",
      "        [ -53.7237,  -51.7654,  -52.8843,  -53.2698,  -65.4428,  -76.4571,\n",
      "          -66.8027,  -67.5478,  -79.9919],\n",
      "        [ -88.2578,  -84.7208,  -86.9788,  -87.5774, -107.4429, -125.5577,\n",
      "         -109.8076, -110.7975, -131.5406],\n",
      "        [ -76.3253,  -73.3388,  -75.1831,  -75.6741,  -92.9120, -108.5507,\n",
      "          -94.9519,  -95.8337, -113.7240],\n",
      "        [-119.0439, -114.0962, -117.3762, -118.1741, -144.8958, -169.3364,\n",
      "         -148.1473, -149.3576, -177.5055]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -49.0912,  -62.2365],\n",
      "        [ -53.2698,  -67.5478],\n",
      "        [-107.4429, -131.5406],\n",
      "        [ -75.6741,  -95.8337],\n",
      "        [-114.0962, -148.1473]], grad_fn=<GatherBackward>)\n",
      "A [[ -76.325325  -73.33878   -75.18314   -75.67413   -92.91196  -108.55072\n",
      "   -94.95194   -95.83375  -113.72403 ]\n",
      " [ -49.468197  -47.694736  -48.6961    -49.09119   -60.300697  -70.4391\n",
      "   -61.507004  -62.23649   -73.6671  ]\n",
      " [-119.043915 -114.096245 -117.376175 -118.17412  -144.89575  -169.33638\n",
      "  -148.14734  -149.35764  -177.50548 ]\n",
      " [ -88.257805  -84.72076   -86.97881   -87.577385 -107.442924 -125.55769\n",
      "  -109.80759  -110.79746  -131.54065 ]\n",
      " [-119.68648  -114.7109   -118.008575 -118.80508  -145.6722   -170.24507\n",
      "  -148.94696  -150.15962  -178.4605  ]]\n",
      "J tensor([ -73.3388,  -47.6947, -114.0962,  -84.7208, -114.7109])\n",
      "P tensor([ -94.9519,  -61.5070, -148.1473, -109.8076, -148.9470])\n",
      "expected_state_action_values  tensor([[ -72.7722,  -92.2240],\n",
      "        [ -47.2153,  -59.6463],\n",
      "        [-113.3875, -144.0335],\n",
      "        [ -84.1172, -106.6953],\n",
      "        [-114.0008, -144.8133]])\n",
      "5\n",
      "reward -11.881967070464844\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -4.6880],\n",
      "        [ -7.8685],\n",
      "        [ -4.2900],\n",
      "        [-10.7009],\n",
      "        [ -6.7672]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [4, 8],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[ -51.1628,  -35.0882,  -50.6443,  -59.8695,  -73.4982,  -73.1452,\n",
      "          -49.1489,  -73.2503,  -87.4742],\n",
      "        [ -90.1875,  -61.5535,  -89.3259, -105.3776, -129.3130, -128.7086,\n",
      "          -86.7301, -128.8380, -154.2258],\n",
      "        [ -63.2431,  -43.3008,  -62.5999,  -73.8831,  -90.7175,  -90.3023,\n",
      "          -60.7820,  -90.4273, -108.0841],\n",
      "        [-102.6737,  -69.9948, -101.7353, -120.0599, -147.2381, -146.5732,\n",
      "          -98.7396, -146.6753, -175.6277],\n",
      "        [ -58.2413,  -39.9004,  -57.6498,  -68.0808,  -83.5879,  -83.1985,\n",
      "          -55.9654,  -83.3152,  -99.5506]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -59.8695,  -73.2503],\n",
      "        [-105.3776, -128.8380],\n",
      "        [ -73.8831,  -90.4273],\n",
      "        [-147.2381, -175.6277],\n",
      "        [ -68.0808,  -83.3152]], grad_fn=<GatherBackward>)\n",
      "A [[ -63.24308   -43.300762  -62.599926  -73.88309   -90.71754   -90.30232\n",
      "   -60.782     -90.42729  -108.084076]\n",
      " [-102.67369   -69.99482  -101.735344 -120.05985  -147.23808  -146.57324\n",
      "   -98.73964  -146.67531  -175.62775 ]\n",
      " [ -58.241295  -39.900368  -57.649757  -68.08082   -83.58792   -83.19848\n",
      "   -55.96536   -83.31519   -99.55062 ]\n",
      " [-137.9518    -93.87363  -136.75253  -161.38708  -197.81873  -196.92503\n",
      "  -132.70087  -197.00519  -236.07057 ]\n",
      " [ -90.1875    -61.5535    -89.325874 -105.377594 -129.31297  -128.70862\n",
      "   -86.730095 -128.83795  -154.22575 ]]\n",
      "J tensor([-43.3008, -69.9948, -39.9004, -93.8736, -61.5535])\n",
      "P tensor([ -60.7820,  -98.7396,  -55.9654, -132.7009,  -86.7301])\n",
      "expected_state_action_values  tensor([[ -43.6587,  -59.3918],\n",
      "        [ -70.8638,  -96.7342],\n",
      "        [ -40.2004,  -54.6588],\n",
      "        [ -95.1872, -130.1317],\n",
      "        [ -62.1654,  -84.8243]])\n",
      "5\n",
      "reward -19.227034259031687\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-19.2270],\n",
      "        [ -4.2900],\n",
      "        [-10.7009],\n",
      "        [ -6.7672],\n",
      "        [-11.8820]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [3, 7],\n",
      "        [4, 8],\n",
      "        [3, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-129.6467,  -88.4177, -128.6596, -134.8920, -165.4401, -185.8483,\n",
      "         -124.1326, -166.9825, -202.2877],\n",
      "        [ -50.8559,  -34.9559,  -50.3564,  -52.7026,  -64.8692,  -72.8936,\n",
      "          -48.6482,  -65.5978,  -79.0812],\n",
      "        [ -81.6980,  -55.8604,  -81.0001,  -84.7998, -104.1779, -117.0770,\n",
      "          -78.2086, -105.2537, -127.2035],\n",
      "        [ -46.7202,  -32.1378,  -46.2613,  -48.4517,  -59.6347,  -67.0034,\n",
      "          -44.6822,  -60.3001,  -72.6594],\n",
      "        [-109.8929,  -74.9687, -109.0146, -114.1469, -140.1187, -157.4698,\n",
      "         -105.2330, -141.5077, -171.2021]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -88.4177, -124.1326],\n",
      "        [ -52.7026,  -65.5978],\n",
      "        [-104.1779, -127.2035],\n",
      "        [ -48.4517,  -60.3001],\n",
      "        [ -74.9687, -105.2330]], grad_fn=<GatherBackward>)\n",
      "A [[-197.29279  -134.40703  -195.89455  -205.25357  -251.66708  -282.72177\n",
      "  -188.97842  -253.99066  -307.8679  ]\n",
      " [ -46.720165  -32.137764  -46.261337  -48.45167   -59.634693  -67.003365\n",
      "   -44.682236  -60.300083  -72.65936 ]\n",
      " [-109.268425  -74.5432   -108.39629  -113.50503  -139.32826  -156.58037\n",
      "  -104.63413  -140.70773  -170.23239 ]\n",
      " [ -72.359505  -49.54336   -71.704704  -75.02312   -92.25133  -103.65232\n",
      "   -69.26854   -93.22352  -112.622154]\n",
      " [-129.64673   -88.41775  -128.65959  -134.89203  -165.44012  -185.84825\n",
      "  -124.13263  -166.98248  -202.28767 ]]\n",
      "J tensor([-134.4070,  -32.1378,  -74.5432,  -49.5434,  -88.4177])\n",
      "P tensor([-188.9784,  -44.6822, -104.6341,  -69.2685, -124.1326])\n",
      "expected_state_action_values  tensor([[-140.1934, -189.3076],\n",
      "        [ -33.2140,  -44.5040],\n",
      "        [ -77.7898, -104.8716],\n",
      "        [ -51.3563,  -69.1089],\n",
      "        [ -91.4579, -123.6013]])\n",
      "5\n",
      "reward -14.016359617389949\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 1.5828, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -3.7268],\n",
      "        [-19.2270],\n",
      "        [-14.0164],\n",
      "        [-11.8820],\n",
      "        [-10.7610]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -41.1848,  -38.8903,  -40.6024,  -40.6333,  -46.0773,  -58.6274,\n",
      "          -47.8586,  -51.7426,  -57.3885],\n",
      "        [-145.4437, -136.3036, -143.7006, -143.6497, -162.3861, -206.6500,\n",
      "         -169.1838, -182.0554, -203.3331],\n",
      "        [-221.1109, -207.0627, -218.5621, -218.3450, -246.7673, -314.0569,\n",
      "         -257.2670, -276.6429, -309.1424],\n",
      "        [-121.3581, -113.7465, -119.8587, -119.6736, -135.3855, -172.3703,\n",
      "         -141.1726, -151.8794, -169.3957],\n",
      "        [-120.7046, -113.1352, -119.2145, -119.0358, -134.6618, -171.4474,\n",
      "         -140.4123, -151.0657, -168.4865]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -40.6333,  -51.7426],\n",
      "        [-136.3036, -169.1838],\n",
      "        [-207.0627, -257.2670],\n",
      "        [-113.7465, -141.1726],\n",
      "        [-113.1352, -140.4123]], grad_fn=<GatherBackward>)\n",
      "A [[ -42.750504  -40.354805  -42.145943  -42.161232  -47.81104   -60.83841\n",
      "   -49.680206  -53.69228   -59.566975]\n",
      " [-221.11087  -207.06265  -218.56209  -218.34497  -246.76726  -314.05685\n",
      "  -257.26703  -276.64294  -309.1424  ]\n",
      " [-163.06355  -152.86708  -161.19855  -161.17976  -182.16586  -231.73544\n",
      "  -189.71729  -204.1693   -228.10968 ]\n",
      " [-145.44368  -136.3036   -143.70058  -143.64975  -162.38612  -206.65004\n",
      "  -169.1838   -182.05537  -203.33311 ]\n",
      " [-121.35809  -113.74647  -119.8587   -119.6736   -135.38548  -172.37027\n",
      "  -141.17262  -151.87943  -169.39572 ]]\n",
      "J tensor([ -40.3548, -207.0627, -152.8671, -136.3036, -113.7465])\n",
      "P tensor([ -49.6802, -257.2670, -189.7173, -169.1838, -141.1726])\n",
      "expected_state_action_values  tensor([[ -40.0461,  -48.4390],\n",
      "        [-205.5834, -250.7673],\n",
      "        [-151.5967, -184.7619],\n",
      "        [-134.5552, -164.1474],\n",
      "        [-113.1328, -137.8164]])\n",
      "reward -19.32986484994728\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -4.2900],\n",
      "        [-11.8820],\n",
      "        [ -4.6880],\n",
      "        [ -6.7672],\n",
      "        [-19.2270]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [1, 6],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -50.2129,  -56.9387,  -49.5107,  -43.1386,  -56.1127,  -71.3988,\n",
      "          -50.0270,  -56.5875,  -70.1199],\n",
      "        [-114.2738, -129.0058, -112.8919,  -98.3703, -127.6255, -162.4183,\n",
      "         -113.8790, -128.4695, -159.9269],\n",
      "        [ -40.4495,  -45.9434,  -39.8841,  -34.8367,  -45.2939,  -57.6065,\n",
      "          -40.2928,  -45.6722,  -56.5017],\n",
      "        [ -46.1704,  -52.3861,  -45.5249,  -39.7012,  -51.6332,  -65.6882,\n",
      "          -45.9966,  -52.0680,  -64.4813],\n",
      "        [-137.8763, -155.6447, -136.2649, -118.9029, -154.1188, -196.0345,\n",
      "         -137.3739, -155.0259, -193.2828]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -43.1386,  -56.5875],\n",
      "        [-129.0058, -113.8790],\n",
      "        [ -34.8367,  -45.6722],\n",
      "        [ -39.7012,  -52.0680],\n",
      "        [-155.6447, -137.3739]], grad_fn=<GatherBackward>)\n",
      "A [[ -46.170376  -52.3861    -45.52487   -39.701202  -51.633236  -65.68817\n",
      "   -45.996555  -52.06804   -64.48129 ]\n",
      " [-137.87625  -155.6447   -136.26494  -118.90295  -154.11882  -196.03448\n",
      "  -137.3739   -155.0259   -193.28279 ]\n",
      " [ -50.212883  -56.93867   -49.51074   -43.138596  -56.112736  -71.39885\n",
      "   -50.026962  -56.587486  -70.119865]\n",
      " [ -73.23803   -82.8439    -72.27545   -62.938076  -81.79596  -104.06902\n",
      "   -72.986244  -82.40497  -102.39131 ]\n",
      " [-206.86134  -233.37575  -204.54091  -178.3447   -231.12967  -294.02197\n",
      "  -206.1411   -232.45944  -290.0064  ]]\n",
      "J tensor([ -39.7012, -118.9029,  -43.1386,  -62.9381, -178.3447])\n",
      "P tensor([ -45.9966, -137.3739,  -50.0270,  -72.9862, -206.1411])\n",
      "expected_state_action_values  tensor([[ -40.0211,  -45.6869],\n",
      "        [-118.8946, -135.5185],\n",
      "        [ -43.5127,  -49.7123],\n",
      "        [ -63.4115,  -72.4549],\n",
      "        [-179.7373, -204.7540]])\n",
      "reward -18.96337005029194\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-11.8820],\n",
      "        [-10.7009],\n",
      "        [-18.9634],\n",
      "        [ -7.8685],\n",
      "        [ -4.2900]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [4, 8],\n",
      "        [3, 6],\n",
      "        [3, 7],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-132.0658, -169.3184, -130.3763, -127.9440, -146.9466, -187.1541,\n",
      "         -151.0424, -163.6474, -183.7451],\n",
      "        [ -96.3913, -123.6863,  -95.1109,  -93.3302, -107.2621, -136.6115,\n",
      "         -110.2380, -119.5000, -134.0298],\n",
      "        [-206.5321, -265.2776, -204.4099, -201.2200, -230.6966, -293.2186,\n",
      "         -236.3123, -256.4016, -288.5579],\n",
      "        [ -84.0493, -107.8982,  -82.8943,  -81.3021,  -93.5148, -119.0812,\n",
      "          -96.1280, -104.2059, -116.8262],\n",
      "        [ -57.4415,  -73.8643,  -56.6089,  -55.5435,  -63.9405,  -81.4303,\n",
      "          -65.6891,  -71.3014,  -79.7575]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-169.3184, -151.0424],\n",
      "        [-107.2621, -134.0298],\n",
      "        [-201.2200, -236.3123],\n",
      "        [ -81.3021, -104.2059],\n",
      "        [ -55.5435,  -71.3014]], grad_fn=<GatherBackward>)\n",
      "A [[-159.69707  -204.7581   -157.71034  -154.93575  -177.82489  -226.37898\n",
      "  -182.62311  -197.91724  -222.51541 ]\n",
      " [-131.36578  -168.42264  -129.6864   -127.272415 -146.173    -186.16733\n",
      "  -150.24196  -162.78397  -182.77397 ]\n",
      " [-210.1353   -269.76163  -207.89413  -204.55728  -234.54007  -298.2416\n",
      "  -240.40186  -260.75818  -293.40475 ]\n",
      " [ -96.39127  -123.686325  -95.11088   -93.330154 -107.262085 -136.61145\n",
      "  -110.23797  -119.50003  -134.02979 ]\n",
      " [ -52.805195  -67.93208   -52.039757  -51.09583   -58.817234  -74.89505\n",
      "   -60.388172  -65.583145  -73.325935]]\n",
      "J tensor([-154.9357, -127.2724, -204.5573,  -93.3302,  -51.0958])\n",
      "P tensor([-182.6231, -150.2420, -240.4019, -110.2380,  -60.3882])\n",
      "expected_state_action_values  tensor([[-151.3241, -176.2428],\n",
      "        [-125.2461, -145.9187],\n",
      "        [-203.0649, -235.3250],\n",
      "        [ -91.8656, -107.0827],\n",
      "        [ -50.2763,  -58.6394]])\n",
      "1\n",
      "reward -21.431573791659392\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-10.7610],\n",
      "        [ -4.2900],\n",
      "        [-11.8820],\n",
      "        [-18.9634],\n",
      "        [ -7.8685]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [3, 7],\n",
      "        [1, 6],\n",
      "        [3, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-142.5416, -163.3589, -141.3330, -156.1045, -178.8957, -203.3128,\n",
      "         -144.6185, -193.1855, -218.9827],\n",
      "        [ -62.4009,  -71.7349,  -61.7670,  -68.1848,  -78.3198,  -89.0130,\n",
      "          -63.3361,  -84.6587,  -95.6771],\n",
      "        [-143.3082, -164.2359, -142.0918, -156.9369, -179.8519, -204.4007,\n",
      "         -145.3962, -194.2202, -220.1570],\n",
      "        [-217.4181, -249.6301, -216.1199, -239.5000, -273.9314, -310.7582,\n",
      "         -220.5607, -295.2920, -335.4250],\n",
      "        [ -91.4169, -104.9101,  -90.5554,  -99.9485, -114.7026, -130.3451,\n",
      "          -92.7742, -123.9235, -140.3028]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-163.3589, -144.6185],\n",
      "        [ -68.1848,  -84.6587],\n",
      "        [-164.2359, -145.3962],\n",
      "        [-239.5000, -220.5607],\n",
      "        [ -99.9485, -123.9235]], grad_fn=<GatherBackward>)\n",
      "A [[-143.30817  -164.23593  -142.09184  -156.93694  -179.85188  -204.40074\n",
      "  -145.39618  -194.2202   -220.15698 ]\n",
      " [ -57.32424   -65.92639   -56.741447  -62.67187   -71.98715   -81.8079\n",
      "   -58.185993  -77.80643   -87.90007 ]\n",
      " [-172.14638  -197.29395  -170.74269  -188.77515  -216.19397  -245.6145\n",
      "  -174.60982  -233.35518  -264.80606 ]\n",
      " [-222.3462   -255.14635  -220.93016  -244.70685  -279.92453  -317.68808\n",
      "  -225.54451  -301.8441   -342.79837 ]\n",
      " [-104.62233  -120.00703  -103.68399  -114.49767  -131.30133  -149.22731\n",
      "  -106.15848  -141.83249  -160.6334  ]]\n",
      "J tensor([-142.0918,  -56.7414, -170.7427, -220.9302, -103.6840])\n",
      "P tensor([-145.3962,  -58.1860, -174.6098, -225.5445, -106.1585])\n",
      "expected_state_action_values  tensor([[-138.6436, -141.6176],\n",
      "        [ -55.3573,  -56.6574],\n",
      "        [-165.5504, -169.0308],\n",
      "        [-217.8005, -221.9534],\n",
      "        [-101.1841, -103.4111]])\n",
      "reward -25.6967857520725\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -6.7672],\n",
      "        [-10.7009],\n",
      "        [-18.9634],\n",
      "        [-21.4316],\n",
      "        [-19.3299]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [4, 8],\n",
      "        [3, 6],\n",
      "        [0, 7],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[ -53.1263,  -61.1616,  -52.4325,  -51.9701,  -66.3626,  -75.4548,\n",
      "          -60.3338,  -66.3896,  -80.9197],\n",
      "        [ -98.9262, -113.5559,  -97.7578,  -96.8386, -123.4665, -140.3975,\n",
      "         -112.3234, -123.4375, -150.8682],\n",
      "        [-206.4517, -237.1635, -204.6099, -203.3003, -258.5602, -293.4915,\n",
      "         -234.4723, -257.9978, -316.2202],\n",
      "        [-212.0839, -243.4985, -210.1112, -208.6736, -265.4227, -301.4042,\n",
      "         -240.8368, -264.9185, -324.6550],\n",
      "        [-180.7401, -207.3404, -178.8335, -177.3842, -225.8145, -256.6078,\n",
      "         -205.1929, -225.5388, -276.2803]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -51.9701,  -66.3896],\n",
      "        [-123.4665, -150.8682],\n",
      "        [-203.3003, -234.4723],\n",
      "        [-212.0839, -264.9185],\n",
      "        [-177.3842, -205.1929]], grad_fn=<GatherBackward>)\n",
      "A [[ -85.63322   -98.3509    -84.58119   -83.74549  -106.856995 -121.493126\n",
      "   -97.23866  -106.85149  -130.54588 ]\n",
      " [-135.87207  -155.8162   -134.33507  -133.08606  -169.57173  -192.8187\n",
      "  -154.26564  -169.4768   -207.33118 ]\n",
      " [-212.08388  -243.49846  -210.11116  -208.67355  -265.42267  -301.4042\n",
      "  -240.83678  -264.91855  -324.65503 ]\n",
      " [-220.74773  -253.6498   -218.93613  -217.78195  -276.80402  -314.0474\n",
      "  -250.75002  -276.04306  -338.4559  ]\n",
      " [-206.45172  -237.16353  -204.6099   -203.30031  -258.5602   -293.49155\n",
      "  -234.47227  -257.99783  -316.22025 ]]\n",
      "J tensor([ -83.7455, -133.0861, -208.6736, -217.7820, -203.3003])\n",
      "P tensor([ -97.2387, -154.2656, -240.8368, -250.7500, -234.4723])\n",
      "expected_state_action_values  tensor([[ -82.1382,  -94.2820],\n",
      "        [-130.4784, -149.5400],\n",
      "        [-206.7696, -235.7165],\n",
      "        [-217.4353, -247.1066],\n",
      "        [-202.3001, -230.3549]])\n",
      "reward -25.43304417270843\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-21.4316],\n",
      "        [ -6.7672],\n",
      "        [-10.7009],\n",
      "        [-19.3299],\n",
      "        [-14.0164]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [3, 7],\n",
      "        [4, 8],\n",
      "        [3, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-321.0611, -286.4650, -247.4755, -268.7754, -341.7332, -354.6201,\n",
      "         -311.4774, -287.5524, -351.1943],\n",
      "        [ -77.8705,  -69.6640,  -59.8379,  -64.9014,  -82.7668,  -85.9872,\n",
      "          -75.5754,  -69.8234,  -84.7973],\n",
      "        [-145.0923, -129.4798, -111.6258, -121.0002, -154.1266, -160.1240,\n",
      "         -140.7828, -129.8978, -158.2124],\n",
      "        [-268.0017, -239.0057, -206.3951, -223.9410, -284.9148, -295.8323,\n",
      "         -259.9868, -239.8842, -292.8401],\n",
      "        [-365.3313, -325.6164, -281.3599, -305.0896, -388.1671, -403.1390,\n",
      "         -354.3943, -326.8418, -399.0777]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-321.0611, -287.5524],\n",
      "        [ -64.9014,  -69.8234],\n",
      "        [-154.1266, -158.2124],\n",
      "        [-223.9410, -259.9868],\n",
      "        [-325.6164, -354.3943]], grad_fn=<GatherBackward>)\n",
      "A [[-339.3351   -302.9029   -261.74585  -284.60446  -361.66345  -375.04398\n",
      "  -329.23932  -304.1216   -371.6036  ]\n",
      " [-125.14536  -111.737404  -96.24206  -104.28097  -132.9156   -138.07579\n",
      "  -121.44309  -112.050644 -136.41208 ]\n",
      " [-199.37358  -177.7707   -153.4543   -166.34976  -211.78653  -220.01617\n",
      "  -193.43762  -178.42049  -217.52274 ]\n",
      " [-314.84372  -281.02493  -242.74252  -263.71765  -335.27872  -347.81265\n",
      "  -305.46432  -282.0657   -344.53912 ]\n",
      " [-268.0017   -239.00572  -206.39507  -223.94102  -284.9148   -295.83228\n",
      "  -259.9868   -239.88416  -292.84006 ]]\n",
      "J tensor([-261.7458,  -96.2421, -153.4543, -242.7425, -206.3951])\n",
      "P tensor([-304.1216, -112.0506, -178.4205, -282.0657, -239.8842])\n",
      "expected_state_action_values  tensor([[-257.0028, -295.1410],\n",
      "        [ -93.3851, -107.6128],\n",
      "        [-148.8098, -171.2793],\n",
      "        [-237.7981, -273.1890],\n",
      "        [-199.7719, -229.9121]])\n",
      "reward -23.401903181889658\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -7.8685],\n",
      "        [ -4.6880],\n",
      "        [-19.2270],\n",
      "        [-14.0164],\n",
      "        [ -4.2900]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [3, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-104.7121,  -98.7728,  -95.9401, -114.3370, -122.5356, -138.0569,\n",
      "         -110.5540, -120.7855, -147.4849],\n",
      "        [ -58.0813,  -54.9437,  -53.1726,  -63.4822,  -68.0712,  -76.6894,\n",
      "          -61.3518,  -67.1394,  -81.7425],\n",
      "        [-196.5671, -185.1261, -180.3172, -215.1508, -230.2239, -259.3076,\n",
      "         -207.4330, -226.6854, -277.4297],\n",
      "        [-294.1288, -276.8947, -269.9203, -321.9108, -344.4111, -387.9268,\n",
      "         -310.3734, -339.0862, -415.1592],\n",
      "        [ -71.9366,  -67.9791,  -65.8621,  -78.5172,  -84.1990,  -94.8812,\n",
      "          -75.9678,  -83.0542, -101.2276]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-114.3370, -120.7855],\n",
      "        [ -63.4822,  -67.1394],\n",
      "        [-185.1261, -207.4330],\n",
      "        [-276.8947, -310.3734],\n",
      "        [ -78.5172,  -83.0542]], grad_fn=<GatherBackward>)\n",
      "A [[-119.384254 -112.54865  -109.42876  -130.45877  -139.73274  -157.44957\n",
      "  -126.02711  -137.70695  -168.20433 ]\n",
      " [ -71.936584  -67.97914   -65.86208   -78.51723   -84.19902   -94.88116\n",
      "   -75.96775   -83.05423  -101.22764 ]\n",
      " [-294.12875  -276.89468  -269.9203   -321.9108   -344.41113  -387.92685\n",
      "  -310.37335  -339.08615  -415.1592  ]\n",
      " [-217.10516  -204.53001  -199.24141  -237.76326  -254.39256  -286.45035\n",
      "  -229.1203   -250.4278   -306.55322 ]\n",
      " [ -66.19987   -62.581863  -60.60803   -72.29203   -77.52136   -87.348915\n",
      "   -69.91608   -76.464745  -93.15989 ]]\n",
      "J tensor([-109.4288,  -65.8621, -269.9203, -199.2414,  -60.6080])\n",
      "P tensor([-126.0271,  -75.9678, -310.3734, -229.1203,  -69.9161])\n",
      "expected_state_action_values  tensor([[-106.3544, -121.2929],\n",
      "        [ -63.9639,  -73.0590],\n",
      "        [-262.1553, -298.5630],\n",
      "        [-193.3336, -220.2246],\n",
      "        [ -58.8372,  -67.2145]])\n",
      "5\n",
      "reward -26.20989500192921\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 4.0320, 5.2239, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 6.4395, 4.3138, 9.1767, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 2.6880, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 4.0320, 5.2239, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -4.2900],\n",
      "        [-23.4019],\n",
      "        [-26.2099],\n",
      "        [-10.7009],\n",
      "        [-21.4316]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [4, 8],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[ -66.9862,  -56.0526,  -61.3004,  -67.6651,  -78.4063,  -88.3597,\n",
      "          -64.7361,  -82.7799,  -94.2361],\n",
      "        [-296.0371, -246.8162, -272.1917, -301.5097, -347.9960, -391.3210,\n",
      "         -285.7550, -366.1978, -419.1085],\n",
      "        [-295.2119, -245.9431, -271.1440, -299.8854, -346.3621, -389.7969,\n",
      "         -284.9321, -364.7661, -417.3218],\n",
      "        [-110.5859,  -92.2381, -101.3240, -111.8584, -129.4406, -145.8621,\n",
      "         -106.7988, -136.5499, -155.7819],\n",
      "        [-238.3316, -198.6827, -218.9262, -242.2673, -279.7914, -314.8077,\n",
      "         -230.0506, -294.6160, -337.0266]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -67.6651,  -82.7799],\n",
      "        [-272.1917, -285.7550],\n",
      "        [-245.9431, -284.9321],\n",
      "        [-129.4406, -155.7819],\n",
      "        [-238.3316, -294.6160]], grad_fn=<GatherBackward>)\n",
      "A [[ -61.720474  -51.671425  -56.4801    -62.381477  -72.27987   -81.44781\n",
      "   -59.654953  -76.306114  -86.83366 ]\n",
      " [-295.21194  -245.94313  -271.14398  -299.88544  -346.36212  -389.79688\n",
      "  -284.93213  -364.76614  -417.3218  ]\n",
      " [-298.9296   -249.30656  -274.82758  -304.29504  -351.2849   -395.0475\n",
      "  -288.5476   -369.7305   -423.11044 ]\n",
      " [-149.81148  -124.79051  -137.33525  -151.63106  -175.36661  -197.60205\n",
      "  -144.64249  -184.93024  -211.16484 ]\n",
      " [-251.71489  -209.96608  -231.45909  -256.4995   -296.05972  -332.80753\n",
      "  -243.00175  -311.4768   -356.5045  ]]\n",
      "J tensor([ -51.6714, -245.9431, -249.3066, -124.7905, -209.9661])\n",
      "P tensor([ -59.6550, -284.9321, -288.5476, -144.6425, -243.0018])\n",
      "expected_state_action_values  tensor([[ -50.7943,  -57.9795],\n",
      "        [-244.7507, -279.8408],\n",
      "        [-250.5858, -285.9027],\n",
      "        [-123.0124, -140.8792],\n",
      "        [-210.4010, -240.1331]])\n",
      "reward -24.100822192622793\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 4.8914, 9.7098, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 6.4395, 4.3138, 9.1767, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-25.6968],\n",
      "        [-11.8820],\n",
      "        [-24.1008],\n",
      "        [-18.9634],\n",
      "        [-21.4316]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [3, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-182.8843, -202.4435, -141.5135, -204.4747, -234.1244, -286.3780,\n",
      "         -199.9156, -250.9236, -286.5653],\n",
      "        [-112.5910, -124.3319,  -86.7578, -124.7856, -143.2281, -175.7079,\n",
      "         -123.0566, -153.9507, -175.3237],\n",
      "        [-217.1905, -240.3397, -168.0299, -242.4894, -277.7151, -339.8774,\n",
      "         -237.3682, -297.8031, -340.0265],\n",
      "        [-171.5933, -189.8854, -132.6551, -191.4210, -219.3165, -268.4355,\n",
      "         -187.5474, -235.2260, -268.5215],\n",
      "        [-175.1827, -193.7213, -135.3638, -195.2279, -223.6976, -273.9382,\n",
      "         -191.4395, -240.0118, -273.9261]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-204.4747, -199.9156],\n",
      "        [-124.3319, -123.0566],\n",
      "        [-168.0299, -237.3682],\n",
      "        [-191.4210, -187.5474],\n",
      "        [-175.1827, -240.0118]], grad_fn=<GatherBackward>)\n",
      "A [[-216.4285   -239.44699  -167.41528  -241.583    -276.6866   -338.65118\n",
      "  -236.52904  -296.71692  -338.77942 ]\n",
      " [-137.29697  -151.61035  -105.847435 -152.4163   -174.81906  -214.351\n",
      "  -150.00291  -187.78519  -214.15735 ]\n",
      " [-194.86707  -215.82462  -150.81793  -217.79118  -249.42825  -305.07516\n",
      "  -213.01793  -267.3642   -305.33286 ]\n",
      " [-175.18272  -193.72134  -135.36377  -195.2279   -223.69762  -273.93823\n",
      "  -191.43953  -240.01176  -273.92615 ]\n",
      " [-182.88434  -202.44348  -141.5135   -204.47475  -234.12442  -286.37796\n",
      "  -199.9156   -250.92358  -286.56528 ]]\n",
      "J tensor([-167.4153, -105.8474, -150.8179, -135.3638, -141.5135])\n",
      "P tensor([-236.5290, -150.0029, -213.0179, -191.4395, -199.9156])\n",
      "expected_state_action_values  tensor([[-176.3705, -238.5729],\n",
      "        [-107.1447, -146.8846],\n",
      "        [-159.8369, -215.8170],\n",
      "        [-140.7908, -191.2589],\n",
      "        [-148.7937, -201.3556]])\n",
      "reward -32.225522898002424\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 4.8914, 9.7098, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 6.4395, 4.3138, 9.1767, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-11.8820],\n",
      "        [-10.7610],\n",
      "        [-24.1008],\n",
      "        [ -6.7672],\n",
      "        [-25.6968]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [3, 7],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[ -84.9375, -100.2202,  -55.3016, -103.2061, -128.5444, -157.5795,\n",
      "         -120.2919, -129.0099, -157.5423],\n",
      "        [ -84.4809,  -99.6833,  -55.0046, -102.6588, -127.8600, -156.7390,\n",
      "         -119.6471, -128.3211, -156.6996],\n",
      "        [-158.0414, -187.0427, -103.4708, -193.8676, -240.7576, -294.3441,\n",
      "         -224.0260, -240.9723, -295.1238],\n",
      "        [ -34.8574,  -41.3861,  -22.5993,  -42.2878,  -52.7971,  -64.7173,\n",
      "          -49.4555,  -53.0710,  -64.4972],\n",
      "        [-133.1420, -157.6467,  -87.1801, -163.6261, -203.1304, -248.1893,\n",
      "         -188.8089, -203.1851, -248.9108]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-100.2202, -120.2919],\n",
      "        [ -99.6833, -119.6471],\n",
      "        [-103.4708, -224.0260],\n",
      "        [ -42.2878,  -53.0710],\n",
      "        [-163.6261, -188.8089]], grad_fn=<GatherBackward>)\n",
      "A [[-103.242714 -121.81869   -67.26344  -125.70396  -156.42213  -191.64366\n",
      "  -146.17307  -156.8715   -191.87367 ]\n",
      " [ -84.93754  -100.22024   -55.30163  -103.206085 -128.54442  -157.5795\n",
      "  -120.29186  -129.00987  -157.54233 ]\n",
      " [-141.17786  -167.282     -92.47722  -173.42763  -215.3586   -263.10583\n",
      "  -200.20541  -215.45192  -263.92587 ]\n",
      " [ -55.223907  -65.31939   -35.884003  -66.96991   -83.547935 -102.40288\n",
      "   -78.257965  -83.91197  -102.30156 ]\n",
      " [-157.78415  -186.68506  -103.28008  -193.49075  -240.30252  -293.82065\n",
      "  -223.64534  -240.53247  -294.57977 ]]\n",
      "J tensor([ -67.2634,  -55.3016,  -92.4772,  -35.8840, -103.2801])\n",
      "P tensor([-146.1731, -120.2919, -200.2054,  -78.2580, -223.6453])\n",
      "expected_state_action_values  tensor([[ -72.4191, -143.4377],\n",
      "        [ -60.5325, -119.0237],\n",
      "        [-107.3303, -204.2857],\n",
      "        [ -39.0628,  -77.1994],\n",
      "        [-118.6489, -226.9776]])\n",
      "reward -25.321490663580043\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-18.9634],\n",
      "        [ -4.6880],\n",
      "        [-21.4316],\n",
      "        [-19.3299],\n",
      "        [-25.6968]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [3, 7],\n",
      "        [0, 7],\n",
      "        [3, 6],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[-116.2816, -122.2085, -101.1452, -129.7579, -176.6926, -216.1462,\n",
      "         -171.4976, -188.8510, -216.0202],\n",
      "        [ -28.9888,  -30.6245,  -25.0130,  -32.0557,  -43.8769,  -53.8043,\n",
      "          -42.8059,  -47.0718,  -53.3939],\n",
      "        [-119.4739, -125.4288, -103.8455, -133.1192, -181.3076, -221.9372,\n",
      "         -176.1350, -193.8727, -221.7083],\n",
      "        [-104.9561, -110.0158,  -91.0115, -116.4281, -158.7880, -194.5876,\n",
      "         -154.6118, -169.9732, -194.2720],\n",
      "        [-122.7250, -129.0395, -106.8980, -137.4046, -186.9088, -228.4581,\n",
      "         -181.1047, -199.5856, -228.4156]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-129.7579, -171.4976],\n",
      "        [ -32.0557,  -47.0718],\n",
      "        [-119.4739, -193.8727],\n",
      "        [-116.4281, -154.6118],\n",
      "        [-137.4046, -181.1047]], grad_fn=<GatherBackward>)\n",
      "A [[-119.47392  -125.42877  -103.84552  -133.11925  -181.30765  -221.9372\n",
      "  -176.135    -193.87267  -221.7083  ]\n",
      " [ -35.853477  -37.796494  -30.943243  -39.53226   -54.13802   -66.415146\n",
      "   -52.886097  -58.099403  -66.000305]\n",
      " [-122.725006 -129.03954  -106.89801  -137.40463  -186.90883  -228.4581\n",
      "  -181.10474  -199.58559  -228.41563 ]\n",
      " [-116.28158  -122.20846  -101.14521  -129.75792  -176.6926   -216.14624\n",
      "  -171.49759  -188.85098  -216.02019 ]\n",
      " [-145.15506  -152.5133   -126.36695  -162.12868  -220.65216  -269.91876\n",
      "  -214.08267  -235.7967   -269.77267 ]]\n",
      "J tensor([-103.8455,  -30.9432, -106.8980, -101.1452, -126.3670])\n",
      "P tensor([-176.1350,  -52.8861, -181.1047, -171.4976, -214.0827])\n",
      "expected_state_action_values  tensor([[-112.4243, -177.4848],\n",
      "        [ -32.5369,  -52.2855],\n",
      "        [-117.6398, -184.4258],\n",
      "        [-110.3605, -173.6777],\n",
      "        [-139.4270, -218.3712]])\n",
      "1\n",
      "reward -24.727283241725065\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[5.7065, 6.4395, 2.6880, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 4.0320, 5.2239, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 7.1502, 6.6508, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 2.6880, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.2506, 6.4395, 6.7159, 4.9156, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-25.4330],\n",
      "        [-19.2270],\n",
      "        [-21.4316],\n",
      "        [-23.4019],\n",
      "        [-24.7273]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-129.1527, -162.8295, -135.4399, -155.3038, -235.6668, -287.6965,\n",
      "         -248.4207, -236.4842, -286.6530],\n",
      "        [ -84.0970, -105.6495,  -87.7226, -100.2823, -152.5332, -186.6078,\n",
      "         -161.3901, -153.3672, -185.6655],\n",
      "        [-105.4698, -132.8429, -110.4148, -126.5091, -192.1100, -234.6737,\n",
      "         -202.7333, -192.8961, -233.7110],\n",
      "        [-128.5006, -162.0017, -134.8170, -154.7350, -234.6845, -286.4317,\n",
      "         -247.2469, -235.4150, -285.4039],\n",
      "        [-143.9124, -180.9404, -150.4587, -171.8596, -261.3287, -319.4934,\n",
      "         -276.2829, -262.6074, -318.1944]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-135.4399, -236.4842],\n",
      "        [-105.6495, -161.3901],\n",
      "        [-105.4698, -192.8961],\n",
      "        [-134.8170, -247.2469],\n",
      "        [-143.9124, -262.6074]], grad_fn=<GatherBackward>)\n",
      "A [[-128.50058 -162.0017  -134.81705 -154.73505 -234.68454 -286.4317\n",
      "  -247.24686 -235.41502 -285.40387]\n",
      " [-123.37751 -154.87422 -128.79613 -147.06235 -223.6863  -273.67905\n",
      "  -236.71211 -224.88939 -272.38623]\n",
      " [-109.9184  -138.69443 -115.34606 -132.50262 -200.94734 -245.13321\n",
      "  -211.58423 -201.51036 -244.32607]\n",
      " [-130.97112 -164.77034 -137.06802 -156.86906 -238.29218 -291.18713\n",
      "  -251.62727 -239.31122 -290.00357]\n",
      " [-135.71571 -170.86517 -142.03343 -162.42455 -246.89127 -301.59055\n",
      "  -260.73785 -247.94815 -300.54007]]\n",
      "J tensor([-128.5006, -123.3775, -109.9184, -130.9711, -135.7157])\n",
      "P tensor([-235.4150, -224.8894, -201.5104, -239.3112, -247.9482])\n",
      "expected_state_action_values  tensor([[-141.0836, -237.3066],\n",
      "        [-130.2668, -221.6275],\n",
      "        [-120.3581, -202.7909],\n",
      "        [-141.2759, -238.7820],\n",
      "        [-146.8714, -247.8806]])\n",
      "5\n",
      "reward -28.372394438864504\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[5.7065, 6.4395, 4.0320, 5.2239, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 6.4395, 4.3138, 9.1767, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[5.7065, 6.4395, 2.6880, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.5828, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 4.0320, 5.2239, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-23.4019],\n",
      "        [-25.6968],\n",
      "        [-18.9634],\n",
      "        [ -3.7268],\n",
      "        [-26.2099]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [3, 6],\n",
      "        [3, 6],\n",
      "        [3, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-173.3563, -204.8105, -185.8957, -174.5729, -265.2788, -324.6897,\n",
      "         -261.4140, -283.7094, -322.9556],\n",
      "        [-148.3084, -175.3232, -159.0547, -149.4653, -227.1106, -277.8594,\n",
      "         -223.7023, -242.8257, -276.4453],\n",
      "        [-138.9943, -164.2384, -148.8936, -139.6869, -212.4556, -260.1036,\n",
      "         -209.5505, -227.3287, -258.6918],\n",
      "        [ -31.8819,  -37.8041,  -33.9335,  -31.7894,  -48.5482,  -59.5696,\n",
      "          -48.1245,  -52.1319,  -58.8620],\n",
      "        [-174.9839, -206.4188, -187.2943, -175.4478, -266.9769, -327.1205,\n",
      "         -263.6451, -285.8193, -325.2272]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-185.8957, -261.4140],\n",
      "        [-149.4653, -223.7023],\n",
      "        [-139.6869, -209.5505],\n",
      "        [ -31.7894,  -52.1319],\n",
      "        [-206.4188, -263.6451]], grad_fn=<GatherBackward>)\n",
      "A [[-174.9839   -206.41884  -187.2943   -175.44785  -266.97693  -327.1205\n",
      "  -263.64514  -285.81934  -325.2272  ]\n",
      " [-174.17908  -205.7956   -186.71596  -175.20229  -266.35538  -326.07184\n",
      "  -262.6082   -284.94775  -324.31967 ]\n",
      " [-141.1676   -166.66817  -151.13293  -141.69981  -215.56131  -264.04532\n",
      "  -212.75946  -230.73503  -262.508   ]\n",
      " [ -33.057663  -39.18606   -35.184803  -32.945503  -50.32059   -61.748295\n",
      "   -49.889957  -54.03721   -61.029205]\n",
      " [-174.82509  -206.61285  -187.44276  -175.90405  -267.406    -327.32333\n",
      "  -263.60138  -286.05444  -325.58615 ]]\n",
      "J tensor([-174.9839, -174.1791, -141.1676,  -32.9455, -174.8251])\n",
      "P tensor([-263.6451, -262.6082, -212.7595,  -49.8900, -263.6014])\n",
      "expected_state_action_values  tensor([[-180.8874, -260.6825],\n",
      "        [-182.4579, -262.0442],\n",
      "        [-146.0142, -210.4469],\n",
      "        [ -33.3778,  -48.6278],\n",
      "        [-183.5525, -263.4511]])\n",
      "reward -24.83265263174255\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 2.6880, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 7.1502, 6.6508, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.2506, 6.4395, 6.7159, 4.9156, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -6.7672],\n",
      "        [-25.4330],\n",
      "        [-24.7273],\n",
      "        [ -4.6880],\n",
      "        [-11.8820]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 7],\n",
      "        [0, 7],\n",
      "        [3, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -37.8926,  -40.2868,  -33.9506,  -42.0167,  -57.9542,  -71.1040,\n",
      "          -61.1487,  -58.4531,  -70.3441],\n",
      "        [-176.7510, -187.2929, -159.5444, -198.0621, -271.9428, -332.7998,\n",
      "         -285.2425, -273.1152, -331.0514],\n",
      "        [-193.2129, -204.3582, -174.0176, -215.3498, -296.1487, -362.8897,\n",
      "         -311.4469, -297.8036, -360.8302],\n",
      "        [ -33.2362,  -35.3750,  -29.7763,  -36.9105,  -50.8920,  -62.4264,\n",
      "          -53.6669,  -51.3261,  -61.7113],\n",
      "        [ -90.2128,  -95.3595,  -81.0527, -100.2178, -137.9761, -169.2715,\n",
      "         -145.4058, -138.9256, -167.9245]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -42.0167,  -58.4531],\n",
      "        [-159.5444, -273.1152],\n",
      "        [-193.2129, -297.8036],\n",
      "        [ -36.9105,  -51.3261],\n",
      "        [ -95.3595, -145.4058]], grad_fn=<GatherBackward>)\n",
      "A [[ -60.19712   -63.77843   -54.003925  -66.72946   -92.003494 -112.86375\n",
      "   -97.051414  -92.71082  -111.902565]\n",
      " [-175.95145  -186.422    -158.87164  -197.3774   -270.89728  -331.45612\n",
      "  -284.00656  -271.98154  -329.72556 ]\n",
      " [-184.86922  -195.72356  -166.61958  -206.374    -283.74063  -347.4416\n",
      "  -298.12866  -285.18015  -345.6517  ]\n",
      " [ -41.18289   -43.757584  -36.90024   -45.624866  -62.944527  -77.23583\n",
      "   -66.435486  -63.489147  -76.44424 ]\n",
      " [-110.729996 -117.04174   -99.53895  -123.23888  -169.52646  -207.86826\n",
      "  -178.44403  -170.57436  -206.49223 ]]\n",
      "J tensor([ -54.0039, -158.8716, -166.6196,  -36.9002,  -99.5389])\n",
      "P tensor([ -92.7108, -271.9815, -285.1801,  -63.4891, -170.5744])\n",
      "expected_state_action_values  tensor([[ -55.3708,  -90.2070],\n",
      "        [-168.4175, -270.2164],\n",
      "        [-174.6849, -281.3894],\n",
      "        [ -37.8982,  -61.8282],\n",
      "        [-101.4670, -165.3989]])\n",
      "reward -36.97238340835783\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 1.5828, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -3.7268],\n",
      "        [ -7.8685],\n",
      "        [ -4.2900],\n",
      "        [-10.7009],\n",
      "        [-19.2270]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [4, 8],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -27.5291,  -38.1633,  -34.1506,  -39.2535,  -49.8958,  -60.8666,\n",
      "          -55.6832,  -47.1478,  -60.3025],\n",
      "        [ -50.4027,  -69.5647,  -62.5909,  -71.7139,  -91.1598, -111.2107,\n",
      "         -101.7894,  -86.0373, -110.5192],\n",
      "        [ -35.0677,  -48.5152,  -43.4947,  -49.8608,  -63.4159,  -77.3868,\n",
      "          -70.8454,  -59.9218,  -76.7766],\n",
      "        [ -55.2489,  -76.2125,  -68.6615,  -78.7125,  -99.9813, -121.9926,\n",
      "         -111.5934,  -94.3365, -121.2070],\n",
      "        [ -89.7573, -123.6702, -111.7200, -128.2452, -162.6794, -198.3672,\n",
      "         -181.2808, -153.2743, -197.4923]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -39.2535,  -47.1478],\n",
      "        [ -71.7139,  -86.0373],\n",
      "        [ -49.8608,  -59.9218],\n",
      "        [ -99.9813, -121.2070],\n",
      "        [-123.6702, -181.2808]], grad_fn=<GatherBackward>)\n",
      "A [[ -28.511164  -39.511887  -35.367905  -40.63533   -51.657127  -63.018715\n",
      "   -57.658455  -48.81189   -62.44863 ]\n",
      " [ -55.248898  -76.21249   -68.66153   -78.71247   -99.98126  -121.99261\n",
      "  -111.593414  -94.336525 -121.20702 ]\n",
      " [ -32.35301   -44.787388  -40.129807  -46.041046  -58.547234  -71.43771\n",
      "   -65.38539   -55.32177   -70.84417 ]\n",
      " [ -73.49333  -101.25148   -91.41735  -104.80948  -133.05177  -162.32767\n",
      "  -148.43677  -125.45516  -161.38905 ]\n",
      " [-130.99226  -180.39026  -163.16548  -187.16034  -237.39366  -289.47824\n",
      "  -264.54724  -223.61905  -288.2905  ]]\n",
      "J tensor([ -28.5112,  -55.2489,  -32.3530,  -73.4933, -130.9923])\n",
      "P tensor([ -48.8119,  -94.3365,  -55.3218, -125.4552, -223.6190])\n",
      "expected_state_action_values  tensor([[ -29.3869,  -47.6575],\n",
      "        [ -57.5925,  -92.7714],\n",
      "        [ -33.4077,  -54.0796],\n",
      "        [ -76.8449, -123.6105],\n",
      "        [-137.1201, -220.4842]])\n",
      "1\n",
      "reward -33.91603470401096\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[4.2799, 3.2197, 4.8914, 9.7098, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 7.4044, 6.5020, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-32.2255],\n",
      "        [-36.9724],\n",
      "        [-25.6968],\n",
      "        [-19.3299],\n",
      "        [-11.8820]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [3, 7],\n",
      "        [3, 6],\n",
      "        [3, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-135.2445, -204.1527, -168.4096, -177.3454, -223.4483, -298.1274,\n",
      "         -289.0537, -245.1433, -318.4363],\n",
      "        [-149.6713, -225.2669, -185.8571, -194.9901, -246.1095, -328.8908,\n",
      "         -319.3334, -270.4158, -351.0123],\n",
      "        [-127.0700, -191.6969, -158.2014, -166.6960, -209.9715, -280.1679,\n",
      "         -271.5882, -230.3245, -299.1947],\n",
      "        [-104.2021, -156.6679, -129.1458, -135.5439, -171.0688, -228.8040,\n",
      "         -222.1273, -188.0892, -244.0469],\n",
      "        [ -78.1233, -117.3589,  -96.6777, -101.2961, -127.9778, -171.3422,\n",
      "         -166.4604, -140.8683, -182.4722]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-168.4096, -289.0537],\n",
      "        [-194.9901, -270.4158],\n",
      "        [-166.6960, -271.5882],\n",
      "        [-135.5439, -222.1273],\n",
      "        [-117.3589, -166.4604]], grad_fn=<GatherBackward>)\n",
      "A [[-179.73396  -271.03397  -223.74571  -235.40987  -296.63974  -395.96008\n",
      "  -383.97467  -325.5201   -422.89655 ]\n",
      " [-207.34412  -312.3145   -257.9563   -271.07602  -341.77396  -456.38596\n",
      "  -442.78915  -375.16376  -487.312   ]\n",
      " [-148.0744   -223.23819  -184.24915  -193.88559  -244.3015   -326.19046\n",
      "  -316.29367  -268.14462  -348.2444  ]\n",
      " [-119.109344 -179.55713  -148.08197  -155.7888   -196.38802  -262.23724\n",
      "  -254.35396  -215.60883  -279.95633 ]\n",
      " [ -95.372635 -143.29813  -118.10334  -123.91759  -156.42749  -209.3053\n",
      "  -203.22662  -172.04897  -223.17847 ]]\n",
      "J tensor([-179.7340, -207.3441, -148.0744, -119.1093,  -95.3726])\n",
      "P tensor([-325.5201, -375.1638, -268.1446, -215.6088, -172.0490])\n",
      "expected_state_action_values  tensor([[-193.9861, -325.1936],\n",
      "        [-223.5821, -374.6198],\n",
      "        [-158.9637, -267.0269],\n",
      "        [-126.5283, -213.3778],\n",
      "        [ -97.7173, -166.7260]])\n",
      "1\n",
      "reward -38.61467087925402\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-11.8820],\n",
      "        [ -6.7672],\n",
      "        [-33.9160],\n",
      "        [-25.3215],\n",
      "        [-25.6968]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [3, 7],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[ -78.5552, -108.4171, -110.6577,  -93.3678, -128.6770, -172.3100,\n",
      "         -158.6230, -149.7904, -183.2518],\n",
      "        [ -34.1952,  -47.4043,  -48.0429,  -40.5507,  -55.9892,  -74.9821,\n",
      "          -69.0892,  -65.2763,  -79.5560],\n",
      "        [-213.7368, -295.7263, -302.6418, -256.1452, -352.2423, -470.4563,\n",
      "         -432.4729, -408.9581, -501.6675],\n",
      "        [-185.3712, -256.7709, -262.6392, -222.5602, -305.8675, -408.3625,\n",
      "         -375.2114, -355.0092, -435.5583],\n",
      "        [-130.1290, -180.3240, -184.4005, -156.4996, -214.9713, -286.9014,\n",
      "         -263.5153, -249.4103, -305.9708]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-108.4171, -158.6230],\n",
      "        [ -40.5507,  -65.2763],\n",
      "        [-213.7368, -408.9581],\n",
      "        [-262.6392, -375.2114],\n",
      "        [-156.4996, -263.5153]], grad_fn=<GatherBackward>)\n",
      "A [[ -95.228065 -131.44696  -134.23212  -113.421555 -156.17352  -209.00833\n",
      "  -192.29268  -181.66481  -222.54831 ]\n",
      " [ -53.552967  -74.03326   -75.320305  -63.49966   -87.64197  -117.35238\n",
      "  -108.12677  -102.09525  -124.753716]\n",
      " [-220.72324  -304.90488  -311.79218  -263.2688   -362.47827  -484.68533\n",
      "  -445.92926  -421.33182  -516.7725  ]\n",
      " [-166.72859  -230.35431  -235.48125  -198.92458  -273.8272   -366.20413\n",
      "  -336.86038  -318.32562  -390.29053 ]\n",
      " [-151.69995  -210.0842   -214.84499  -182.09119  -250.23012  -334.17294\n",
      "  -307.02237  -290.49054  -356.2862  ]]\n",
      "J tensor([ -95.2281,  -53.5530, -220.7232, -166.7286, -151.7000])\n",
      "P tensor([-181.6648, -102.0953, -421.3318, -318.3256, -290.4905])\n",
      "expected_state_action_values  tensor([[ -97.5872, -175.3803],\n",
      "        [ -54.9649,  -98.6530],\n",
      "        [-232.5669, -413.1147],\n",
      "        [-175.3772, -311.8145],\n",
      "        [-162.2267, -287.1382]])\n",
      "1\n",
      "reward -36.99886427469392\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-33.9160],\n",
      "        [ -4.2900],\n",
      "        [-25.4330],\n",
      "        [ -3.7268],\n",
      "        [-23.4019]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [3, 7],\n",
      "        [2, 7],\n",
      "        [3, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-274.9106, -298.7705, -296.2995, -306.7342, -388.0412, -519.7563,\n",
      "         -503.9912, -475.5294, -553.3605],\n",
      "        [ -47.1831,  -51.4181,  -50.5279,  -52.1644,  -66.2280,  -88.9312,\n",
      "          -86.4170,  -81.4693,  -94.2780],\n",
      "        [-194.4973, -211.5431, -209.6455, -217.2956, -274.7360, -367.9648,\n",
      "         -356.6240, -336.6512, -391.6902],\n",
      "        [ -36.8362,  -40.2098,  -39.4465,  -40.8235,  -51.8011,  -69.5339,\n",
      "          -67.5283,  -63.7108,  -73.6242],\n",
      "        [-193.0000, -209.8946, -208.0965, -215.8398, -272.8008, -365.3003,\n",
      "         -353.9530, -334.1808, -388.8609]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-274.9106, -475.5294],\n",
      "        [ -52.1644,  -81.4693],\n",
      "        [-209.6455, -336.6512],\n",
      "        [ -40.8235,  -63.7108],\n",
      "        [-208.0965, -353.9530]], grad_fn=<GatherBackward>)\n",
      "A [[-281.30707  -305.4026   -302.64767  -312.68332  -395.9421   -530.88904\n",
      "  -515.15875  -485.71777  -565.13464 ]\n",
      " [ -43.45712   -47.381878  -46.53744   -48.08045   -61.032764  -81.946106\n",
      "   -79.61506   -75.07435   -86.84041 ]\n",
      " [-193.00002  -209.89456  -208.09648  -215.83977  -272.80084  -365.3003\n",
      "  -353.95303  -334.18085  -388.86087 ]\n",
      " [ -38.18416   -41.66994   -40.890144  -42.3009    -53.68054   -72.06085\n",
      "   -69.98899   -66.024254  -76.31484 ]\n",
      " [-192.92206  -209.5409   -207.66615  -214.94519  -271.94272  -364.52826\n",
      "  -353.48157  -333.46558  -387.88837 ]]\n",
      "J tensor([-281.3071,  -43.4571, -193.0000,  -38.1842, -192.9221])\n",
      "P tensor([-485.7178,  -75.0743, -334.1808,  -66.0243, -333.4656])\n",
      "expected_state_action_values  tensor([[-287.0924, -471.0620],\n",
      "        [ -43.4014,  -71.8569],\n",
      "        [-199.1331, -326.1958],\n",
      "        [ -38.0926,  -63.1486],\n",
      "        [-197.0318, -323.5209]])\n",
      "reward -32.695854804497245\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.5828, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-10.7610],\n",
      "        [-14.0164],\n",
      "        [ -3.7268],\n",
      "        [ -4.6880],\n",
      "        [-25.6968]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[-102.0783,  -97.9204,  -85.0844,  -92.3195, -126.8102, -170.2056,\n",
      "         -156.7841, -147.9924, -180.9740],\n",
      "        [-182.4202, -174.8653, -152.2425, -165.2712, -226.7924, -304.2491,\n",
      "         -280.0899, -264.4755, -323.9955],\n",
      "        [ -37.6122,  -36.3187,  -31.2485,  -34.0152,  -46.7872,  -62.7859,\n",
      "          -57.8631,  -54.6668,  -66.5353],\n",
      "        [ -39.0140,  -37.6616,  -32.4156,  -35.2677,  -48.5158,  -65.1096,\n",
      "          -60.0107,  -56.6889,  -69.0132],\n",
      "        [-170.1909, -163.6526, -142.5069, -155.5259, -212.8996, -284.8201,\n",
      "         -261.7469, -247.6558, -303.7000]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -97.9204, -156.7841],\n",
      "        [-174.8653, -280.0899],\n",
      "        [ -34.0152,  -54.6668],\n",
      "        [ -35.2677,  -56.6889],\n",
      "        [-155.5259, -261.7469]], grad_fn=<GatherBackward>)\n",
      "A [[-102.66336   -98.48089   -85.57157   -92.84229  -127.53172  -171.17549\n",
      "  -157.68051  -148.83644  -182.00826 ]\n",
      " [-136.5522   -131.04004  -113.954445 -123.85515  -169.92688  -227.86089\n",
      "  -209.72899  -198.11046  -242.65805 ]\n",
      " [ -39.013985  -37.661617  -32.415577  -35.2677    -48.515762  -65.1096\n",
      "   -60.010742  -56.688896  -69.013176]\n",
      " [ -48.37279   -46.627464  -40.20715   -43.62975   -60.056282  -80.622955\n",
      "   -74.34905   -70.1891    -85.55597 ]\n",
      " [-198.37181  -190.67407  -166.05772  -180.95473  -247.8239   -331.75607\n",
      "  -304.97766  -288.45956  -353.64487 ]]\n",
      "J tensor([ -85.5716, -113.9544,  -32.4156,  -40.2071, -166.0577])\n",
      "P tensor([-148.8364, -198.1105,  -56.6889,  -70.1891, -288.4596])\n",
      "expected_state_action_values  tensor([[ -87.7754, -144.7138],\n",
      "        [-116.5754, -192.3158],\n",
      "        [ -32.9008,  -54.7468],\n",
      "        [ -40.8744,  -67.8582],\n",
      "        [-175.1487, -285.3104]])\n",
      "reward -35.200938138820206\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -7.8685],\n",
      "        [-36.9724],\n",
      "        [-38.6147],\n",
      "        [-25.6968],\n",
      "        [-10.7610]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [3, 7],\n",
      "        [0, 7],\n",
      "        [3, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -66.5578,  -57.8620,  -55.4574,  -65.8227,  -83.0584, -111.3597,\n",
      "          -96.9307, -101.6126, -118.3496],\n",
      "        [-194.7390, -169.0294, -163.0108, -193.8051, -243.9305, -326.4578,\n",
      "         -283.6272, -297.7366, -347.9132],\n",
      "        [-276.9342, -240.1840, -231.7373, -275.3100, -346.5199, -463.9630,\n",
      "         -403.1239, -423.0854, -494.6078],\n",
      "        [-165.4195, -143.7534, -138.6989, -165.6394, -208.0426, -278.0303,\n",
      "         -241.1120, -253.5192, -296.4666],\n",
      "        [ -96.4289,  -83.6091,  -80.4654,  -95.5521, -120.3968, -161.4456,\n",
      "         -140.3892, -147.2096, -171.6352]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -65.8227, -101.6126],\n",
      "        [-193.8051, -297.7366],\n",
      "        [-276.9342, -423.0854],\n",
      "        [-165.6394, -241.1120],\n",
      "        [ -83.6091, -140.3892]], grad_fn=<GatherBackward>)\n",
      "A [[ -72.56124   -63.022285  -60.495     -71.83344   -90.57375  -121.46439\n",
      "  -105.66716  -110.79487  -129.05096 ]\n",
      " [-271.47412  -235.6366   -227.57297  -270.98386  -340.70438  -455.645\n",
      "  -395.5205   -415.49118  -485.81323 ]\n",
      " [-306.6899   -265.92612  -256.72556  -305.11346  -383.8939   -513.99146\n",
      "  -446.46768  -468.64966  -547.8829  ]\n",
      " [-191.99379  -166.78874  -160.93349  -191.90646  -241.13068  -322.46204\n",
      "  -279.7468   -294.03113  -343.7405  ]\n",
      " [ -96.9813    -84.08768   -80.92577   -96.093    -121.08121  -162.36479\n",
      "  -141.19138  -148.04854  -172.6154  ]]\n",
      "J tensor([ -60.4950, -227.5730, -256.7256, -160.9335,  -80.9258])\n",
      "P tensor([-105.6672, -395.5205, -446.4677, -279.7468, -141.1914])\n",
      "expected_state_action_values  tensor([[ -62.3140, -102.9689],\n",
      "        [-241.7880, -392.9408],\n",
      "        [-269.6677, -440.4355],\n",
      "        [-170.5369, -277.4689],\n",
      "        [ -83.5942, -137.8332]])\n",
      "reward -35.78329078118929\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 4.8914, 9.7098, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 7.1502, 6.6508, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 6.4395, 4.3138, 9.1767, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 4.7294, 3.1707, 4.7834, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.2506, 6.4395, 6.7159, 4.9156, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-14.0164],\n",
      "        [-24.1008],\n",
      "        [-21.4316],\n",
      "        [ -4.2900],\n",
      "        [-24.7273]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [3, 7],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-167.3661, -161.7899, -157.4018, -200.5265, -235.1219, -315.2468,\n",
      "         -288.8585, -301.1362, -335.2381],\n",
      "        [-189.1931, -183.4664, -178.5417, -228.1758, -267.1676, -357.4525,\n",
      "         -327.0726, -341.5464, -380.5331],\n",
      "        [-149.8199, -145.1653, -141.1835, -180.3173, -211.2309, -282.8042,\n",
      "         -258.8750, -270.2010, -300.9166],\n",
      "        [ -44.4373,  -43.2055,  -41.6167,  -52.9966,  -62.3238,  -83.6176,\n",
      "          -76.7389,  -79.9874,  -88.6249],\n",
      "        [-202.6914, -196.1903, -190.8517, -243.2174, -285.1328, -382.0193,\n",
      "         -349.9792, -365.0043, -406.5101]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-161.7899, -288.8585],\n",
      "        [-178.5417, -327.0726],\n",
      "        [-149.8199, -270.2010],\n",
      "        [ -52.9966,  -79.9874],\n",
      "        [-202.6914, -365.0043]], grad_fn=<GatherBackward>)\n",
      "A [[-125.84257  -121.78994  -118.34902  -150.92819  -176.95758  -237.15964\n",
      "  -217.26323  -226.5822   -252.20905 ]\n",
      " [-174.9236   -169.77962  -165.13457  -211.1591   -247.24701  -330.6144\n",
      "  -302.49362  -315.95984  -352.11102 ]\n",
      " [-161.58963  -156.72202  -152.51955  -195.15195  -228.44304  -305.48978\n",
      "  -279.44608  -291.8896   -325.28018 ]\n",
      " [ -40.906803  -39.79446   -38.306313  -48.82002   -57.40605   -77.01137\n",
      "   -70.661766  -73.67025   -81.58995 ]\n",
      " [-196.45198  -190.32695  -185.10245  -236.09251  -276.73688  -370.4992\n",
      "  -339.35236  -354.05286  -394.45624 ]]\n",
      "J tensor([-118.3490, -165.1346, -152.5195,  -38.3063, -185.1024])\n",
      "P tensor([-217.2632, -302.4936, -279.4461,  -70.6618, -339.3524])\n",
      "expected_state_action_values  tensor([[-120.5305, -209.5533],\n",
      "        [-172.7219, -296.3451],\n",
      "        [-158.6992, -272.9330],\n",
      "        [ -38.7657,  -67.8856],\n",
      "        [-191.3195, -330.1444]])\n",
      "reward -30.97303847040709\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -6.7672],\n",
      "        [-35.7833],\n",
      "        [-26.2099],\n",
      "        [-30.9730],\n",
      "        [-35.2009]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[ -33.0671,  -32.8710,  -30.4251,  -41.7388,  -52.6246,  -70.5651,\n",
      "          -61.4649,  -64.5023,  -74.8295],\n",
      "        [-192.5541, -191.0682, -179.0602, -246.0637, -308.9441, -412.8790,\n",
      "         -358.7264, -377.0005, -440.5003],\n",
      "        [-148.5275, -146.8847, -137.7778, -189.2232, -237.6153, -318.0481,\n",
      "         -276.3994, -290.2408, -338.8620],\n",
      "        [-203.9322, -202.1149, -189.5599, -260.4773, -327.0395, -437.1600,\n",
      "         -379.8476, -399.0946, -466.3553],\n",
      "        [-211.7136, -209.3792, -196.1723, -268.4924, -337.7162, -452.2672,\n",
      "         -393.6023, -412.8908, -482.1399]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -41.7388,  -64.5023],\n",
      "        [-179.0602, -358.7264],\n",
      "        [-146.8847, -276.3994],\n",
      "        [-189.5599, -379.8476],\n",
      "        [-196.1723, -393.6023]], grad_fn=<GatherBackward>)\n",
      "A [[ -51.969624  -51.473125  -47.917168  -65.62063   -82.68837  -110.85868\n",
      "   -96.569435 -101.26718  -117.80526 ]\n",
      " [-203.9322   -202.1149   -189.55986  -260.4773   -327.03952  -437.16003\n",
      "  -379.84756  -399.09457  -466.35532 ]\n",
      " [-151.46916  -150.10277  -140.7808   -193.67062  -243.00801  -324.92563\n",
      "  -282.16476  -296.56885  -346.35974 ]\n",
      " [-179.18471  -177.53455  -166.52148  -228.92003  -287.37305  -384.16266\n",
      "  -333.75323  -350.6805   -409.736   ]\n",
      " [-192.55406  -191.06822  -179.06017  -246.06366  -308.94412  -412.87903\n",
      "  -358.72638  -377.0005   -440.5003  ]]\n",
      "J tensor([ -47.9172, -189.5599, -140.7808, -166.5215, -179.0602])\n",
      "P tensor([ -96.5694, -379.8476, -282.1648, -333.7532, -358.7264])\n",
      "expected_state_action_values  tensor([[ -49.8927,  -93.6797],\n",
      "        [-206.3872, -377.6461],\n",
      "        [-152.9126, -280.1582],\n",
      "        [-180.8424, -331.3510],\n",
      "        [-196.3551, -358.0547]])\n",
      "5\n",
      "reward -35.56284494188823\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.3299],\n",
      "        [-24.8327],\n",
      "        [-24.7273],\n",
      "        [-14.0164],\n",
      "        [-32.6959]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [2, 7],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-104.2488, -114.9083, -110.5674, -142.1801, -166.5496, -223.0128,\n",
      "         -183.7128, -213.0527, -237.9011],\n",
      "        [-173.5788, -191.5155, -184.5158, -236.9983, -277.6372, -371.5392,\n",
      "         -306.0918, -355.0280, -396.5675],\n",
      "        [-167.9816, -185.2203, -178.4326, -229.2451, -268.5152, -359.4344,\n",
      "         -296.0920, -343.4080, -383.6777],\n",
      "        [-138.9707, -152.9979, -147.4035, -189.3264, -221.7929, -297.1210,\n",
      "         -244.8185, -283.8026, -316.9448],\n",
      "        [-237.5018, -261.9694, -252.4372, -324.1485, -379.7416, -508.1515,\n",
      "         -418.6730, -485.5655, -542.7502]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-142.1801, -183.7128],\n",
      "        [-184.5158, -355.0280],\n",
      "        [-167.9816, -343.4080],\n",
      "        [-152.9979, -244.8185],\n",
      "        [-252.4372, -485.5655]], grad_fn=<GatherBackward>)\n",
      "A [[-124.28057 -137.32617 -132.22322 -170.36882 -199.37483 -266.53537\n",
      "  -219.32564 -254.68153 -284.61252]\n",
      " [-156.68391 -172.8949  -166.55344 -214.03821 -250.74583 -335.44128\n",
      "  -276.3467  -320.54834 -358.1147 ]\n",
      " [-162.19728 -179.03714 -172.4301  -221.74149 -259.68008 -347.33646\n",
      "  -286.04666 -331.90402 -370.96783]\n",
      " [-104.24878 -114.90835 -110.56735 -142.18013 -166.54958 -223.0128\n",
      "  -183.71284 -213.05269 -237.90112]\n",
      " [-216.66595 -238.94347 -230.07385 -295.13983 -345.95203 -463.1106\n",
      "  -381.76904 -442.57788 -494.6049 ]]\n",
      "J tensor([-124.2806, -156.6839, -162.1973, -104.2488, -216.6660])\n",
      "P tensor([-219.3256, -276.3467, -286.0467, -183.7128, -381.7690])\n",
      "expected_state_action_values  tensor([[-131.1824, -216.7229],\n",
      "        [-165.8482, -273.5447],\n",
      "        [-170.7048, -282.1693],\n",
      "        [-107.8403, -179.3579],\n",
      "        [-227.6952, -376.2880]])\n",
      "reward -31.906900661454458\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-11.8820],\n",
      "        [-35.7833],\n",
      "        [-35.5628],\n",
      "        [-31.9069],\n",
      "        [-30.9730]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [0, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[ -80.9055,  -71.4580,  -66.5242,  -91.0930, -114.2051, -153.0063,\n",
      "         -119.3557, -139.8468, -163.1577],\n",
      "        [-203.7372, -180.7207, -168.6452, -231.5760, -289.6778, -386.7303,\n",
      "         -300.9577, -353.6561, -413.9695],\n",
      "        [-189.6089, -167.9269, -156.8473, -215.4712, -269.4834, -359.8797,\n",
      "         -280.0465, -329.0005, -385.1038],\n",
      "        [-209.0509, -184.9587, -172.9853, -237.5584, -297.0835, -396.8389,\n",
      "         -308.8030, -362.7193, -424.4155],\n",
      "        [-215.6932, -191.0908, -178.4711, -245.0555, -306.5343, -409.3297,\n",
      "         -318.5672, -374.2468, -438.1134]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -71.4580, -119.3557],\n",
      "        [-168.6452, -300.9577],\n",
      "        [-189.6089, -280.0465],\n",
      "        [-172.9853, -308.8030],\n",
      "        [-178.4711, -318.5672]], grad_fn=<GatherBackward>)\n",
      "A [[ -97.61075  -86.2008   -80.32061 -110.15086 -137.95815 -184.72078\n",
      "  -143.96474 -168.80302 -197.23788]\n",
      " [-215.69318 -191.09077 -178.47113 -245.05548 -306.53433 -409.32965\n",
      "  -318.56717 -374.24677 -438.11343]\n",
      " [-209.05095 -184.95871 -172.98534 -237.55841 -297.08353 -396.83893\n",
      "  -308.80304 -362.71933 -424.41547]\n",
      " [-192.7577  -170.4348  -159.37439 -218.72575 -273.62308 -365.67883\n",
      "  -284.6572  -334.2172  -390.95856]\n",
      " [-189.60893 -167.92691 -156.84732 -215.47122 -269.4834  -359.8797\n",
      "  -280.04654 -329.00046 -385.10376]]\n",
      "J tensor([ -80.3206, -178.4711, -172.9853, -159.3744, -156.8473])\n",
      "P tensor([-143.9647, -318.5672, -308.8030, -284.6572, -280.0465])\n",
      "expected_state_action_values  tensor([[ -84.1705, -141.4502],\n",
      "        [-196.4073, -322.4937],\n",
      "        [-191.2496, -313.4856],\n",
      "        [-175.3439, -288.0984],\n",
      "        [-172.1356, -283.0149]])\n",
      "reward -33.53409068597606\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-32.6959],\n",
      "        [-14.0164],\n",
      "        [-23.4019],\n",
      "        [-24.1008],\n",
      "        [-10.7009]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [4, 8]])\n",
      "policy_net1(state_batch) tensor([[-296.2038, -259.4462, -248.8839, -299.7050, -375.6630, -503.7611,\n",
      "         -414.6608, -460.4071, -538.2357],\n",
      "        [-172.9235, -151.2166, -145.0219, -174.6856, -218.9604, -293.9558,\n",
      "         -241.9900, -268.5461, -313.6632],\n",
      "        [-192.4081, -168.6825, -161.9161, -195.8551, -244.9948, -328.1381,\n",
      "         -269.5995, -299.8170, -350.5203],\n",
      "        [-195.3869, -171.3776, -164.4078, -198.7289, -248.6761, -333.1017,\n",
      "         -273.7525, -304.4000, -355.8410],\n",
      "        [ -72.1264,  -63.2140,  -60.3170,  -72.6527,  -91.2177, -122.5287,\n",
      "         -100.9908, -112.0036, -130.4516]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-248.8839, -460.4071],\n",
      "        [-151.2166, -241.9900],\n",
      "        [-161.9161, -269.5995],\n",
      "        [-164.4078, -273.7525],\n",
      "        [ -91.2177, -130.4516]], grad_fn=<GatherBackward>)\n",
      "A [[-270.1112   -236.59833  -226.79105  -272.8132   -342.17252  -459.02216\n",
      "  -378.04248  -419.56894  -490.39728 ]\n",
      " [-129.52066  -113.387276 -108.59706  -130.98021  -164.15538  -220.27956\n",
      "  -181.3006   -201.27122  -235.05537 ]\n",
      " [-190.6091   -166.91823  -160.14627  -193.27573  -242.0313   -324.52908\n",
      "  -266.9101   -296.51318  -346.51575 ]\n",
      " [-179.93953  -157.96881  -151.4546   -183.20164  -229.23544  -306.87457\n",
      "  -252.17091  -280.48575  -327.97302 ]\n",
      " [ -96.26587   -84.23824   -80.59801   -97.06424  -121.785515 -163.5776\n",
      "  -134.76376  -149.47253  -174.26581 ]]\n",
      "J tensor([-226.7910, -108.5971, -160.1463, -151.4546,  -80.5980])\n",
      "P tensor([-378.0425, -181.3006, -266.9101, -252.1709, -134.7638])\n",
      "expected_state_action_values  tensor([[-236.8078, -372.9341],\n",
      "        [-111.7537, -177.1869],\n",
      "        [-167.5335, -263.6210],\n",
      "        [-160.4100, -251.0546],\n",
      "        [ -83.2391, -131.9883]])\n",
      "reward -27.942404676918727\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 2.6880, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 7.4044, 6.5020, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[0.0000, 0.0000, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.5005, 6.4395, 8.1867, 7.2458, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[ -4.2900],\n",
      "        [-25.4330],\n",
      "        [-10.7009],\n",
      "        [-24.8327],\n",
      "        [-11.8820]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 7],\n",
      "        [4, 8],\n",
      "        [2, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -42.1064,  -33.7417,  -31.0812,  -42.3882,  -48.2422,  -71.4876,\n",
      "          -55.8816,  -62.4931,  -81.0716],\n",
      "        [-177.2245, -141.4137, -132.0947, -180.5825, -204.5221, -302.1363,\n",
      "         -235.2382, -263.7406, -344.2804],\n",
      "        [ -66.8038,  -53.2981,  -49.4624,  -67.3846,  -76.5711, -113.4685,\n",
      "          -88.6180,  -99.0946, -128.8639],\n",
      "        [-198.8994, -158.5665, -148.0630, -201.6511, -228.7560, -338.3677,\n",
      "         -263.8962, -295.4184, -385.3919],\n",
      "        [ -89.6172,  -71.3616,  -66.4416,  -90.4801, -102.7456, -152.2495,\n",
      "         -118.8586, -132.9064, -173.0207]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -42.3882,  -62.4931],\n",
      "        [-132.0947, -263.7406],\n",
      "        [ -76.5711, -128.8639],\n",
      "        [-148.0630, -295.4184],\n",
      "        [ -71.3616, -118.8586]], grad_fn=<GatherBackward>)\n",
      "A [[ -38.740265  -31.06147   -28.585161  -39.0308    -44.413998  -65.804245\n",
      "   -51.425655  -57.527138  -74.59227 ]\n",
      " [-175.87065  -140.28708  -131.11938  -179.40681  -203.11073  -299.976\n",
      "  -233.46968  -261.8212   -341.824   ]\n",
      " [ -89.10895   -70.95695   -66.06475   -89.97318  -102.16755  -151.39137\n",
      "  -118.18577  -132.15654  -172.04237 ]\n",
      " [-179.68698  -143.26012  -133.74414  -182.26942  -206.77396  -305.732\n",
      "  -238.43536  -266.93744  -348.30185 ]\n",
      " [-107.6181    -85.67632   -79.837036 -108.88372  -123.5264   -182.93188\n",
      "  -142.68997  -159.65773  -208.14374 ]]\n",
      "J tensor([ -28.5852, -131.1194,  -66.0648, -133.7441,  -79.8370])\n",
      "P tensor([ -51.4257, -233.4697, -118.1858, -238.4354, -142.6900])\n",
      "expected_state_action_values  tensor([[ -30.0167,  -50.5731],\n",
      "        [-143.4405, -235.5558],\n",
      "        [ -70.1592, -117.0681],\n",
      "        [-145.2024, -239.4245],\n",
      "        [ -83.7353, -140.3029]])\n",
      "reward -26.82285247249525\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[8.1998, 6.4395, 6.1390, 9.1287, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000],\n",
      "        [8.5598, 9.6592, 9.7966, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-31.9069],\n",
      "        [-38.6147],\n",
      "        [ -4.2900],\n",
      "        [-19.2270],\n",
      "        [ -7.8685]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 7],\n",
      "        [3, 7],\n",
      "        [1, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-213.7369, -186.8658, -139.5645, -202.9149, -222.5951, -364.3030,\n",
      "         -298.6747, -303.3124, -391.5252],\n",
      "        [-235.0413, -205.2855, -153.1359, -221.8490, -243.7223, -399.5986,\n",
      "         -328.0865, -332.7310, -429.4344],\n",
      "        [ -38.7149,  -34.0016,  -25.0052,  -36.2834,  -40.0041,  -65.7109,\n",
      "          -54.0657,  -54.7938,  -70.2273],\n",
      "        [ -99.7999,  -87.0998,  -64.7999,  -94.0542, -103.3282, -169.6080,\n",
      "         -139.2518, -141.1882, -181.9628],\n",
      "        [ -56.1011,  -49.1129,  -36.3219,  -52.6297,  -57.9780,  -95.2097,\n",
      "          -78.3158,  -79.3413, -101.9533]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-139.5645, -298.6747],\n",
      "        [-235.0413, -332.7310],\n",
      "        [ -36.2834,  -54.7938],\n",
      "        [ -87.0998, -139.2518],\n",
      "        [ -52.6297,  -79.3413]], grad_fn=<GatherBackward>)\n",
      "A [[-197.45735  -172.5273   -128.83313  -187.1677   -205.38837  -336.3512\n",
      "  -275.85724  -280.02225  -361.3546  ]\n",
      " [-260.71054  -227.64491  -169.93277  -246.2817   -270.45343  -443.41455\n",
      "  -363.94754  -369.16223  -476.4613  ]\n",
      " [ -35.593773  -31.278513  -22.978897  -33.3892    -36.806084  -60.445038\n",
      "   -49.71901   -50.40443   -64.565735]\n",
      " [-147.36867  -128.5327    -95.81908  -138.862    -152.54353  -250.42216\n",
      "  -205.62111  -208.44116  -268.7669  ]\n",
      " [ -61.505695  -53.788338  -39.854633  -57.766678  -63.57909  -104.44281\n",
      "   -85.85837   -86.99989  -111.80354 ]]\n",
      "J tensor([-128.8331, -169.9328,  -22.9789,  -95.8191,  -39.8546])\n",
      "P tensor([-275.8572, -363.9475,  -49.7190, -205.6211,  -85.8584])\n",
      "expected_state_action_values  tensor([[-147.8567, -280.1784],\n",
      "        [-191.5542, -366.1674],\n",
      "        [ -24.9710,  -49.0371],\n",
      "        [-105.4642, -204.2860],\n",
      "        [ -43.7377,  -85.1410]])\n",
      "reward -35.203801721497676\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.7833],\n",
      "        [-26.2099],\n",
      "        [-11.8820],\n",
      "        [-10.7009],\n",
      "        [-24.8327]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [4, 8],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-186.9882, -199.2820, -154.9207, -183.2887, -217.8595, -354.3523,\n",
      "         -276.2104, -309.9741, -381.8136],\n",
      "        [-147.3780, -156.5245, -121.7688, -143.8959, -171.0841, -278.8741,\n",
      "         -217.4948, -243.7773, -300.0109],\n",
      "        [ -75.8867,  -80.4777,  -62.4260,  -73.5204,  -87.6380, -143.1966,\n",
      "         -111.9223, -125.1882, -153.6527],\n",
      "        [ -56.2642,  -59.7581,  -46.2188,  -54.4440,  -64.9517, -106.1359,\n",
      "          -82.9967,  -92.8237, -113.7971],\n",
      "        [-165.7349, -176.0937, -136.9578, -161.4464, -192.1797, -313.3396,\n",
      "         -244.5969, -274.0015, -337.0624]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-154.9207, -276.2104],\n",
      "        [-156.5245, -217.4948],\n",
      "        [ -80.4777, -111.9223],\n",
      "        [ -64.9517, -113.7971],\n",
      "        [-136.9578, -274.0015]], grad_fn=<GatherBackward>)\n",
      "A [[-199.23045  -212.07173  -164.9816   -195.15547  -231.96846  -377.42856\n",
      "  -294.2331   -330.0868   -406.62875 ]\n",
      " [-148.59474  -158.1553   -123.02334  -145.72144  -173.07234  -281.70248\n",
      "  -219.47159  -246.29901  -303.2217  ]\n",
      " [ -92.14555   -97.718285  -75.854614  -89.50776  -106.56482  -173.9923\n",
      "  -135.86246  -152.0826   -186.96695 ]\n",
      " [ -75.4588    -80.024536  -62.07424   -73.11192   -87.14854  -142.39494\n",
      "  -111.29291  -124.486496 -152.78943 ]\n",
      " [-149.83041  -159.20796  -123.79955  -146.04465  -173.84648  -283.31876\n",
      "  -221.15694  -247.7588   -304.85    ]]\n",
      "J tensor([-164.9816, -123.0233,  -75.8546,  -62.0742, -123.7996])\n",
      "P tensor([-294.2331, -219.4716, -135.8625, -111.2929, -221.1569])\n",
      "expected_state_action_values  tensor([[-184.2667, -300.5931],\n",
      "        [-136.9309, -223.7343],\n",
      "        [ -80.1511, -134.1582],\n",
      "        [ -66.5677, -110.8645],\n",
      "        [-136.2522, -223.8739]])\n",
      "reward -30.456964409352043\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.3299],\n",
      "        [ -7.8685],\n",
      "        [-35.5628],\n",
      "        [-36.9724],\n",
      "        [-25.3215]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [3, 7],\n",
      "        [0, 6],\n",
      "        [3, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-104.5355, -101.5265,  -96.2649, -100.5986, -133.4052, -197.0201,\n",
      "         -162.8662, -164.6166, -198.5612],\n",
      "        [ -52.6508,  -51.2328,  -48.2892,  -50.3444,  -66.9771,  -99.0191,\n",
      "          -82.0392,  -82.8074,  -99.5529],\n",
      "        [-185.0272, -180.1551, -171.1371, -179.2703, -237.3522, -349.6432,\n",
      "         -288.6581, -292.2278, -353.0959],\n",
      "        [-155.9996, -151.6923, -144.0211, -150.4115, -199.4806, -294.2482,\n",
      "         -243.2355, -245.9359, -296.8288],\n",
      "        [-188.7837, -183.8475, -174.7008, -183.0932, -242.3001, -356.9333,\n",
      "         -294.5677, -298.2941, -360.3640]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-100.5986, -162.8662],\n",
      "        [ -50.3444,  -82.8074],\n",
      "        [-185.0272, -288.6581],\n",
      "        [-150.4115, -245.9359],\n",
      "        [-174.7008, -294.5677]], grad_fn=<GatherBackward>)\n",
      "A [[-123.307686 -120.06119  -113.92059  -119.41927  -158.09851  -233.00246\n",
      "  -192.35263  -194.72563  -235.11359 ]\n",
      " [ -57.63779   -56.035805  -52.910934  -55.185783  -73.351944 -108.47038\n",
      "   -89.81364   -90.67668  -109.0187  ]\n",
      " [-203.14442  -197.61836  -187.96252  -196.82265  -260.5754   -383.95215\n",
      "  -316.9799   -320.8368   -387.49847 ]\n",
      " [-217.67513  -211.73111  -201.35022  -210.68459  -279.033    -411.22665\n",
      "  -339.58798  -343.64618  -415.0622  ]\n",
      " [-167.7852   -163.02414  -154.78813  -161.58292  -214.26997  -316.307\n",
      "  -261.4535   -264.31503  -319.02716 ]]\n",
      "J tensor([-113.9206,  -52.9109, -187.9625, -201.3502, -154.7881])\n",
      "P tensor([-192.3526,  -89.8136, -316.9799, -339.5880, -261.4535])\n",
      "expected_state_action_values  tensor([[-121.8584, -192.4472],\n",
      "        [ -55.4883,  -88.7008],\n",
      "        [-204.7291, -320.8447],\n",
      "        [-218.1876, -342.6016],\n",
      "        [-164.6308, -260.6296]])\n",
      "reward -36.49068732332063\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[1.4266, 0.0000, 3.3406, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 2.6880, 7.8173, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [8.5598, 9.6592, 9.7966, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [8.1998, 6.4395, 6.1390, 9.1287, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 7.1502, 6.6508, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -6.7672],\n",
      "        [-21.4316],\n",
      "        [-38.6147],\n",
      "        [-31.9069],\n",
      "        [-24.7273]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [0, 7],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[ -39.4499,  -34.8834,  -29.3727,  -36.8621,  -45.4427,  -67.2401,\n",
      "          -58.4123,  -58.8554,  -67.3752],\n",
      "        [-147.4059, -129.8214, -110.6953, -139.0527, -170.7189, -251.9853,\n",
      "         -218.2299, -220.2840, -253.8248],\n",
      "        [-266.2959, -234.2890, -199.9568, -250.2745, -307.6864, -454.4528,\n",
      "         -394.0072, -397.3276, -458.0255],\n",
      "        [-244.8754, -215.6505, -184.2505, -231.3588, -284.0104, -418.8522,\n",
      "         -362.7017, -366.1791, -422.1966],\n",
      "        [-200.1960, -176.1503, -150.2602, -188.1848, -231.3040, -341.7087,\n",
      "         -296.2115, -298.7359, -344.2244]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -36.8621,  -58.8554],\n",
      "        [-147.4059, -220.2840],\n",
      "        [-266.2959, -397.3276],\n",
      "        [-184.2505, -362.7017],\n",
      "        [-200.1960, -298.7359]], grad_fn=<GatherBackward>)\n",
      "A [[ -62.681793  -55.244286  -46.794262  -58.57364   -72.18157  -106.782875\n",
      "   -92.74924   -93.41715  -107.25199 ]\n",
      " [-158.52516  -139.74976  -119.24015  -150.14973  -184.15353  -271.41937\n",
      "  -234.85457  -237.28514  -273.63538 ]\n",
      " [-295.39804  -259.83704  -221.89856  -277.83813  -341.4485   -504.3013\n",
      "  -437.10654  -440.85782  -508.207   ]\n",
      " [-225.87651  -198.80551  -169.83466  -213.10573  -261.6937   -386.14233\n",
      "  -334.4806   -337.56177  -389.0882  ]\n",
      " [-193.51517  -170.43495  -145.33861  -182.24301  -223.93413  -330.52234\n",
      "  -286.43015  -289.00568  -333.17075 ]]\n",
      "J tensor([ -46.7943, -119.2402, -221.8986, -169.8347, -145.3386])\n",
      "P tensor([ -92.7492, -234.8546, -437.1065, -334.4806, -286.4301])\n",
      "expected_state_action_values  tensor([[ -48.8821,  -90.2416],\n",
      "        [-128.7477, -232.8007],\n",
      "        [-238.3234, -432.0106],\n",
      "        [-184.7581, -332.9394],\n",
      "        [-155.5320, -282.5144]])\n",
      "reward -28.137941453783206\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-36.4907],\n",
      "        [-25.3215],\n",
      "        [-35.2038],\n",
      "        [-10.7610],\n",
      "        [-24.1008]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-181.3982, -177.1485, -168.5606, -205.3136, -235.1902, -344.5588,\n",
      "         -284.5049, -314.7303, -347.2968],\n",
      "        [-190.5023, -186.3729, -177.2317, -216.2280, -247.5182, -362.2740,\n",
      "         -298.8932, -330.9659, -365.4682],\n",
      "        [-159.7990, -156.1916, -148.5296, -181.0593, -207.4001, -303.6654,\n",
      "         -250.7098, -277.4216, -306.1875],\n",
      "        [ -78.4702,  -76.5316,  -72.5344,  -88.1501, -101.1680, -148.6213,\n",
      "         -122.9539, -135.7527, -149.3854],\n",
      "        [-156.2444, -152.8651, -145.3062, -177.3453, -202.9835, -297.1595,\n",
      "         -245.1484, -271.4618, -299.6510]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-168.5606, -284.5049],\n",
      "        [-177.2317, -298.8932],\n",
      "        [-148.5296, -250.7098],\n",
      "        [ -76.5316, -122.9539],\n",
      "        [-145.3062, -245.1484]], grad_fn=<GatherBackward>)\n",
      "A [[-213.51305 -208.42139 -198.55724 -242.061   -277.10565 -405.8679\n",
      "  -334.94028 -370.65167 -409.10513]\n",
      " [-169.33192 -165.2874  -157.05199 -190.96849 -218.92325 -321.0878\n",
      "  -265.3244  -293.30533 -323.60052]\n",
      " [-191.00525 -186.91821 -177.92264 -217.30682 -248.63768 -363.6575\n",
      "  -299.89682 -332.21844 -366.8233 ]\n",
      " [ -78.9213   -76.97099  -72.95093  -88.65012 -101.74438 -149.46959\n",
      "  -123.65839 -136.52838 -150.24107]\n",
      " [-144.09996 -141.13928 -134.06879 -163.77644 -187.44402 -274.2017\n",
      "  -226.1726  -250.54147 -276.66116]]\n",
      "J tensor([-198.5572, -157.0520, -177.9226,  -72.9509, -134.0688])\n",
      "P tensor([-334.9403, -265.3244, -299.8968, -123.6584, -226.1726])\n",
      "expected_state_action_values  tensor([[-215.1922, -337.9369],\n",
      "        [-166.6683, -264.1135],\n",
      "        [-195.3342, -305.1109],\n",
      "        [ -76.4168, -122.0535],\n",
      "        [-144.7627, -227.6562]])\n",
      "5\n",
      "reward -27.467291724201168\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-10.7610],\n",
      "        [-19.2270],\n",
      "        [ -4.6880],\n",
      "        [-32.2255],\n",
      "        [-28.1379]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[ -77.8598,  -72.2302,  -79.0168,  -87.6625, -100.5011, -147.5965,\n",
      "         -116.2727, -134.8013, -148.3921],\n",
      "        [ -94.7649,  -87.8957,  -96.2414, -106.9260, -122.4694, -179.7431,\n",
      "         -141.4650, -164.1312, -180.9801],\n",
      "        [ -29.6857,  -27.7696,  -29.9860,  -33.4209,  -38.3715,  -56.3413,\n",
      "          -44.4321,  -51.5226,  -56.4402],\n",
      "        [-141.2834, -131.6185, -144.3338, -160.9979, -184.0470, -269.1247,\n",
      "         -211.3096, -245.8735, -271.6220],\n",
      "        [-211.4616, -196.2877, -215.9130, -240.3286, -274.8095, -402.3601,\n",
      "         -316.0962, -367.4035, -405.6922]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -72.2302, -116.2727],\n",
      "        [ -87.8957, -141.4650],\n",
      "        [ -33.4209,  -51.5226],\n",
      "        [-144.3338, -211.3096],\n",
      "        [-215.9130, -316.0962]], grad_fn=<GatherBackward>)\n",
      "A [[ -78.310394  -72.64773   -79.47345   -88.163055 -101.07753  -148.44443\n",
      "  -116.94341  -135.57658  -149.24773 ]\n",
      " [-139.1611   -128.97983  -141.4868   -156.99727  -179.81506  -263.9094\n",
      "  -207.70282  -240.97397  -265.8216  ]\n",
      " [ -36.892715  -34.448128  -37.290417  -41.427578  -47.59127   -69.90453\n",
      "   -55.1597    -63.92381   -70.12624 ]\n",
      " [-186.60905  -173.631    -190.62431  -212.36249  -242.80716  -355.24033\n",
      "  -278.99658  -324.50064  -358.47974 ]\n",
      " [-171.42975  -159.21771  -175.00336  -194.93335  -222.9429   -326.23422\n",
      "  -256.31747  -297.93866  -329.08417 ]]\n",
      "J tensor([ -72.6477, -128.9798,  -34.4481, -173.6310, -159.2177])\n",
      "P tensor([-116.9434, -207.7028,  -55.1597, -278.9966, -256.3175])\n",
      "expected_state_action_values  tensor([[ -76.1439, -116.0101],\n",
      "        [-135.3089, -206.1596],\n",
      "        [ -35.6913,  -54.3317],\n",
      "        [-188.4934, -283.3224],\n",
      "        [-171.4339, -258.8237]])\n",
      "1\n",
      "reward -32.9790013030307\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.7833],\n",
      "        [-35.2038],\n",
      "        [-19.3299],\n",
      "        [-25.6968],\n",
      "        [-28.1379]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [3, 6],\n",
      "        [3, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-201.7161, -205.5536, -188.1997, -244.4374, -262.2187, -383.5596,\n",
      "         -305.0018, -365.3065, -387.8534],\n",
      "        [-161.7809, -164.5031, -150.7690, -195.7816, -210.0490, -307.4931,\n",
      "         -244.6021, -292.7434, -310.5796],\n",
      "        [-106.8907, -108.5219,  -99.2399, -128.6925, -138.1674, -202.7017,\n",
      "         -161.4119, -192.9275, -204.4829],\n",
      "        [-133.6897, -136.0997, -124.6786, -162.3513, -173.9980, -254.5188,\n",
      "         -202.2170, -242.2794, -257.1609],\n",
      "        [-212.8392, -216.1574, -198.4577, -257.7564, -276.3590, -404.6975,\n",
      "         -321.7556, -385.1463, -408.6208]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-188.1997, -305.0018],\n",
      "        [-150.7690, -244.6021],\n",
      "        [-128.6925, -161.4119],\n",
      "        [-162.3513, -202.2170],\n",
      "        [-198.4577, -321.7556]], grad_fn=<GatherBackward>)\n",
      "A [[-214.54654 -218.3701  -200.07812 -259.82144 -278.72168 -407.82596\n",
      "  -324.33292 -388.3409  -412.3326 ]\n",
      " [-190.39046 -193.83752 -177.82707 -231.37344 -247.96582 -362.5983\n",
      "  -288.069   -345.19528 -366.38162]\n",
      " [-125.12783 -127.3439  -116.54095 -151.50754 -162.49294 -237.896\n",
      "  -189.16754 -226.47856 -240.27405]\n",
      " [-154.18517 -156.89153 -143.74696 -186.88441 -200.35149 -293.28897\n",
      "  -233.11807 -279.18698 -296.22217]\n",
      " [-173.38939 -176.18144 -161.63776 -210.06714 -225.28282 -329.7211\n",
      "  -262.18152 -313.83862 -333.0628 ]]\n",
      "J tensor([-200.0781, -177.8271, -116.5409, -143.7470, -161.6378])\n",
      "P tensor([-324.3329, -288.0690, -189.1675, -233.1181, -262.1815])\n",
      "expected_state_action_values  tensor([[-215.8536, -327.6829],\n",
      "        [-195.2482, -294.4659],\n",
      "        [-124.2167, -189.5806],\n",
      "        [-155.0691, -235.5031],\n",
      "        [-173.6119, -264.1013]])\n",
      "5\n",
      "reward -34.116470611492105\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[ -6.7672],\n",
      "        [-25.3215],\n",
      "        [-36.9724],\n",
      "        [-35.2009],\n",
      "        [-27.4673]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 6],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -37.2595,  -37.9884,  -37.6170,  -41.4745,  -47.9171,  -70.3947,\n",
      "          -59.2206,  -67.1459,  -70.5388],\n",
      "        [-200.0161, -203.2893, -203.8794, -225.3187, -259.1780, -379.3713,\n",
      "         -317.8283, -361.5894, -382.3845],\n",
      "        [-168.5712, -171.0845, -171.4372, -188.9059, -217.6475, -318.9940,\n",
      "         -267.6671, -304.0696, -321.2883],\n",
      "        [-234.4586, -237.8246, -238.2151, -261.9682, -302.0217, -442.9875,\n",
      "         -371.9345, -422.3008, -446.3689],\n",
      "        [-183.2388, -185.9384, -186.6557, -206.2279, -237.2972, -347.4162,\n",
      "         -291.1750, -331.0673, -350.0360]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -41.4745,  -67.1459],\n",
      "        [-203.8794, -317.8283],\n",
      "        [-188.9059, -304.0696],\n",
      "        [-238.2151, -371.9345],\n",
      "        [-185.9384, -291.1750]], grad_fn=<GatherBackward>)\n",
      "A [[ -58.77201   -59.70961   -59.444767  -65.408585  -75.53638  -110.94716\n",
      "   -93.304344 -105.78798  -111.41825 ]\n",
      " [-177.95456  -180.47318  -180.84993  -199.20595  -229.47461  -336.57706\n",
      "  -282.40244  -320.76398  -338.92407 ]\n",
      " [-232.35732  -235.88554  -236.7284   -261.2601   -300.7005   -440.3426\n",
      "  -369.12933  -419.68787  -443.72562 ]\n",
      " [-214.86995  -218.63702  -219.0501   -241.92995  -278.4305   -407.39392\n",
      "  -341.428    -388.44244  -410.90228 ]\n",
      " [-204.52473  -207.32227  -207.94736  -229.24406  -264.022    -386.99323\n",
      "  -324.6359   -368.78796  -389.94034 ]]\n",
      "J tensor([ -58.7720, -177.9546, -232.3573, -214.8699, -204.5247])\n",
      "P tensor([ -93.3043, -282.4024, -369.1293, -341.4280, -324.6359])\n",
      "expected_state_action_values  tensor([[ -59.6621,  -90.7412],\n",
      "        [-185.4806, -279.4837],\n",
      "        [-246.0940, -369.1888],\n",
      "        [-228.5839, -342.4861],\n",
      "        [-211.5395, -319.6396]])\n",
      "reward -38.66569817411958\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-36.9989],\n",
      "        [-11.8820],\n",
      "        [-14.0164],\n",
      "        [-33.9160],\n",
      "        [ -7.8685]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-255.3809, -281.3528, -238.3353, -308.6027, -330.9041, -485.1157,\n",
      "         -385.5935, -480.1335, -490.5680],\n",
      "        [ -82.2911,  -90.6022,  -76.4747,  -98.9643, -106.2597, -156.0766,\n",
      "         -124.2752, -154.4634, -157.3617],\n",
      "        [-144.4723, -158.9645, -134.5046, -174.0965, -186.7532, -274.1397,\n",
      "         -218.0503, -271.2562, -276.8712],\n",
      "        [-227.3437, -250.8367, -212.5423, -275.7734, -295.5261, -432.6564,\n",
      "         -343.6090, -428.2616, -437.6829],\n",
      "        [ -58.5312,  -64.5847,  -54.2975,  -70.2716,  -75.5479, -110.9379,\n",
      "          -88.4350, -109.8577, -111.8351]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-255.3809, -480.1335],\n",
      "        [ -90.6022, -124.2752],\n",
      "        [-158.9645, -218.0503],\n",
      "        [-227.3437, -428.2616],\n",
      "        [ -70.2716, -109.8577]], grad_fn=<GatherBackward>)\n",
      "A [[-247.7508   -273.1109   -231.1887   -299.2411   -320.98065  -470.48233\n",
      "  -374.0691   -465.75092  -475.9354  ]\n",
      " [ -99.456276 -109.49271   -92.48874  -119.83956  -128.57141  -188.73381\n",
      "  -150.14403  -186.755    -190.54764 ]\n",
      " [-109.67113  -120.82184  -102.09013  -132.32314  -141.93414  -208.23389\n",
      "  -165.60097  -206.07315  -210.33168 ]\n",
      " [-233.0272   -256.7589   -217.37646  -281.36478  -301.80286  -442.46597\n",
      "  -351.81226  -437.97177  -447.50168 ]\n",
      " [ -61.984276  -68.34791   -57.53543   -74.48484   -80.02246  -117.54583\n",
      "   -93.647194 -116.3655   -118.437836]]\n",
      "J tensor([-231.1887,  -92.4887, -102.0901, -217.3765,  -57.5354])\n",
      "P tensor([-374.0691, -150.1440, -165.6010, -351.8123,  -93.6472])\n",
      "expected_state_action_values  tensor([[-245.0687, -373.6610],\n",
      "        [ -95.1218, -147.0116],\n",
      "        [-105.8975, -163.0572],\n",
      "        [-229.5548, -350.5471],\n",
      "        [ -59.6504,  -92.1510]])\n",
      "5\n",
      "reward -36.90974836995631\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-38.6147],\n",
      "        [-32.6959],\n",
      "        [-35.5628],\n",
      "        [-33.5341],\n",
      "        [-24.8327]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [2, 7],\n",
      "        [0, 6],\n",
      "        [2, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-194.2469, -220.8920, -201.4679, -245.5558, -280.0296, -410.2371,\n",
      "         -310.6811, -390.8622, -415.4008],\n",
      "        [-206.3206, -234.7531, -214.0752, -260.9215, -297.5539, -435.8106,\n",
      "         -330.0266, -415.2745, -441.3931],\n",
      "        [-162.7941, -185.6254, -169.2851, -207.1055, -235.8633, -344.8535,\n",
      "         -260.7575, -328.6067, -349.4855],\n",
      "        [-164.1741, -186.8852, -170.6149, -208.4941, -237.5032, -347.5586,\n",
      "         -262.9129, -331.1004, -351.8517],\n",
      "        [-148.6975, -169.2630, -154.3042, -188.1632, -214.5585, -314.2686,\n",
      "         -237.9778, -299.4511, -318.0185]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-194.2469, -390.8622],\n",
      "        [-214.0752, -415.2745],\n",
      "        [-162.7941, -260.7575],\n",
      "        [-170.6149, -262.9129],\n",
      "        [-154.3042, -299.4511]], grad_fn=<GatherBackward>)\n",
      "A [[-212.5708  -241.69725 -220.59189 -268.97195 -306.61133 -449.16647\n",
      "  -340.03275 -427.89987 -454.74783]\n",
      " [-190.38191 -216.56189 -197.31992 -240.20227 -274.11548 -401.67715\n",
      "  -304.3985  -382.79675 -406.78778]\n",
      " [-177.15724 -201.82997 -184.29172 -225.39076 -256.65735 -375.36047\n",
      "  -283.81012 -357.61087 -380.14975]\n",
      " [-160.2063  -182.68562 -166.78526 -204.20059 -232.42558 -339.76758\n",
      "  -256.77386 -323.7089  -344.10425]\n",
      " [-137.83751 -156.91554 -143.01935 -174.49771 -198.99171 -291.35107\n",
      "  -220.63559 -277.62662 -294.92896]]\n",
      "J tensor([-212.5708, -190.3819, -177.1572, -160.2063, -137.8375])\n",
      "P tensor([-340.0327, -304.3985, -283.8101, -256.7739, -220.6356])\n",
      "expected_state_action_values  tensor([[-229.9284, -344.6441],\n",
      "        [-204.0396, -306.6545],\n",
      "        [-195.0044, -290.9919],\n",
      "        [-177.7198, -264.6306],\n",
      "        [-148.8864, -223.4047]])\n",
      "reward -34.3329009899173\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-24.8327],\n",
      "        [-30.9730],\n",
      "        [-10.7009],\n",
      "        [-27.9424],\n",
      "        [-26.8229]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [2, 6],\n",
      "        [4, 8],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-157.3182, -160.1484, -133.2409, -177.7463, -202.9365, -298.4486,\n",
      "         -237.5501, -273.0963, -301.2701],\n",
      "        [-196.7180, -200.5843, -166.9532, -223.3502, -254.7213, -373.8938,\n",
      "         -297.2170, -342.1845, -378.1107],\n",
      "        [ -55.2250,  -56.2298,  -46.4678,  -61.9968,  -70.9160, -104.5187,\n",
      "          -83.3910,  -95.6390, -105.1480],\n",
      "        [-170.6402, -173.9259, -144.9273, -194.1208, -221.2271, -324.7123,\n",
      "         -257.9677, -297.0850, -328.0552],\n",
      "        [-152.0389, -154.9326, -129.0056, -172.7015, -196.9303, -289.0952,\n",
      "         -229.8055, -264.5225, -292.0975]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-133.2409, -273.0963],\n",
      "        [-166.9532, -297.2170],\n",
      "        [ -70.9160, -105.1480],\n",
      "        [-144.9273, -257.9677],\n",
      "        [-129.0056, -229.8055]], grad_fn=<GatherBackward>)\n",
      "A [[-146.42421  -149.06075  -123.988495 -165.49673  -188.96866  -277.79523\n",
      "  -221.12819  -254.20808  -280.52805 ]\n",
      " [-173.83292  -177.18965  -147.47926  -197.38367  -225.07669  -330.42105\n",
      "  -262.64392  -302.36096  -334.0618  ]\n",
      " [ -72.849495  -74.03846   -61.390965  -81.86379   -93.57959  -137.9178\n",
      "  -109.96346  -126.158966 -138.84018 ]\n",
      " [-152.03894  -154.93259  -129.00558  -172.70154  -196.93034  -289.09518\n",
      "  -229.8055   -264.52252  -292.09747 ]\n",
      " [-150.08028  -152.86008  -127.260025 -170.23163  -194.18228  -285.1848\n",
      "  -226.78468  -260.94016  -288.09158 ]]\n",
      "J tensor([-123.9885, -147.4793,  -61.3910, -129.0056, -127.2600])\n",
      "P tensor([-221.1282, -262.6439, -109.9635, -229.8055, -226.7847])\n",
      "expected_state_action_values  tensor([[-136.4223, -223.8480],\n",
      "        [-163.7044, -267.3525],\n",
      "        [ -65.9528, -109.6680],\n",
      "        [-144.0474, -234.7673],\n",
      "        [-141.3569, -230.9291]])\n",
      "reward -39.93053535286582\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-34.1165],\n",
      "        [-32.9790],\n",
      "        [ -6.7672],\n",
      "        [ -7.8685],\n",
      "        [ -4.2900]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [0, 7],\n",
      "        [3, 7],\n",
      "        [3, 7],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-180.7076, -183.8355, -138.3370, -205.2864, -212.0985, -343.3621,\n",
      "         -260.2164, -301.6338, -369.1092],\n",
      "        [-167.3764, -170.1538, -127.9401, -189.5989, -196.0521, -317.5804,\n",
      "         -240.9077, -279.0099, -341.4099],\n",
      "        [ -31.3681,  -32.0998,  -23.7329,  -35.2385,  -36.5610,  -59.3697,\n",
      "          -45.2388,  -52.2216,  -63.4335],\n",
      "        [ -49.4732,  -50.4236,  -37.5498,  -55.5749,  -57.6334,  -93.5791,\n",
      "          -71.2486,  -82.2745, -100.2326],\n",
      "        [ -34.0150,  -34.7861,  -25.7475,  -38.1781,  -39.6180,  -64.3456,\n",
      "          -49.0355,  -56.5990,  -68.7850]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-180.7076, -260.2164],\n",
      "        [-167.3764, -279.0099],\n",
      "        [ -35.2385,  -52.2216],\n",
      "        [ -55.5749,  -82.2745],\n",
      "        [ -38.1781,  -56.5990]], grad_fn=<GatherBackward>)\n",
      "A [[-189.29993  -192.38141  -144.83604  -214.59727  -221.8537   -359.37518\n",
      "  -272.55026  -315.69452  -386.15506 ]\n",
      " [-180.70755  -183.83548  -138.33702  -205.28638  -212.09846  -343.36212\n",
      "  -260.21637  -301.6338   -369.1092  ]\n",
      " [ -49.473152  -50.423607  -37.549763  -55.574886  -57.633373  -93.579056\n",
      "   -71.24857   -82.27454  -100.232635]\n",
      " [ -51.871178  -52.822906  -39.398453  -58.32896   -60.44053   -98.18134\n",
      "   -74.70074   -86.28859  -105.09137 ]\n",
      " [ -31.368137  -32.09975   -23.732851  -35.238457  -36.560986  -59.3697\n",
      "   -45.23877   -52.221603  -63.43349 ]]\n",
      "J tensor([-144.8360, -138.3370,  -37.5498,  -39.3985,  -23.7329])\n",
      "P tensor([-272.5503, -260.2164,  -71.2486,  -74.7007,  -45.2388])\n",
      "expected_state_action_values  tensor([[-164.4689, -279.4117],\n",
      "        [-157.4823, -267.1737],\n",
      "        [ -40.5620,  -70.8910],\n",
      "        [ -43.3271,  -75.0992],\n",
      "        [ -25.6496,  -45.0049]])\n",
      "reward -38.97819213244341\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-30.9730],\n",
      "        [-10.7610],\n",
      "        [-27.9424],\n",
      "        [-35.7833],\n",
      "        [-31.9069]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-151.2274, -171.2639, -128.9539, -179.8374, -198.6236, -319.5731,\n",
      "         -253.6515, -269.6536, -344.2332],\n",
      "        [ -57.6080,  -64.9503,  -48.6884,  -67.6232,  -74.8713, -121.0946,\n",
      "          -96.4380, -102.1246, -129.8035],\n",
      "        [-130.5815, -147.8429, -111.4650, -155.6824, -171.7945, -276.3785,\n",
      "         -219.2153, -233.1228, -297.3783],\n",
      "        [-140.1838, -159.0250, -119.6294, -166.9052, -184.3247, -296.4183,\n",
      "         -235.2264, -250.1824, -319.3445],\n",
      "        [-145.0576, -164.0494, -123.7071, -172.5387, -190.4962, -306.6569,\n",
      "         -243.3627, -258.6530, -329.9644]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-128.9539, -253.6515],\n",
      "        [ -64.9503,  -96.4380],\n",
      "        [-111.4650, -219.2153],\n",
      "        [-119.6294, -235.2264],\n",
      "        [-123.7071, -243.3627]], grad_fn=<GatherBackward>)\n",
      "A [[-133.92178  -151.60466  -114.14803  -159.27449  -175.88095  -283.02454\n",
      "  -224.62613  -238.77972  -304.77826 ]\n",
      " [ -57.945145  -65.32942   -48.9732    -68.012634  -75.304756 -121.79719\n",
      "   -96.9997   -102.71796  -130.55988 ]\n",
      " [-116.93178  -132.33655   -99.686356 -139.13618  -153.63977  -247.23126\n",
      "  -196.22844  -208.56052  -266.04752 ]\n",
      " [-151.2274   -171.26393  -128.95395  -179.83742  -198.62363  -319.57312\n",
      "  -253.65154  -269.65356  -344.23322 ]\n",
      " [-135.04247  -152.57648  -115.02631  -160.26888  -177.02176  -285.19528\n",
      "  -226.44574  -240.53058  -306.7368  ]]\n",
      "J tensor([-114.1480,  -48.9732,  -99.6864, -128.9539, -115.0263])\n",
      "P tensor([-224.6261,  -96.9997, -196.2284, -253.6515, -226.4457])\n",
      "expected_state_action_values  tensor([[-133.7063, -233.1366],\n",
      "        [ -54.8369,  -98.0607],\n",
      "        [-117.6601, -204.5480],\n",
      "        [-151.8418, -264.0697],\n",
      "        [-135.4306, -235.7081]])\n",
      "reward -37.98967465230477\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-36.9989],\n",
      "        [-14.0164],\n",
      "        [-35.2038],\n",
      "        [ -4.2900],\n",
      "        [-33.9160]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [3, 7],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-175.9610, -183.7143, -166.2147, -209.4716, -231.1221, -372.2317,\n",
      "         -281.8896, -313.9945, -400.2978],\n",
      "        [-100.8136, -105.0690,  -94.9168, -119.5441, -131.9598, -212.8969,\n",
      "         -161.3798, -179.5290, -228.6163],\n",
      "        [-116.5463, -121.9608, -110.2918, -139.4539, -153.7648, -247.1549,\n",
      "         -187.0258, -208.5268, -265.8510],\n",
      "        [ -29.1281,  -30.6009,  -27.2437,  -34.3215,  -38.0176,  -61.3852,\n",
      "          -46.7213,  -51.8459,  -65.6513],\n",
      "        [-155.3248, -162.5031, -147.1004, -185.9209, -204.9574, -329.4520,\n",
      "         -249.2240, -277.9464, -354.4428]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-175.9610, -313.9945],\n",
      "        [-105.0690, -161.3798],\n",
      "        [-110.2918, -187.0258],\n",
      "        [ -34.3215,  -51.8459],\n",
      "        [-155.3248, -277.9464]], grad_fn=<GatherBackward>)\n",
      "A [[-171.58531  -179.30525  -162.05     -204.10634  -225.32208  -362.80704\n",
      "  -274.86322  -306.13693  -390.33636 ]\n",
      " [ -77.023186  -80.413605  -72.51035   -91.48658  -100.97324  -162.77469\n",
      "  -123.36914  -137.29103  -174.8246  ]\n",
      " [-133.23729  -139.71333  -126.51134  -160.45416  -176.63332  -283.44632\n",
      "  -214.09615  -239.13667  -305.00705 ]\n",
      " [ -26.833628  -28.2123    -25.091959  -31.650919  -35.051098  -56.584587\n",
      "   -43.06296   -47.79105   -60.48451 ]\n",
      " [-161.69296  -168.84872  -152.63477  -192.2517   -212.22765  -341.82095\n",
      "  -258.98096  -288.38754  -367.6702  ]]\n",
      "J tensor([-162.0500,  -72.5104, -126.5113,  -25.0920, -152.6348])\n",
      "P tensor([-274.8632, -123.3691, -214.0961,  -43.0630, -258.9810])\n",
      "expected_state_action_values  tensor([[-182.8439, -284.3758],\n",
      "        [ -79.2757, -125.0486],\n",
      "        [-149.0640, -227.8903],\n",
      "        [ -26.8728,  -43.0467],\n",
      "        [-171.2873, -266.9989]])\n",
      "5\n",
      "reward -40.35887486071309\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 10.5880, 11.7574,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 0.0000,  0.0000,  1.5828,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[ -3.7268],\n",
      "        [-32.2255],\n",
      "        [-38.9782],\n",
      "        [-21.4316],\n",
      "        [-30.4570]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[ -24.0425,  -20.8168,  -22.1065,  -23.8223,  -28.2049,  -45.7723,\n",
      "          -36.3813,  -36.9388,  -48.7201],\n",
      "        [-107.8055,  -92.7425, -100.4453, -108.5454, -127.8012, -206.3792,\n",
      "         -163.0323, -166.3904, -221.5149],\n",
      "        [-189.4223, -162.1876, -176.1815, -189.3928, -223.4070, -361.5587,\n",
      "         -286.0804, -291.3981, -387.8703],\n",
      "        [ -92.8353,  -79.6484,  -86.2456,  -92.9694, -109.5605, -177.3401,\n",
      "         -140.2429, -142.9165, -190.0573],\n",
      "        [-141.9200, -121.9199, -132.4493, -143.1338, -168.4415, -271.9451,\n",
      "         -214.7245, -219.1940, -291.7870]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -23.8223,  -36.9388],\n",
      "        [-100.4453, -163.0323],\n",
      "        [-176.1815, -286.0804],\n",
      "        [ -92.8353, -142.9165],\n",
      "        [-132.4493, -214.7245]], grad_fn=<GatherBackward>)\n",
      "A [[ -24.914692  -21.562868  -22.911573  -24.672121  -29.215893  -47.418312\n",
      "   -37.69245   -38.26738   -50.48683 ]\n",
      " [-140.15211  -120.387535 -130.58894  -140.90594  -165.93488  -268.15625\n",
      "  -211.86427  -216.15646  -287.7546  ]\n",
      " [-189.85281  -162.66252  -176.57812  -189.80066  -223.96222  -362.31448\n",
      "  -286.73965  -292.06946  -388.88034 ]\n",
      " [-100.60052   -86.41475   -93.6858   -101.330795 -119.25529  -192.61804\n",
      "  -152.13028  -155.23938  -206.67238 ]\n",
      " [-137.36436  -117.704605 -127.78188  -137.5166   -162.13298  -262.38254\n",
      "  -207.54068  -211.45589  -281.28204 ]]\n",
      "J tensor([ -21.5629, -120.3875, -162.6625,  -86.4147, -117.7046])\n",
      "P tensor([ -37.6925, -211.8643, -286.7397, -152.1303, -207.5407])\n",
      "expected_state_action_values  tensor([[ -23.1334,  -37.6500],\n",
      "        [-140.5743, -222.9034],\n",
      "        [-185.3745, -297.0439],\n",
      "        [ -99.2048, -158.3488],\n",
      "        [-136.3911, -217.2436]])\n",
      "5\n",
      "reward -40.41966993021767\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-27.9424],\n",
      "        [-28.1379],\n",
      "        [-35.5628],\n",
      "        [-23.4019],\n",
      "        [ -7.8685]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [0, 6],\n",
      "        [2, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-167.0490, -131.2414, -154.6177, -142.7818, -180.8242, -292.5292,\n",
      "         -241.5713, -246.0845, -313.4464],\n",
      "        [-188.7429, -147.9465, -174.5322, -160.9734, -203.9157, -330.2576,\n",
      "         -272.7964, -277.7169, -353.6671],\n",
      "        [-171.2154, -134.5627, -158.2854, -146.0071, -185.0650, -299.4636,\n",
      "         -247.4452, -251.9710, -321.1280],\n",
      "        [-134.1670, -105.4022, -124.0999, -114.7515, -145.2215, -234.9999,\n",
      "         -193.9897, -197.6431, -251.7395],\n",
      "        [ -52.2960,  -41.1671,  -47.9774,  -43.9627,  -56.0072,  -91.0164,\n",
      "          -75.5542,  -76.6125,  -97.1718]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-154.6177, -241.5713],\n",
      "        [-174.5322, -272.7964],\n",
      "        [-171.2154, -247.4452],\n",
      "        [-124.0999, -193.9897],\n",
      "        [ -43.9627,  -76.6125]], grad_fn=<GatherBackward>)\n",
      "A [[-149.36107  -117.335594 -138.1142   -127.43032  -161.50877  -261.33884\n",
      "  -215.95084  -219.86932  -280.05917 ]\n",
      " [-158.74072  -124.49579  -146.72385  -135.38455  -171.57176  -277.6975\n",
      "  -229.46524  -233.56544  -297.56784 ]\n",
      " [-184.73302  -145.01302  -170.85855  -157.56584  -199.67859  -323.21243\n",
      "  -267.03552  -271.8883   -346.33167 ]\n",
      " [-135.45143  -106.25739  -124.997955 -115.12711  -145.97795  -236.67014\n",
      "  -195.65315  -199.0455   -253.38387 ]\n",
      " [ -54.539898  -42.884384  -50.06883   -45.909863  -58.429214  -94.99244\n",
      "   -78.79758   -79.92764  -101.343796]]\n",
      "J tensor([-117.3356, -124.4958, -145.0130, -106.2574,  -42.8844])\n",
      "P tensor([-215.9508, -229.4652, -267.0355, -195.6532,  -78.7976])\n",
      "expected_state_action_values  tensor([[-133.5444, -222.2981],\n",
      "        [-140.1841, -234.6566],\n",
      "        [-166.0746, -275.8948],\n",
      "        [-119.0336, -199.4897],\n",
      "        [ -46.4644,  -78.7863]])\n",
      "5\n",
      "reward -40.94024316747581\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-33.5341],\n",
      "        [-32.2255],\n",
      "        [-34.1165],\n",
      "        [-36.9724],\n",
      "        [-30.4570]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [0, 6],\n",
      "        [3, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-144.6461, -123.7204, -134.5646, -144.8729, -170.5839, -275.9081,\n",
      "         -218.3097, -241.6862, -295.8242],\n",
      "        [-109.1063,  -93.6897, -101.6388, -109.8398, -129.1794, -208.5339,\n",
      "         -164.8211, -182.7361, -223.8240],\n",
      "        [-162.1073, -138.5700, -150.7373, -162.3325, -191.1079, -309.1249,\n",
      "         -244.5524, -270.7536, -331.6215],\n",
      "        [-122.5764, -104.9224, -113.8429, -122.2942, -144.2124, -233.4068,\n",
      "         -184.9174, -204.5263, -250.2565],\n",
      "        [-142.6995, -122.3674, -133.1494, -143.9007, -169.1518, -273.0000,\n",
      "         -215.6590, -239.1760, -292.9103]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-134.5646, -218.3097],\n",
      "        [-101.6388, -164.8211],\n",
      "        [-162.1073, -244.5524],\n",
      "        [-122.2942, -204.5263],\n",
      "        [-133.1494, -215.6590]], grad_fn=<GatherBackward>)\n",
      "A [[-140.6351  -120.50505 -131.0842  -141.4732  -166.40071 -268.76974\n",
      "  -212.44269 -235.46104 -288.28802]\n",
      " [-141.59006 -121.40184 -131.90298 -142.33578 -167.42688 -270.47418\n",
      "  -213.79558 -236.9766  -290.23804]\n",
      " [-169.24272 -144.50078 -157.26721 -169.06104 -199.1945  -322.42703\n",
      "  -255.26341 -282.39648 -345.7225 ]\n",
      " [-166.01593 -142.1035  -154.55554 -166.43193 -195.95706 -316.77716\n",
      "  -250.59593 -277.5324  -339.82794]\n",
      " [-138.12225 -118.14377 -128.46661 -138.27615 -162.8339  -263.421\n",
      "  -208.4546  -230.74817 -282.38922]]\n",
      "J tensor([-120.5051, -121.4018, -144.5008, -142.1035, -118.1438])\n",
      "P tensor([-212.4427, -213.7956, -255.2634, -250.5959, -208.4546])\n",
      "expected_state_action_values  tensor([[-141.9886, -224.7325],\n",
      "        [-141.4872, -224.6415],\n",
      "        [-164.1672, -263.8535],\n",
      "        [-164.8655, -262.5087],\n",
      "        [-136.7864, -218.0661]])\n",
      "5\n",
      "reward -42.73283535949969\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  7.3475,  9.7098,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 10.1104,  9.0302,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-10.7009],\n",
      "        [-36.9097],\n",
      "        [-25.4330],\n",
      "        [ -3.7268],\n",
      "        [-40.4197]])\n",
      "action_batch tensor([[4, 8],\n",
      "        [0, 6],\n",
      "        [2, 7],\n",
      "        [3, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -55.3266,  -43.5927,  -50.8535,  -53.7646,  -59.4384,  -96.5874,\n",
      "          -80.1101,  -87.9499, -103.0612],\n",
      "        [-224.7149, -176.4001, -207.7424, -220.2231, -242.8375, -393.3899,\n",
      "         -325.1547, -358.1578, -421.5130],\n",
      "        [-135.6962, -106.8779, -125.6116, -133.6174, -147.1253, -238.0696,\n",
      "         -196.5342, -216.7631, -255.0365],\n",
      "        [ -29.2519,  -23.2229,  -26.8166,  -28.4526,  -31.4822,  -51.1195,\n",
      "          -42.4708,  -46.5704,  -54.3948],\n",
      "        [-232.7457, -182.9829, -215.2478, -228.1545, -251.6715, -407.4453,\n",
      "         -336.8217, -371.0871, -436.8629]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -59.4384, -103.0612],\n",
      "        [-224.7149, -325.1547],\n",
      "        [-125.6116, -216.7631],\n",
      "        [ -28.4526,  -46.5704],\n",
      "        [-182.9829, -336.8217]], grad_fn=<GatherBackward>)\n",
      "A [[ -72.032005  -56.60898   -66.275986  -70.07321   -77.40961  -125.79096\n",
      "  -104.24085  -114.505394 -134.30394 ]\n",
      " [-210.03185  -164.94603  -194.21996  -205.96994  -227.08492  -367.8012\n",
      "  -303.96057  -334.86914  -394.07852 ]\n",
      " [-134.82478  -106.13889  -124.846466 -132.93987  -146.31049  -236.67659\n",
      "  -195.30707  -215.46059  -253.54799 ]\n",
      " [ -30.270605  -24.023218  -27.752735  -29.429972  -32.567196  -52.886265\n",
      "   -43.93938   -48.181404  -56.289204]\n",
      " [-239.33876  -187.86737  -221.14104  -234.15164  -258.32858  -418.67557\n",
      "  -346.21005  -381.2123   -448.604   ]]\n",
      "J tensor([ -56.6090, -164.9460, -106.1389,  -24.0232, -187.8674])\n",
      "P tensor([-104.2409, -303.9606, -195.3071,  -43.9394, -346.2101])\n",
      "expected_state_action_values  tensor([[ -61.6490, -104.5177],\n",
      "        [-185.3612, -310.4743],\n",
      "        [-120.9580, -201.2094],\n",
      "        [ -25.3477,  -43.2723],\n",
      "        [-209.5003, -352.0087]])\n",
      "reward -47.60743678431324\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[2.8533, 3.2197, 2.6279, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 4.0320, 5.2239, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 2.6880, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 4.8914, 9.7098, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.5005, 6.4395, 8.1867, 7.2458, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "state_batch tensor([[1.4266, 1.7539, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 2.6880, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 6.4395, 4.3138, 9.1767, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 7.1502, 6.6508, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-10.7009],\n",
      "        [-23.4019],\n",
      "        [-25.4330],\n",
      "        [-24.1008],\n",
      "        [-28.3724]])\n",
      "action_batch tensor([[4, 8],\n",
      "        [2, 6],\n",
      "        [2, 7],\n",
      "        [2, 6],\n",
      "        [0, 6]])\n",
      "policy_net1(state_batch) tensor([[ -48.8942,  -45.9749,  -45.3150,  -48.5354,  -63.2505,  -92.8896,\n",
      "          -78.6039,  -81.5528, -105.2292],\n",
      "        [-116.3446, -109.4467, -108.7386, -117.4082, -152.1494, -222.4811,\n",
      "         -187.3040, -195.2667, -253.0003],\n",
      "        [-117.3548, -110.4390, -109.6391, -118.2413, -153.3216, -224.2619,\n",
      "         -188.8815, -196.8639, -255.0257],\n",
      "        [-117.8603, -110.9610, -110.1432, -118.8083, -154.0429, -225.2755,\n",
      "         -189.7180, -197.7661, -256.2027],\n",
      "        [-130.2559, -122.4465, -121.4199, -130.4726, -169.5514, -248.2258,\n",
      "         -209.3997, -217.9642, -282.4109]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -63.2505, -105.2292],\n",
      "        [-108.7386, -187.3040],\n",
      "        [-109.6391, -196.8639],\n",
      "        [-110.1432, -189.7180],\n",
      "        [-130.2559, -209.3997]], grad_fn=<GatherBackward>)\n",
      "A [[ -63.298885  -59.380722  -58.730923  -62.907192  -81.91676  -120.30449\n",
      "  -101.71618  -105.58421  -136.36124 ]\n",
      " [-118.28756  -111.071075 -110.263435 -118.59654  -153.98396  -225.5624\n",
      "  -190.17888  -197.97018  -256.36957 ]\n",
      " [-116.3446   -109.446655 -108.738556 -117.40819  -152.14941  -222.48111\n",
      "  -187.30399  -195.26666  -253.00026 ]\n",
      " [-111.93749  -105.51649  -104.64968  -112.981026 -146.4891   -214.03107\n",
      "  -180.25003  -187.94688  -243.6035  ]\n",
      " [-134.90251  -126.76605  -125.76961  -134.91454  -175.41609  -256.99823\n",
      "  -216.87518  -225.66322  -292.10614 ]]\n",
      "J tensor([ -58.7309, -110.2634, -108.7386, -104.6497, -125.7696])\n",
      "P tensor([-101.7162, -190.1789, -187.3040, -180.2500, -216.8752])\n",
      "expected_state_action_values  tensor([[ -63.5587, -102.2455],\n",
      "        [-122.6390, -194.5629],\n",
      "        [-123.2977, -194.0066],\n",
      "        [-118.2855, -186.3258],\n",
      "        [-141.5650, -223.5600]])\n",
      "1\n",
      "reward -46.0040541459654\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[4.2799, 3.2197, 3.7932, 6.0371, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 1.7268, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 4.8914, 9.7098, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 4.0320, 5.2239, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-18.9634],\n",
      "        [ -4.6880],\n",
      "        [-19.2270],\n",
      "        [-32.2255],\n",
      "        [-26.2099]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [3, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-113.5636,  -98.3541, -105.3664, -104.5848, -147.4302, -199.4674,\n",
      "         -175.3197, -168.4031, -214.6050],\n",
      "        [ -30.6262,  -26.6831,  -28.1938,  -27.8842,  -39.4814,  -53.6067,\n",
      "          -47.3530,  -45.2925,  -57.3036],\n",
      "        [ -88.5538,  -76.4527,  -81.8097,  -80.8615, -114.2470, -155.0213,\n",
      "         -136.5062, -130.8227, -166.4866],\n",
      "        [-130.5332, -113.1927, -121.3152, -120.5935, -169.8611, -229.5563,\n",
      "         -201.6282, -193.8437, -247.1625],\n",
      "        [-137.3406, -118.6925, -127.3219, -126.1231, -177.9056, -240.9771,\n",
      "         -211.8740, -203.3872, -259.1009]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-104.5848, -175.3197],\n",
      "        [ -27.8842,  -45.2925],\n",
      "        [ -76.4527, -136.5062],\n",
      "        [-121.3152, -201.6282],\n",
      "        [-118.6925, -211.8740]], grad_fn=<GatherBackward>)\n",
      "A [[-111.851616  -96.77466  -103.70945  -102.85575  -145.02444  -196.36479\n",
      "  -172.62161  -165.74837  -211.129   ]\n",
      " [ -37.65397   -32.74966   -34.67356   -34.197884  -48.467396  -65.82167\n",
      "   -58.150997  -55.61857   -70.45789 ]\n",
      " [-126.87794  -109.44817  -117.33645  -115.86873  -163.70686  -222.12671\n",
      "  -195.53503  -187.44678  -238.62749 ]\n",
      " [-169.39505  -146.707    -157.42485  -156.30367  -220.21973  -297.76736\n",
      "  -261.55093  -251.40382  -320.53885 ]\n",
      " [-137.57585  -119.16612  -127.81763  -126.958    -178.8311   -241.87262\n",
      "  -212.44185  -204.18996  -260.2287  ]]\n",
      "J tensor([ -96.7747,  -32.7497, -109.4482, -146.7070, -119.1661])\n",
      "P tensor([-165.7484,  -55.6186, -187.4468, -251.4038, -204.1900])\n",
      "expected_state_action_values  tensor([[-106.0606, -168.1369],\n",
      "        [ -34.1627,  -54.7447],\n",
      "        [-117.7304, -187.9291],\n",
      "        [-164.2618, -258.4890],\n",
      "        [-133.4594, -209.9809]])\n",
      "5\n",
      "reward -41.64505426446154\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-30.9730],\n",
      "        [-28.3724],\n",
      "        [-32.9790],\n",
      "        [-24.7273],\n",
      "        [-24.1008]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 6],\n",
      "        [0, 7],\n",
      "        [0, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-210.8684, -198.4182, -210.2976, -207.6262, -274.1060, -370.3443,\n",
      "         -312.9823, -325.4555, -399.2684],\n",
      "        [-161.2481, -151.5558, -160.5058, -158.2053, -209.0391, -282.7961,\n",
      "         -239.2044, -248.4753, -304.6195],\n",
      "        [-190.2621, -178.5200, -189.3111, -186.5511, -246.5097, -333.5704,\n",
      "         -282.1626, -293.0111, -359.3138],\n",
      "        [-162.8231, -152.8736, -161.9519, -159.4354, -210.7271, -285.3666,\n",
      "         -241.4413, -250.6803, -307.1375],\n",
      "        [-147.6885, -138.9817, -147.3128, -145.6532, -192.0942, -259.6380,\n",
      "         -219.2929, -228.0837, -279.5914]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-210.2976, -312.9823],\n",
      "        [-161.2481, -239.2044],\n",
      "        [-190.2621, -293.0111],\n",
      "        [-162.8231, -250.6803],\n",
      "        [-147.3128, -219.2929]], grad_fn=<GatherBackward>)\n",
      "A [[-185.73053 -174.70964 -185.18764 -182.90138 -241.42073 -326.22595\n",
      "  -275.68365 -286.6454  -351.61514]\n",
      " [-167.7406  -157.60582 -166.98848 -164.37451 -217.27904 -294.11258\n",
      "  -248.84676 -258.41306 -316.5264 ]\n",
      " [-205.9347  -193.32631 -205.16888 -202.46614 -267.28387 -361.5008\n",
      "  -305.53503 -317.51822 -389.37576]\n",
      " [-161.24814 -151.55585 -160.5058  -158.20529 -209.03911 -282.7961\n",
      "  -239.20438 -248.47528 -304.6195 ]\n",
      " [-140.04897 -131.92606 -139.73692 -138.25943 -182.34363 -246.27417\n",
      "  -208.00801 -216.39609 -265.3873 ]]\n",
      "J tensor([-174.7096, -157.6058, -193.3263, -151.5558, -131.9261])\n",
      "P tensor([-275.6837, -248.8468, -305.5350, -239.2044, -208.0080])\n",
      "expected_state_action_values  tensor([[-188.2117, -279.0883],\n",
      "        [-170.2176, -252.3345],\n",
      "        [-206.9727, -307.9605],\n",
      "        [-161.1275, -240.0112],\n",
      "        [-142.8343, -211.3080]])\n",
      "reward -43.98816172314577\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 10.5880, 11.7574,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  7.3475,  9.7098,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-38.9782],\n",
      "        [ -3.7268],\n",
      "        [-34.3329],\n",
      "        [-28.3724],\n",
      "        [-36.9724]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [0, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-247.9188, -215.9369, -213.8039, -226.7966, -298.5948, -404.0899,\n",
      "         -327.4343, -368.1632, -435.7523],\n",
      "        [ -32.4023,  -28.4614,  -27.7225,  -29.4186,  -38.8347,  -52.7245,\n",
      "          -42.9547,  -48.0419,  -56.4604],\n",
      "        [-231.2683, -201.3739, -199.4215, -211.5896, -278.5364, -376.9807,\n",
      "         -305.4526, -343.4268, -406.4435],\n",
      "        [-164.3871, -143.3851, -141.6301, -150.2448, -197.8571, -267.8232,\n",
      "         -217.1080, -244.0491, -288.8194],\n",
      "        [-159.3414, -138.9426, -137.2846, -145.4989, -191.7041, -259.5447,\n",
      "         -210.4903, -236.5132, -279.7438]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-213.8039, -327.4343],\n",
      "        [ -29.4186,  -48.0419],\n",
      "        [-199.4215, -305.4526],\n",
      "        [-164.3871, -217.1080],\n",
      "        [-145.4989, -236.5132]], grad_fn=<GatherBackward>)\n",
      "A [[-248.73563  -216.75945  -214.50465  -227.52748  -299.62808  -405.36017\n",
      "  -328.52063  -369.38742  -437.3286  ]\n",
      " [ -33.53768   -29.449371  -28.696033  -30.436035  -40.184437  -54.559032\n",
      "   -44.44957   -49.715633  -58.439667]\n",
      " [-229.46365  -199.81064  -197.72476  -209.56525  -276.07788  -373.7227\n",
      "  -303.00247  -340.5145   -403.0491  ]\n",
      " [-170.26355  -148.46707  -146.72542  -155.41438  -204.76886  -277.33435\n",
      "  -224.89993  -252.71855  -298.78815 ]\n",
      " [-216.00385  -188.35341  -186.44287  -198.0205   -260.5528   -352.3998\n",
      "  -285.41202  -321.09372  -380.01636 ]]\n",
      "J tensor([-214.5047,  -28.6960, -197.7248, -146.7254, -186.4429])\n",
      "P tensor([-328.5206,  -44.4496, -303.0025, -224.8999, -285.4120])\n",
      "expected_state_action_values  tensor([[-232.0324, -334.6467],\n",
      "        [ -29.5532,  -43.7314],\n",
      "        [-212.2852, -307.0351],\n",
      "        [-160.4253, -230.7823],\n",
      "        [-204.7710, -293.8432]])\n",
      "5\n",
      "reward -45.230413345202265\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[4.5005, 6.4395, 8.1867, 7.2458, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.6263, 6.4395, 4.0320, 8.5990, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 4.8914, 9.7098, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 6.4395, 4.0320, 5.2239, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [0.0000, 0.0000, 2.2900, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-24.8327],\n",
      "        [-25.4330],\n",
      "        [-32.2255],\n",
      "        [-26.2099],\n",
      "        [ -6.7672]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [2, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-170.3838, -159.6655, -168.9848, -177.4912, -220.0177, -298.2482,\n",
      "         -251.8726, -281.5960, -321.0217],\n",
      "        [-148.9605, -139.7612, -147.9895, -156.0668, -193.0819, -261.3892,\n",
      "         -220.3798, -246.7363, -281.4910],\n",
      "        [-142.0516, -133.4541, -141.1986, -149.0283, -184.3656, -249.3715,\n",
      "         -210.2302, -235.4552, -268.7594],\n",
      "        [-149.0904, -139.6326, -147.8497, -155.6033, -192.6819, -261.1759,\n",
      "         -220.3983, -246.4919, -281.1154],\n",
      "        [ -37.7569,  -35.5652,  -37.2027,  -39.0495,  -48.5245,  -65.9382,\n",
      "          -55.9306,  -62.2482,  -70.6405]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-168.9848, -281.5960],\n",
      "        [-147.9895, -246.7363],\n",
      "        [-141.1986, -210.2302],\n",
      "        [-139.6326, -220.3983],\n",
      "        [ -39.0495,  -62.2482]], grad_fn=<GatherBackward>)\n",
      "A [[-159.12132  -149.11095  -157.78587  -165.81009  -205.54303  -278.5337\n",
      "  -235.242    -262.9863   -299.91913 ]\n",
      " [-147.91896  -138.74258  -147.00047  -155.16206  -191.87704  -259.69797\n",
      "  -218.87497  -245.10434  -279.6734  ]\n",
      " [-184.36871  -173.0171   -183.25595  -193.22557  -239.08351  -323.53006\n",
      "  -272.75192  -305.4523   -348.61765 ]\n",
      " [-149.72502  -140.52551  -148.78387  -156.92772  -194.13622  -262.77438\n",
      "  -221.53108  -248.05795  -283.0081  ]\n",
      " [ -58.42209   -54.824707  -57.639706  -60.425484  -75.0725   -101.97627\n",
      "   -86.403244  -96.270035 -109.488884]]\n",
      "J tensor([-149.1109, -138.7426, -173.0171, -140.5255,  -54.8247])\n",
      "P tensor([-235.2420, -218.8750, -272.7519, -221.5311,  -86.4032])\n",
      "expected_state_action_values  tensor([[-159.0325, -236.5505],\n",
      "        [-150.3014, -222.4205],\n",
      "        [-187.9409, -277.7022],\n",
      "        [-152.6829, -225.5879],\n",
      "        [ -56.1095,  -84.5302]])\n",
      "5\n",
      "reward -47.95915658482678\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-25.6968],\n",
      "        [-24.1008],\n",
      "        [-27.9424],\n",
      "        [-27.4673],\n",
      "        [-36.4907]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-138.9241, -140.2485, -146.8603, -153.2613, -179.6712, -243.2168,\n",
      "         -213.6849, -221.8186, -261.8970],\n",
      "        [-157.2262, -158.7295, -166.2162, -173.2558, -203.1648, -275.1535,\n",
      "         -241.7813, -250.9681, -296.1759],\n",
      "        [-193.0819, -194.8370, -204.2520, -212.8645, -249.6405, -337.9666,\n",
      "         -296.9750, -308.2764, -363.8485],\n",
      "        [-182.9219, -184.3316, -193.2732, -201.3358, -236.2032, -319.9071,\n",
      "         -281.2309, -291.7515, -344.4215],\n",
      "        [-187.8641, -189.3158, -198.4195, -206.4237, -242.2645, -328.3458,\n",
      "         -288.7484, -299.4668, -353.3170]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-153.2613, -213.6849],\n",
      "        [-166.2162, -241.7813],\n",
      "        [-204.2520, -296.9750],\n",
      "        [-184.3316, -281.2309],\n",
      "        [-198.4195, -288.7484]], grad_fn=<GatherBackward>)\n",
      "A [[-156.36066 -157.80719 -165.26491 -172.242   -201.98439 -273.59647\n",
      "  -240.42876 -249.53416 -294.4746 ]\n",
      " [-149.3951  -150.95772 -157.98068 -164.7734  -193.22581 -261.51328\n",
      "  -229.79446 -238.57597 -281.68237]\n",
      " [-172.47823 -174.01271 -182.3076  -189.89453 -222.80522 -301.69223\n",
      "  -265.23282 -275.20416 -324.83304]\n",
      " [-202.60939 -203.974   -213.67911 -222.12985 -260.8266  -353.6638\n",
      "  -311.15732 -322.55008 -380.79495]\n",
      " [-217.11044 -218.70163 -229.48022 -238.96367 -280.27493 -379.76822\n",
      "  -333.75797 -346.29813 -408.635  ]]\n",
      "J tensor([-156.3607, -149.3951, -172.4782, -202.6094, -217.1104])\n",
      "P tensor([-240.4288, -229.7945, -265.2328, -311.1573, -333.7580])\n",
      "expected_state_action_values  tensor([[-166.4214, -242.0827],\n",
      "        [-158.5564, -230.9158],\n",
      "        [-183.1728, -266.6519],\n",
      "        [-209.8157, -307.5089],\n",
      "        [-231.8901, -336.8729]])\n",
      "reward -47.52405987391407\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-32.2255],\n",
      "        [-25.3215],\n",
      "        [-33.9160],\n",
      "        [-45.2304],\n",
      "        [-10.7009]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [4, 8]])\n",
      "policy_net1(state_batch) tensor([[-157.6597, -170.4890, -156.5267, -183.1916, -203.5687, -275.7184,\n",
      "         -251.3102, -251.5955, -297.0475],\n",
      "        [-204.2305, -220.6489, -202.7575, -237.1037, -263.4963, -357.0345,\n",
      "         -325.4334, -325.7708, -384.5844],\n",
      "        [-239.5664, -258.5384, -237.7183, -277.6509, -308.7151, -418.4800,\n",
      "         -381.6303, -381.8268, -450.6741],\n",
      "        [-328.6931, -354.1754, -325.4401, -379.0506, -421.9584, -572.7251,\n",
      "         -522.8896, -522.6210, -616.9303],\n",
      "        [ -65.7936,  -70.9830,  -64.9141,  -75.5905,  -84.2182, -114.5484,\n",
      "         -104.7828, -104.4824, -122.8906]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-156.5267, -251.3102],\n",
      "        [-202.7575, -325.4334],\n",
      "        [-239.5664, -381.8268],\n",
      "        [-354.1754, -522.8896],\n",
      "        [ -84.2182, -122.8906]], grad_fn=<GatherBackward>)\n",
      "A [[-204.2305   -220.64894  -202.75752  -237.10365  -263.49634  -357.03452\n",
      "  -325.43338  -325.77084  -384.5844  ]\n",
      " [-180.82428  -194.95166  -179.02455  -208.7257   -232.2304   -315.29932\n",
      "  -287.7862   -287.6435   -339.29477 ]\n",
      " [-244.04436  -263.06775  -241.67288  -281.6582   -313.4387   -425.44485\n",
      "  -388.3384   -388.18393  -458.08282 ]\n",
      " [-333.60123  -359.42825  -330.37628  -384.95602  -428.4331   -581.46344\n",
      "  -530.7538   -530.5484   -626.3024  ]\n",
      " [ -85.6195    -92.23764   -84.542404  -98.45565  -109.638214 -149.10742\n",
      "  -136.30096  -135.9775   -160.05026 ]]\n",
      "J tensor([-202.7575, -179.0246, -241.6729, -330.3763,  -84.5424])\n",
      "P tensor([-325.4334, -287.6435, -388.1839, -530.5484, -135.9775])\n",
      "expected_state_action_values  tensor([[-214.7073, -325.1156],\n",
      "        [-186.4436, -284.2007],\n",
      "        [-251.4216, -383.2816],\n",
      "        [-342.5691, -522.7240],\n",
      "        [ -86.7891, -133.0806]])\n",
      "reward -48.41205331266182\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.8397,  6.4395, 10.1104,  9.0302,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-40.9402],\n",
      "        [-33.9160],\n",
      "        [-30.4570],\n",
      "        [-27.9424],\n",
      "        [-10.7610]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-325.0021, -303.4046, -279.9589, -349.1539, -418.9286, -527.3340,\n",
      "         -463.5921, -497.4907, -598.1345],\n",
      "        [-269.7820, -252.2549, -232.7122, -290.7222, -348.5657, -438.3642,\n",
      "         -385.0976, -413.5982, -497.3017],\n",
      "        [-235.6720, -220.6190, -203.5682, -254.8192, -305.2483, -383.5323,\n",
      "         -336.6416, -361.8627, -435.1658],\n",
      "        [-230.6004, -215.7669, -199.0646, -248.9916, -298.3665, -375.0547,\n",
      "         -329.3199, -353.8506, -425.4522],\n",
      "        [ -95.8440,  -89.5167,  -82.3222, -102.5458, -123.1544, -155.3330,\n",
      "         -136.7986, -146.4821, -175.6834]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-303.4046, -463.5921],\n",
      "        [-269.7820, -413.5982],\n",
      "        [-203.5682, -336.6416],\n",
      "        [-199.0646, -329.3199],\n",
      "        [ -89.5167, -136.7986]], grad_fn=<GatherBackward>)\n",
      "A [[-351.80084  -328.34763  -302.8454   -377.33734  -452.9978   -570.34717\n",
      "  -501.66367  -538.13513  -647.0734  ]\n",
      " [-273.0667   -255.10182  -235.1317   -293.13647  -351.7773   -442.92047\n",
      "  -389.45758  -417.89783  -502.36884 ]\n",
      " [-223.08913  -208.50452  -192.33401  -240.21124  -288.0318   -362.4\n",
      "  -318.43552  -341.87473  -410.91107 ]\n",
      " [-205.588    -192.33891  -177.35397  -221.73555  -265.81326  -334.18393\n",
      "  -293.56693  -315.30472  -379.12277 ]\n",
      " [ -96.40597   -90.04114   -82.80436  -103.14129  -123.872154 -156.23897\n",
      "  -137.59822  -147.33786  -176.71152 ]]\n",
      "J tensor([-302.8454, -235.1317, -192.3340, -177.3540,  -82.8044])\n",
      "P tensor([-501.6637, -389.4576, -318.4355, -293.5669, -137.5982])\n",
      "expected_state_action_values  tensor([[-313.5011, -492.4375],\n",
      "        [-245.5346, -384.4279],\n",
      "        [-203.5576, -317.0489],\n",
      "        [-187.5610, -292.1526],\n",
      "        [ -85.2849, -134.5994]])\n",
      "reward -44.925067542864916\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.7833],\n",
      "        [-33.9160],\n",
      "        [-47.5241],\n",
      "        [-24.7273],\n",
      "        [-24.1008]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 7],\n",
      "        [2, 7],\n",
      "        [0, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-217.5895, -233.5259, -187.9624, -252.2326, -302.2710, -379.5831,\n",
      "         -321.1117, -346.0172, -431.3656],\n",
      "        [-235.1872, -251.8650, -203.0392, -272.3156, -326.3449, -410.1354,\n",
      "         -347.0055, -373.6974, -465.6653],\n",
      "        [-331.7455, -354.8187, -285.9209, -382.4007, -458.8502, -577.2475,\n",
      "         -488.9676, -526.0490, -655.4597],\n",
      "        [-177.3316, -189.7191, -152.7480, -204.4892, -245.2502, -308.6814,\n",
      "         -261.4293, -281.2196, -350.2097],\n",
      "        [-162.4865, -174.2219, -140.3163, -188.4787, -225.7148, -283.6223,\n",
      "         -239.8359, -258.4001, -321.9440]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-187.9624, -321.1117],\n",
      "        [-235.1872, -373.6974],\n",
      "        [-285.9209, -526.0490],\n",
      "        [-177.3316, -281.2196],\n",
      "        [-140.3163, -239.8359]], grad_fn=<GatherBackward>)\n",
      "A [[-232.06834 -248.79095 -200.36763 -268.8427  -322.18216 -404.71173\n",
      "  -342.3931  -368.84872 -459.8624 ]\n",
      " [-239.30286 -255.97696 -206.17564 -275.9076  -330.96896 -416.46393\n",
      "  -352.72083 -379.46744 -472.75552]\n",
      " [-339.80478 -363.23    -292.69955 -391.21805 -469.55872 -590.9386\n",
      "  -500.7152  -538.5019  -670.91547]\n",
      " [-176.82643 -189.35393 -152.41138 -204.24393 -244.90834 -307.9865\n",
      "  -260.77173 -280.63885 -349.68417]\n",
      " [-154.85216 -166.1714  -133.75679 -179.7708  -215.29274 -270.3574\n",
      "  -228.6175  -246.36429 -307.08252]]\n",
      "J tensor([-200.3676, -206.1756, -292.6996, -152.4114, -133.7568])\n",
      "P tensor([-342.3931, -352.7208, -500.7152, -260.7717, -228.6175])\n",
      "expected_state_action_values  tensor([[-216.1142, -343.9371],\n",
      "        [-219.4741, -351.3648],\n",
      "        [-310.9536, -498.1677],\n",
      "        [-161.8975, -259.4218],\n",
      "        [-144.4819, -229.8566]])\n",
      "reward -45.10849584709913\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.2038],\n",
      "        [-40.3589],\n",
      "        [-19.3299],\n",
      "        [-23.4019],\n",
      "        [-36.4907]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [3, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-153.2362, -176.9747, -152.8393, -191.6517, -229.6506, -288.2131,\n",
      "         -253.0429, -253.3741, -327.4442],\n",
      "        [-232.3392, -267.7563, -231.3733, -289.4714, -347.1385, -436.1480,\n",
      "         -383.1822, -383.3997, -495.4651],\n",
      "        [ -98.5523, -113.6198,  -97.9475, -122.6168, -147.0348, -184.9238,\n",
      "         -162.5413, -162.4996, -209.8146],\n",
      "        [-139.3606, -161.0664, -139.1327, -174.8231, -209.2313, -262.5254,\n",
      "         -230.2309, -230.7238, -298.2028],\n",
      "        [-168.8547, -194.8654, -168.3748, -211.0159, -252.8493, -317.4982,\n",
      "         -278.7502, -279.0765, -360.5538]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-152.8393, -253.0429],\n",
      "        [-267.7563, -383.1822],\n",
      "        [-122.6168, -162.5413],\n",
      "        [-139.1327, -230.2309],\n",
      "        [-168.3748, -278.7502]], grad_fn=<GatherBackward>)\n",
      "A [[-177.00398  -204.73094  -176.95671  -222.3541   -266.1352   -333.64725\n",
      "  -292.56616  -293.3188   -379.17563 ]\n",
      " [-242.36618  -279.66754  -241.6025   -302.5603   -362.7464   -455.34952\n",
      "  -399.91724  -400.37204  -517.63806 ]\n",
      " [-116.265526 -134.39236  -115.9221   -145.49843  -174.27818  -218.74573\n",
      "  -192.00653  -192.28622  -248.4921  ]\n",
      " [-139.71838  -161.19664  -139.17708  -174.42798  -208.99483  -262.58823\n",
      "  -230.55765  -230.77124  -298.1177  ]\n",
      " [-194.8971   -224.85532  -194.49103  -243.98392  -292.1636   -366.78604\n",
      "  -321.81155  -322.33173  -416.50665 ]]\n",
      "J tensor([-176.9567, -241.6025, -115.9221, -139.1771, -194.4910])\n",
      "P tensor([-292.5662, -399.9172, -192.0065, -230.5576, -321.8116])\n",
      "expected_state_action_values  tensor([[-194.4648, -298.5133],\n",
      "        [-257.8011, -400.2844],\n",
      "        [-123.6598, -192.1357],\n",
      "        [-148.6613, -230.9038],\n",
      "        [-211.5326, -326.1211]])\n",
      "1\n",
      "reward -54.00508643988465\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[5.7065, 6.4395, 3.9703, 1.1107, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [5.7065, 3.2197, 7.4044, 6.5020, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.2197, 2.3823, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [4.2799, 3.3899, 3.0863, 1.2602, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000],\n",
      "        [2.8533, 3.2197, 2.6880, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000]])\n",
      "reward_batch tensor([[-14.0164],\n",
      "        [-36.9724],\n",
      "        [-19.2270],\n",
      "        [-19.3299],\n",
      "        [-11.8820]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [3, 7],\n",
      "        [1, 6],\n",
      "        [3, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-136.1367, -146.9335, -144.2367, -177.9035, -202.7363, -255.2584,\n",
      "         -232.5268, -224.2903, -289.3376],\n",
      "        [-159.7211, -172.7886, -169.5897, -209.3470, -238.5799, -299.8734,\n",
      "         -273.0970, -263.6496, -340.3039],\n",
      "        [ -94.8990, -102.4851, -100.4745, -123.9999, -141.3187, -177.9356,\n",
      "         -162.1425, -156.3358, -201.6398],\n",
      "        [-104.8519, -113.3122, -111.1097, -137.1752, -156.3095, -196.7057,\n",
      "         -179.1918, -172.8564, -223.0072],\n",
      "        [ -78.5863,  -84.8851,  -83.1779, -102.4980, -116.9015, -147.2751,\n",
      "         -134.3303, -129.4066, -166.6746]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-146.9335, -232.5268],\n",
      "        [-209.3470, -263.6496],\n",
      "        [-102.4851, -162.1425],\n",
      "        [-137.1752, -179.1918],\n",
      "        [ -84.8851, -134.3303]], grad_fn=<GatherBackward>)\n",
      "A [[-104.851906 -113.3122   -111.109695 -137.17523  -156.30952  -196.7057\n",
      "  -179.19183  -172.85638  -223.00716 ]\n",
      " [-218.33061  -236.27446  -232.20226  -287.08002  -326.88425  -410.53992\n",
      "  -373.49316  -360.92004  -466.09317 ]\n",
      " [-136.13667  -146.93353  -144.2367   -177.90347  -202.73633  -255.25844\n",
      "  -232.52681  -224.29033  -289.33762 ]\n",
      " [-124.24357  -134.59479  -132.05147  -163.41452  -186.03592  -233.67023\n",
      "  -212.59879  -205.40611  -265.22253 ]\n",
      " [ -94.89901  -102.48506  -100.47453  -123.99991  -141.31868  -177.93562\n",
      "  -162.14246  -156.33575  -201.63982 ]]\n",
      "J tensor([-104.8519, -218.3306, -136.1367, -124.2436,  -94.8990])\n",
      "P tensor([-172.8564, -360.9200, -224.2903, -205.4061, -156.3358])\n",
      "expected_state_action_values  tensor([[-108.3831, -169.5871],\n",
      "        [-233.4699, -361.8004],\n",
      "        [-141.7500, -221.0883],\n",
      "        [-131.1491, -204.1954],\n",
      "        [ -97.2911, -152.5842]])\n",
      "1\n",
      "reward -51.181006387566555\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-26.8229],\n",
      "        [-42.7328],\n",
      "        [-28.1379],\n",
      "        [-32.9790],\n",
      "        [-31.9069]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-177.7106, -205.0918, -189.0111, -245.1195, -266.0016, -334.1931,\n",
      "         -314.6261, -304.5041, -379.1236],\n",
      "        [-303.7845, -349.6139, -322.2413, -416.6520, -452.6462, -569.5273,\n",
      "         -536.8082, -518.9445, -646.1264],\n",
      "        [-223.0415, -257.0576, -237.1934, -307.5596, -333.6388, -419.4281,\n",
      "         -394.7445, -382.0404, -475.5457],\n",
      "        [-205.9841, -237.2182, -218.6026, -283.0804, -307.3479, -386.5907,\n",
      "         -364.1780, -352.1833, -438.5460],\n",
      "        [-219.3146, -252.9880, -233.2740, -302.4557, -328.1831, -412.4013,\n",
      "         -388.1964, -375.7421, -467.8068]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-189.0111, -314.6261],\n",
      "        [-349.6139, -536.8082],\n",
      "        [-237.1934, -394.7445],\n",
      "        [-205.9841, -352.1833],\n",
      "        [-233.2740, -388.1964]], grad_fn=<GatherBackward>)\n",
      "A [[-174.7327  -201.56519 -185.74097 -240.75214 -261.31577 -328.41653\n",
      "  -309.2678  -299.23288 -372.51224]\n",
      " [-303.65475 -349.6705  -322.30106 -417.0735  -452.9009  -569.7064\n",
      "  -536.71173 -519.0847  -646.38916]\n",
      " [-188.0923  -216.84616 -199.97717 -259.3755  -281.4416  -353.6612\n",
      "  -332.93362 -322.17368 -401.17947]\n",
      " [-223.94138 -258.06418 -237.93681 -308.39728 -334.65738 -420.76575\n",
      "  -396.12958 -383.30167 -477.2986 ]\n",
      " [-202.10284 -232.98866 -214.81369 -278.35895 -302.09866 -379.80646\n",
      "  -357.62234 -346.01413 -430.68796]]\n",
      "J tensor([-174.7327, -303.6548, -188.0923, -223.9414, -202.1028])\n",
      "P tensor([-299.2329, -519.0847, -322.1737, -383.3017, -346.0141])\n",
      "expected_state_action_values  tensor([[-184.0823, -296.1324],\n",
      "        [-316.0221, -509.9091],\n",
      "        [-197.4210, -318.0942],\n",
      "        [-234.5262, -377.9505],\n",
      "        [-213.7995, -343.3196]])\n",
      "reward -47.436098530923175\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 9.9864,  0.0000, 18.4464, 14.4922,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-44.9251],\n",
      "        [-47.4361],\n",
      "        [-30.9730],\n",
      "        [-38.6657],\n",
      "        [-47.5241]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-332.4658, -334.2244, -307.7705, -424.8343, -461.2862, -579.5484,\n",
      "         -527.7983, -546.6561, -658.3439],\n",
      "        [-378.3026, -379.7500, -349.6160, -481.7868, -523.3853, -658.3721,\n",
      "         -599.9828, -620.9284, -747.5758],\n",
      "        [-232.4698, -234.0630, -215.4620, -298.0443, -323.3564, -405.9153,\n",
      "         -369.3413, -382.8528, -461.1923],\n",
      "        [-236.8749, -237.8071, -219.2203, -302.8546, -328.6581, -413.1316,\n",
      "         -376.1064, -389.4631, -468.8339],\n",
      "        [-331.6955, -333.1695, -306.8461, -423.2324, -459.6175, -577.8455,\n",
      "         -526.3873, -544.9743, -656.0833]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-307.7705, -527.7983],\n",
      "        [-349.6160, -599.9828],\n",
      "        [-215.4620, -369.3413],\n",
      "        [-219.2203, -376.1064],\n",
      "        [-306.8461, -544.9743]], grad_fn=<GatherBackward>)\n",
      "A [[-315.8856  -317.6968  -292.3824  -403.48605 -438.22308 -550.50085\n",
      "  -501.4679  -519.34186 -625.51245]\n",
      " [-357.5463  -359.06308 -330.41888 -455.28574 -494.6829  -622.1686\n",
      "  -567.07587 -586.8597  -706.63684]\n",
      " [-204.76491 -206.11133 -189.74976 -262.54797 -284.81274 -357.57465\n",
      "  -325.33987 -337.2092  -406.17776]\n",
      " [-267.57455 -268.6605  -247.67764 -342.1852  -371.33792 -466.69672\n",
      "  -424.8442  -439.9926  -529.75684]\n",
      " [-339.75137 -341.06442 -314.11545 -433.0111  -470.33688 -591.54364\n",
      "  -539.01416 -557.8687  -671.54626]]\n",
      "J tensor([-292.3824, -330.4189, -189.7498, -247.6776, -314.1154])\n",
      "P tensor([-501.4679, -567.0759, -325.3399, -424.8442, -539.0142])\n",
      "expected_state_action_values  tensor([[-308.0692, -496.2462],\n",
      "        [-344.8131, -557.8044],\n",
      "        [-201.7478, -323.7789],\n",
      "        [-261.5756, -421.0255],\n",
      "        [-330.2279, -532.6368]])\n",
      "reward -50.76506209222104\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-47.9592],\n",
      "        [-28.1379],\n",
      "        [-19.3299],\n",
      "        [-33.9160],\n",
      "        [-14.0164]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [2, 6],\n",
      "        [3, 6],\n",
      "        [0, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-308.5165, -309.7732, -303.8377, -394.1334, -427.8077, -537.5613,\n",
      "         -472.8318, -489.9539, -610.9008],\n",
      "        [-210.2206, -211.2426, -207.5027, -269.9359, -292.5974, -367.3422,\n",
      "         -322.6440, -334.6685, -417.1079],\n",
      "        [-106.9054, -107.3987, -105.1215, -136.4943, -148.1352, -186.3105,\n",
      "         -163.9369, -169.7204, -211.3795],\n",
      "        [-220.7882, -222.0748, -217.9139, -283.3191, -307.2567, -385.6233,\n",
      "         -338.8385, -351.4570, -438.1496],\n",
      "        [-138.5318, -138.9865, -136.2057, -176.6735, -191.7487, -241.2929,\n",
      "         -212.3196, -219.7903, -273.7042]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-308.5165, -472.8318],\n",
      "        [-207.5027, -322.6440],\n",
      "        [-136.4943, -163.9369],\n",
      "        [-220.7882, -351.4570],\n",
      "        [-138.9865, -212.3196]], grad_fn=<GatherBackward>)\n",
      "A [[-312.26248  -313.6663   -307.6204   -398.89127  -433.0557   -544.09656\n",
      "  -478.6609   -495.98273  -618.2727  ]\n",
      " [-178.2147   -179.15219  -175.85744  -228.85098  -248.13553  -311.37112\n",
      "  -273.56403  -283.71408  -353.76526 ]\n",
      " [-126.08018  -126.959015 -124.34445  -161.8351   -175.48225  -220.26314\n",
      "  -193.54744  -200.71996  -250.20364 ]\n",
      " [-225.62529  -226.67674  -222.20177  -288.2822   -312.89206  -393.21445\n",
      "  -345.86667  -358.3796   -446.68875 ]\n",
      " [-106.905365 -107.3987   -105.121506 -136.49426  -148.13516  -186.31047\n",
      "  -163.93687  -169.72041  -211.37952 ]]\n",
      "J tensor([-307.6204, -175.8574, -124.3445, -222.2018, -105.1215])\n",
      "P tensor([-478.6609, -273.5640, -193.5474, -345.8667, -163.9369])\n",
      "expected_state_action_values  tensor([[-324.8175, -478.7540],\n",
      "        [-186.4096, -274.3456],\n",
      "        [-131.2399, -193.5226],\n",
      "        [-233.8976, -345.1960],\n",
      "        [-108.6257, -161.5595]])\n",
      "reward -52.639953577272884\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-24.7273],\n",
      "        [-30.4570],\n",
      "        [-47.6074],\n",
      "        [-19.2270],\n",
      "        [-26.8229]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-176.0538, -152.9963, -151.1476, -198.2916, -225.7670, -285.3243,\n",
      "         -259.9913, -250.5499, -322.9677],\n",
      "        [-198.2334, -172.6232, -170.8673, -225.1184, -255.8623, -322.4765,\n",
      "         -293.2608, -283.2048, -365.3476],\n",
      "        [-306.0808, -265.7764, -262.8985, -344.9132, -392.7130, -496.0151,\n",
      "         -451.9092, -435.6155, -562.0300],\n",
      "        [-102.1469,  -88.6980,  -87.5012, -114.8080, -130.7524, -165.4229,\n",
      "         -150.8372, -145.1917, -187.0371],\n",
      "        [-175.2978, -152.5456, -150.8637, -198.4778, -225.7743, -284.7565,\n",
      "         -259.2032, -250.0855, -322.5809]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-176.0538, -250.5499],\n",
      "        [-170.8673, -293.2608],\n",
      "        [-262.8985, -451.9092],\n",
      "        [ -88.6980, -150.8372],\n",
      "        [-150.8637, -259.2032]], grad_fn=<GatherBackward>)\n",
      "A [[-176.00014  -153.09793  -151.20135  -198.56972  -226.04823  -285.40808\n",
      "  -260.0015   -250.6751   -323.3313  ]\n",
      " [-190.05641  -165.19353  -163.44427  -214.77287  -244.36533  -308.4945\n",
      "  -280.8947   -270.88358  -349.2449  ]\n",
      " [-331.0737   -287.63144  -284.45428  -373.17523  -424.90356  -536.57056\n",
      "  -488.84366  -471.29672  -608.08417 ]\n",
      " [-145.55896  -126.298325 -124.79193  -163.61821  -186.32167  -235.72508\n",
      "  -214.88103  -206.91154  -266.5829  ]\n",
      " [-172.91298  -150.40953  -148.72778  -195.53894  -222.49323  -280.72873\n",
      "  -255.61827  -246.54193  -317.9635  ]]\n",
      "J tensor([-151.2014, -163.4443, -284.4543, -124.7919, -148.7278])\n",
      "P tensor([-250.6751, -270.8836, -471.2967, -206.9115, -246.5419])\n",
      "expected_state_action_values  tensor([[-160.8085, -250.3349],\n",
      "        [-177.5568, -274.2522],\n",
      "        [-303.6163, -471.7745],\n",
      "        [-131.5398, -205.4474],\n",
      "        [-160.6779, -248.7106]])\n",
      "reward -53.50954382849513\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-47.4361],\n",
      "        [-38.6657],\n",
      "        [-36.9724],\n",
      "        [-26.8229],\n",
      "        [-36.4907]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-371.8099, -369.6020, -363.2771, -446.3394, -509.2582, -643.3941,\n",
      "         -606.7216, -545.5684, -728.7375],\n",
      "        [-229.2648, -227.9237, -224.3388, -276.3711, -314.9501, -397.6133,\n",
      "         -374.5519, -337.0042, -450.0636],\n",
      "        [-167.5237, -166.8105, -163.8876, -201.7784, -230.0725, -290.4329,\n",
      "         -273.7294, -246.2548, -328.7483],\n",
      "        [-171.7540, -171.1832, -168.3394, -207.7591, -236.6448, -298.3624,\n",
      "         -280.9018, -252.9588, -337.8720],\n",
      "        [-186.5430, -185.6975, -182.6867, -225.2105, -256.5768, -323.7912,\n",
      "         -304.9267, -274.4653, -366.4371]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-363.2771, -606.7216],\n",
      "        [-224.3388, -374.5519],\n",
      "        [-201.7784, -246.2548],\n",
      "        [-168.3394, -280.9018],\n",
      "        [-182.6867, -304.9267]], grad_fn=<GatherBackward>)\n",
      "A [[-352.42484 -350.48306 -344.31876 -423.00415 -482.72144 -609.76514\n",
      "  -575.0939  -517.12775 -690.8295 ]\n",
      " [-259.06268 -257.5824  -253.54474 -312.3694  -355.97073 -449.31555\n",
      "  -423.23428 -380.852   -508.72348]\n",
      " [-226.67584 -225.7722  -222.12196 -273.9181  -312.0407  -393.5779\n",
      "  -370.57504 -333.6769  -445.68512]\n",
      " [-169.45479 -168.81387 -165.98775 -204.72835 -233.25374 -294.20065\n",
      "  -277.06378 -249.4233  -333.103  ]\n",
      " [-214.18193 -213.12444 -209.90521 -259.0071  -294.89932 -372.0733\n",
      "  -350.18613 -315.3218  -421.04822]]\n",
      "J tensor([-344.3188, -253.5447, -222.1220, -165.9877, -209.9052])\n",
      "P tensor([-517.1277, -380.8520, -333.6769, -249.4233, -315.3218])\n",
      "expected_state_action_values  tensor([[-357.3230, -512.8511],\n",
      "        [-266.8560, -381.4325],\n",
      "        [-236.8821, -337.2816],\n",
      "        [-176.2118, -251.3038],\n",
      "        [-225.4054, -320.2803]])\n",
      "1\n",
      "reward -51.42070865992231\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395, 17.3845,  9.2981,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.5628],\n",
      "        [-54.0051],\n",
      "        [-11.8820],\n",
      "        [-46.0041],\n",
      "        [ -4.2900]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [1, 7],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-206.1711, -205.8214, -214.2008, -262.0787, -284.2262, -358.3329,\n",
      "         -326.2211, -314.9107, -406.0005],\n",
      "        [-323.4261, -322.0929, -335.2032, -408.5369, -443.6736, -560.4083,\n",
      "         -511.0031, -492.5243, -634.5386],\n",
      "        [ -86.4023,  -86.0336,  -89.3591, -108.9180, -118.3119, -149.6823,\n",
      "         -136.6067, -131.4608, -168.9706],\n",
      "        [-338.1259, -336.8915, -350.7632, -428.1921, -464.7102, -586.5590,\n",
      "         -534.4328, -515.4615, -664.4494],\n",
      "        [ -44.6955,  -44.7319,  -46.1500,  -56.2294,  -61.1722,  -77.3892,\n",
      "          -70.7638,  -68.0131,  -87.2518]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-206.1711, -326.2211],\n",
      "        [-323.4261, -492.5243],\n",
      "        [ -86.0336, -136.6067],\n",
      "        [-336.8915, -515.4615],\n",
      "        [ -56.2294,  -68.0131]], grad_fn=<GatherBackward>)\n",
      "A [[-222.32835  -221.77151  -231.05647  -282.63907  -306.4912   -386.49936\n",
      "  -351.84277  -339.6002   -437.6251  ]\n",
      " [-375.03995  -373.8552   -389.13098  -474.9411   -515.50415  -650.5735\n",
      "  -592.79297  -571.8072   -737.11084 ]\n",
      " [-104.22265  -103.758644 -107.82177  -131.61192  -142.86209  -180.63947\n",
      "  -164.71346  -158.63765  -204.18654 ]\n",
      " [-333.23688  -331.8      -345.58087  -421.85123  -457.81516  -577.9819\n",
      "  -526.63544  -507.85193  -654.6313  ]\n",
      " [ -41.326733  -41.381065  -42.673447  -52.02198   -56.589424  -71.58623\n",
      "   -65.45603   -62.908375  -80.677704]]\n",
      "J tensor([-221.7715, -373.8552, -103.7586, -331.8000,  -41.3267])\n",
      "P tensor([-339.6002, -571.8072, -158.6376, -507.8519,  -62.9084])\n",
      "expected_state_action_values  tensor([[-235.1572, -341.2030],\n",
      "        [-390.4748, -568.6315],\n",
      "        [-105.2647, -154.6559],\n",
      "        [-344.6241, -503.0708],\n",
      "        [ -41.4841,  -60.9076]])\n",
      "reward -53.14081091446814\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-48.4121],\n",
      "        [ -6.7672],\n",
      "        [-27.9424],\n",
      "        [-40.3589],\n",
      "        [-47.9592]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [0, 6]])\n",
      "policy_net1(state_batch) tensor([[-390.2357, -386.7020, -378.4666, -439.3116, -500.6219, -632.7361,\n",
      "         -596.0862, -536.6286, -715.9951],\n",
      "        [ -46.2856,  -46.1056,  -44.7220,  -51.9035,  -59.2496,  -75.0042,\n",
      "          -70.8522,  -63.6194,  -84.4876],\n",
      "        [-229.0548, -227.5109, -222.7910, -259.6482, -295.3820, -372.6497,\n",
      "         -350.4797, -316.0088, -421.6594],\n",
      "        [-306.1624, -303.4166, -297.0909, -345.2747, -393.2481, -496.8602,\n",
      "         -467.8577, -421.3101, -562.1302],\n",
      "        [-375.4628, -372.1229, -364.2329, -423.1571, -482.0222, -609.0738,\n",
      "         -573.5735, -516.5180, -689.3542]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-378.4666, -596.0862],\n",
      "        [ -51.9035,  -63.6194],\n",
      "        [-222.7910, -350.4797],\n",
      "        [-303.4166, -467.8577],\n",
      "        [-375.4628, -573.5735]], grad_fn=<GatherBackward>)\n",
      "A [[-381.50983 -378.53632 -370.41968 -430.53513 -490.42487 -619.2248\n",
      "  -583.0757  -525.26624 -701.129  ]\n",
      " [ -71.71463  -71.22789  -69.34108  -80.43527  -91.78271 -116.1596\n",
      "  -109.64651  -98.51467 -131.08336]\n",
      " [-204.62486 -203.21254 -198.89665 -231.704   -263.6885  -332.7142\n",
      "  -313.04416 -282.15805 -376.51437]\n",
      " [-319.41272 -316.87482 -310.18967 -360.79587 -410.851   -518.68475\n",
      "  -488.27533 -439.9088  -587.19147]\n",
      " [-380.89035 -377.63556 -369.59436 -429.24942 -489.04938 -617.8873\n",
      "  -581.95636 -524.0601  -699.2806 ]]\n",
      "J tensor([-370.4197,  -69.3411, -198.8967, -310.1897, -369.5944])\n",
      "P tensor([-525.2662,  -98.5147, -282.1581, -439.9088, -524.0601])\n",
      "expected_state_action_values  tensor([[-381.7898, -521.1517],\n",
      "        [ -69.1742,  -95.4305],\n",
      "        [-206.9494, -281.8846],\n",
      "        [-319.5296, -436.2768],\n",
      "        [-380.5941, -519.6132]])\n",
      "reward -56.43949372020264\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-38.6147],\n",
      "        [-50.7651],\n",
      "        [-27.9424],\n",
      "        [-27.4673],\n",
      "        [ -6.7672]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-277.2388, -274.2722, -267.0532, -308.8924, -335.0325, -423.4037,\n",
      "         -385.4446, -371.9136, -479.4868],\n",
      "        [-414.4477, -409.8986, -398.9674, -460.8425, -500.1902, -632.2328,\n",
      "         -575.9393, -555.5046, -716.4257],\n",
      "        [-229.5057, -227.4255, -221.6557, -257.1627, -278.6026, -351.4595,\n",
      "         -319.5073, -308.7075, -398.0374],\n",
      "        [-218.1318, -215.9002, -210.4533, -244.0965, -264.5081, -333.8105,\n",
      "         -303.5704, -293.1494, -378.0716],\n",
      "        [ -47.3801,  -47.0888,  -45.4514,  -52.5312,  -57.0978,  -72.2887,\n",
      "          -66.0092,  -63.5022,  -81.4960]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-277.2388, -371.9136],\n",
      "        [-398.9674, -575.9393],\n",
      "        [-221.6557, -319.5073],\n",
      "        [-215.9002, -303.5704],\n",
      "        [ -52.5312,  -63.5022]], grad_fn=<GatherBackward>)\n",
      "A [[-301.24686  -297.97055  -290.28467  -335.86115  -364.18533  -460.23773\n",
      "  -418.85855  -404.2238   -521.09436 ]\n",
      " [-429.7965   -425.2523   -413.94604  -478.3851   -519.1348   -655.9335\n",
      "  -597.38     -576.3563   -743.4107  ]\n",
      " [-205.5249   -203.63242  -198.36076  -230.04425  -249.311    -314.55872\n",
      "  -286.0793   -276.30875  -356.29352 ]\n",
      " [-242.57729  -239.90445  -233.67142  -270.58136  -293.38382  -370.63818\n",
      "  -337.30267  -325.50183  -419.82126 ]\n",
      " [ -73.155815  -72.49685   -70.23197   -81.12084   -88.135925 -111.55716\n",
      "  -101.79509   -97.98986  -125.998474]]\n",
      "J tensor([-290.2847, -413.9460, -198.3608, -233.6714,  -70.2320])\n",
      "P tensor([-404.2238, -576.3563, -276.3087, -325.5018,  -97.9899])\n",
      "expected_state_action_values  tensor([[-299.8708, -402.4161],\n",
      "        [-423.3165, -569.4858],\n",
      "        [-206.4671, -276.6203],\n",
      "        [-237.7716, -320.4189],\n",
      "        [ -69.9760,  -94.9581]])\n",
      "reward -54.71440904060414\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.2270],\n",
      "        [-30.9730],\n",
      "        [-51.1810],\n",
      "        [-40.3589],\n",
      "        [ -7.8685]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-132.1995, -130.3642, -126.2556, -145.1127, -150.4425, -190.5098,\n",
      "         -167.8320, -173.0341, -215.4509],\n",
      "        [-297.3282, -293.8929, -284.9956, -328.1541, -339.9859, -429.2891,\n",
      "         -377.6214, -390.2251, -486.6982],\n",
      "        [-476.1387, -469.9804, -455.8956, -523.8755, -543.0928, -686.4744,\n",
      "         -604.3469, -623.9945, -778.0943],\n",
      "        [-343.7397, -338.9822, -329.0082, -378.1023, -391.9377, -495.6182,\n",
      "         -436.3542, -450.3615, -561.3404],\n",
      "        [ -81.2286,  -80.2696,  -77.4881,  -88.9282,  -92.3367, -116.9528,\n",
      "         -103.2250, -106.2821, -132.1291]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-130.3642, -167.8320],\n",
      "        [-284.9956, -377.6214],\n",
      "        [-476.1387, -623.9945],\n",
      "        [-338.9822, -436.3542],\n",
      "        [ -88.9282, -106.2821]], grad_fn=<GatherBackward>)\n",
      "A [[-189.35526  -186.63005  -180.96135  -207.86655  -215.48268  -272.86276\n",
      "  -240.33414  -247.84933  -308.64706 ]\n",
      " [-261.78186  -258.69922  -250.88371  -288.94168  -299.3296   -377.99957\n",
      "  -332.48746  -343.55756  -428.4565  ]\n",
      " [-487.5107   -480.71744  -466.26132  -535.08264  -554.96704  -702.1146\n",
      "  -618.543    -638.1539   -795.5395  ]\n",
      " [-358.4207   -353.78842  -343.29517  -394.82147  -409.2191   -517.0571\n",
      "  -455.08875  -469.94083  -585.99854 ]\n",
      " [ -84.16666   -83.12368   -80.319336  -92.192314  -95.68366  -121.2329\n",
      "  -106.95739  -110.14188  -136.87453 ]]\n",
      "J tensor([-180.9613, -250.8837, -466.2613, -343.2952,  -80.3193])\n",
      "P tensor([-240.3341, -332.4875, -618.5430, -455.0887, -106.9574])\n",
      "expected_state_action_values  tensor([[-182.0922, -235.5278],\n",
      "        [-256.7684, -330.2117],\n",
      "        [-470.8162, -607.8698],\n",
      "        [-349.3245, -449.9388],\n",
      "        [ -80.1559, -104.1301]])\n",
      "reward -52.21151458056428\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-25.6968],\n",
      "        [-10.7009],\n",
      "        [ -7.8685],\n",
      "        [-47.6074],\n",
      "        [-40.3589]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [4, 8],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-166.0013, -182.6050, -160.0472, -185.9698, -201.2243, -253.8837,\n",
      "         -230.6388, -223.0569, -287.8756],\n",
      "        [ -76.3499,  -83.8754,  -73.2466,  -84.6583,  -91.8525, -116.3185,\n",
      "         -106.0455, -102.1826, -131.4113],\n",
      "        [ -73.5389,  -80.8313,  -70.5216,  -81.4942,  -88.4605, -111.9853,\n",
      "         -102.1390,  -98.4036, -126.6001],\n",
      "        [-356.5724, -391.2495, -343.1764, -397.2365, -430.3790, -543.8571,\n",
      "         -494.7693, -477.8777, -616.7090],\n",
      "        [-309.0048, -339.0112, -297.4124, -344.2202, -372.9606, -471.3464,\n",
      "         -428.8475, -414.1369, -534.2512]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-185.9698, -230.6388],\n",
      "        [ -91.8525, -131.4113],\n",
      "        [ -81.4942,  -98.4036],\n",
      "        [-343.1764, -494.7693],\n",
      "        [-339.0112, -428.8475]], grad_fn=<GatherBackward>)\n",
      "A [[-186.56439  -205.16641  -179.84702  -208.73633  -225.9094   -285.19672\n",
      "  -259.15305  -250.5766   -323.23154 ]\n",
      " [ -99.00642  -108.620514  -95.03744  -109.85811  -119.134636 -150.86224\n",
      "  -137.45763  -132.49959  -170.50995 ]\n",
      " [ -76.34991   -83.8754    -73.2466    -84.65828   -91.852455 -116.31849\n",
      "  -106.045525 -102.1826   -131.41135 ]\n",
      " [-385.21014  -422.84177  -370.83728  -429.23978  -465.06644  -587.5846\n",
      "  -534.53864  -516.3651   -666.3959  ]\n",
      " [-321.2813   -352.83206  -309.46423  -358.46997  -388.3345   -490.366\n",
      "  -446.0129   -430.94174  -556.17816 ]]\n",
      "J tensor([-179.8470,  -95.0374,  -73.2466, -370.8373, -309.4642])\n",
      "P tensor([-250.5766, -132.4996, -102.1826, -516.3651, -430.9417])\n",
      "expected_state_action_values  tensor([[-187.5591, -251.2157],\n",
      "        [ -96.2346, -129.9505],\n",
      "        [ -73.7904,  -99.8328],\n",
      "        [-381.3610, -512.3360],\n",
      "        [-318.8767, -428.2065]])\n",
      "1\n",
      "reward -50.30052598087004\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-36.4907],\n",
      "        [-56.4395],\n",
      "        [-51.1810],\n",
      "        [ -4.6880],\n",
      "        [-34.1165]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 7],\n",
      "        [0, 7],\n",
      "        [3, 7],\n",
      "        [0, 6]])\n",
      "policy_net1(state_batch) tensor([[-235.6852, -245.9975, -238.7551, -274.5004, -306.5504, -359.7201,\n",
      "         -338.0616, -326.8189, -385.5375],\n",
      "        [-486.8824, -507.1906, -492.0591, -563.7918, -630.4594, -740.9150,\n",
      "         -697.3345, -673.2805, -794.3856],\n",
      "        [-450.8918, -470.5694, -456.5154, -524.1750, -585.7076, -687.3537,\n",
      "         -646.2942, -624.7264, -737.5123],\n",
      "        [ -43.9439,  -46.0708,  -44.2822,  -50.8426,  -56.9374,  -66.9589,\n",
      "          -63.1406,  -60.8336,  -71.4196],\n",
      "        [-276.9827, -289.0073, -280.5341, -322.6490, -360.2525, -422.7185,\n",
      "         -397.1813, -384.0311, -453.3079]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-238.7551, -338.0616],\n",
      "        [-492.0591, -673.2805],\n",
      "        [-450.8918, -624.7264],\n",
      "        [ -50.8426,  -60.8336],\n",
      "        [-276.9827, -397.1813]], grad_fn=<GatherBackward>)\n",
      "A [[-272.69925  -284.5453   -276.40555  -318.00906  -354.97046  -416.46793\n",
      "  -391.19626  -378.311    -446.33698 ]\n",
      " [-490.93768  -511.95496  -496.7073   -569.8789   -636.96643  -747.8765\n",
      "  -703.4675   -679.68634  -802.303   ]\n",
      " [-461.65485  -481.27667  -466.85867  -535.34235  -598.50183  -702.955\n",
      "  -661.3933   -638.8502   -753.97345 ]\n",
      " [ -53.653755  -56.183605  -54.0712    -61.99655   -69.44189   -81.675644\n",
      "   -77.0276    -74.219894  -87.20716 ]\n",
      " [-289.04886  -301.3817   -292.63394  -336.26804  -375.60028  -440.89056\n",
      "  -414.4416   -400.52728  -472.61288 ]]\n",
      "J tensor([-272.6992, -490.9377, -461.6548,  -53.6538, -289.0489])\n",
      "P tensor([-378.3110, -679.6863, -638.8502,  -74.2199, -400.5273])\n",
      "expected_state_action_values  tensor([[-281.9200, -376.9706],\n",
      "        [-498.2834, -668.1572],\n",
      "        [-466.6703, -626.1462],\n",
      "        [ -52.9764,  -71.4859],\n",
      "        [-294.2604, -394.5910]])\n",
      "5\n",
      "reward -49.955535262955216\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-43.9882],\n",
      "        [-19.2270],\n",
      "        [ -4.2900],\n",
      "        [-24.1008],\n",
      "        [-32.2255]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-414.2903, -408.7362, -415.8560, -474.6656, -508.6569, -597.2462,\n",
      "         -544.1596, -550.7189, -640.3633],\n",
      "        [-139.7540, -137.8103, -139.9752, -159.7160, -171.1993, -201.3500,\n",
      "         -183.5818, -185.5471, -215.4143],\n",
      "        [ -59.8690,  -59.2776,  -59.8514,  -68.1911,  -73.2436,  -86.1853,\n",
      "          -78.7762,  -79.4651,  -91.9780],\n",
      "        [-222.8543, -220.2625, -224.0027, -256.2017, -274.3083, -321.8795,\n",
      "         -292.9590, -296.7718, -344.8749],\n",
      "        [-211.4948, -209.1770, -212.6295, -243.3084, -260.5143, -305.5287,\n",
      "         -278.0655, -281.7472, -327.5552]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-415.8560, -544.1596],\n",
      "        [-137.8103, -183.5818],\n",
      "        [ -68.1911,  -79.4651],\n",
      "        [-224.0027, -292.9590],\n",
      "        [-212.6295, -278.0655]], grad_fn=<GatherBackward>)\n",
      "A [[-438.49585  -432.4874   -439.93832  -501.70065  -537.7888   -631.7606\n",
      "  -575.83356  -582.5543   -677.1782  ]\n",
      " [-200.90833  -198.01987  -201.35486  -229.64429  -246.12718  -289.4562\n",
      "  -263.86108  -266.76016  -309.74045 ]\n",
      " [ -55.408524  -54.882423  -55.390144  -63.13627   -67.810745  -79.78881\n",
      "   -72.92765   -73.56204   -85.12101 ]\n",
      " [-211.49484  -209.17696  -212.62946  -243.30838  -260.51434  -305.52866\n",
      "  -278.0655   -281.74722  -327.55518 ]\n",
      " [-274.279    -271.06628  -275.73746  -315.33362  -337.63824  -396.1139\n",
      "  -360.5274   -365.25436  -424.58905 ]]\n",
      "J tensor([-432.4874, -198.0199,  -54.8824, -209.1770, -271.0663])\n",
      "P tensor([-575.8336, -263.8611,  -72.9277, -278.0655, -360.5274])\n",
      "expected_state_action_values  tensor([[-433.2268, -562.2383],\n",
      "        [-197.4449, -256.7020],\n",
      "        [ -53.6842,  -69.9249],\n",
      "        [-212.3601, -274.3598],\n",
      "        [-276.1852, -356.7002]])\n",
      "1\n",
      "reward -46.94262949594488\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-46.9426],\n",
      "        [-53.1408],\n",
      "        [-25.6968],\n",
      "        [-24.7273],\n",
      "        [-24.8327]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [4, 7],\n",
      "        [3, 6],\n",
      "        [0, 7],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-501.9897, -520.3760, -527.2425, -551.6457, -616.2805, -723.8713,\n",
      "         -680.5318, -645.8548, -775.5475],\n",
      "        [-560.3542, -580.5631, -587.6261, -613.2756, -685.9336, -806.4927,\n",
      "         -759.1801, -719.7419, -863.9659],\n",
      "        [-207.2572, -215.2343, -217.8724, -228.4957, -255.0737, -299.3628,\n",
      "         -281.2141, -267.0804, -320.6456],\n",
      "        [-255.8889, -265.3161, -268.5167, -280.7239, -313.7257, -368.8104,\n",
      "         -346.8993, -329.0337, -394.7195],\n",
      "        [-265.0300, -274.9232, -278.2649, -290.8850, -325.1162, -382.0746,\n",
      "         -359.3972, -340.9250, -408.8934]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-501.9897, -645.8548],\n",
      "        [-685.9336, -719.7419],\n",
      "        [-228.4957, -281.2141],\n",
      "        [-255.8889, -329.0337],\n",
      "        [-278.2649, -340.9250]], grad_fn=<GatherBackward>)\n",
      "A [[-477.04745 -494.71124 -501.07962 -524.3162  -585.79297 -687.9096\n",
      "  -646.7525  -613.8453  -737.22406]\n",
      " [-569.6614  -589.8666  -597.2809  -623.2288  -697.00903 -819.84076\n",
      "  -771.72424 -731.5132  -877.83246]\n",
      " [-234.0371  -242.9913  -245.99149 -257.7519  -287.78793 -337.91205\n",
      "  -317.49304 -301.47958 -361.79288]\n",
      " [-253.97838 -263.51822 -266.6403  -278.9772  -311.73196 -366.2125\n",
      "  -344.38025 -326.77112 -392.20398]\n",
      " [-246.70634 -255.91939 -259.00156 -270.84244 -302.7205  -355.66962\n",
      "  -334.5652  -317.36734 -380.7583 ]]\n",
      "J tensor([-477.0475, -569.6614, -234.0371, -253.9784, -246.7063])\n",
      "P tensor([-613.8453, -731.5132, -301.4796, -326.7711, -317.3673])\n",
      "expected_state_action_values  tensor([[-476.2853, -599.4034],\n",
      "        [-565.8361, -711.5027],\n",
      "        [-236.3302, -297.0284],\n",
      "        [-253.3078, -318.8213],\n",
      "        [-246.8684, -310.4632]])\n",
      "1\n",
      "reward -43.03660993253308\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 10.5880, 11.7574,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-51.4207],\n",
      "        [-37.9897],\n",
      "        [-36.9724],\n",
      "        [-41.6451],\n",
      "        [-19.2270]])\n",
      "action_batch tensor([[1, 7],\n",
      "        [4, 7],\n",
      "        [3, 7],\n",
      "        [1, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-517.9911, -567.0588, -549.1238, -625.1921, -624.2304, -788.1429,\n",
      "         -764.4795, -679.9323, -844.7158],\n",
      "        [-346.1410, -379.4326, -367.5718, -419.4581, -418.5270, -527.7025,\n",
      "         -511.3675, -455.2321, -565.7036],\n",
      "        [-221.3499, -242.6974, -234.9111, -267.9191, -267.4036, -337.3344,\n",
      "         -327.0291, -290.9984, -361.3546],\n",
      "        [-421.5704, -461.7120, -447.3310, -510.0994, -509.0078, -642.2330,\n",
      "         -622.4863, -553.9652, -688.3617],\n",
      "        [-131.8129, -144.3424, -139.5770, -159.1482, -158.8491, -200.7055,\n",
      "         -194.6471, -173.0253, -214.7185]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-567.0588, -679.9323],\n",
      "        [-418.5270, -455.2321],\n",
      "        [-267.9191, -290.9984],\n",
      "        [-461.7120, -622.4863],\n",
      "        [-144.3424, -194.6471]], grad_fn=<GatherBackward>)\n",
      "A [[-504.2945  -552.14905 -534.5038  -608.38293 -607.567   -767.0927\n",
      "  -744.2139  -661.848   -822.30347]\n",
      " [-343.31146 -376.00516 -364.31305 -415.40836 -414.54227 -523.0733\n",
      "  -507.0322  -451.15924 -560.41223]\n",
      " [-303.01718 -332.3282  -321.97964 -367.64377 -366.71002 -462.2885\n",
      "  -447.809   -398.77625 -495.40295]\n",
      " [-391.6706  -429.16428 -415.6718  -474.1305  -473.13376 -596.7714\n",
      "  -578.40265 -514.8105  -639.846  ]\n",
      " [-189.66368 -207.6009  -200.96265 -229.03317 -228.5726  -288.78885\n",
      "  -280.02533 -248.98401 -309.0178 ]]\n",
      "J tensor([-504.2945, -343.3115, -303.0172, -391.6706, -189.6637])\n",
      "P tensor([-661.8480, -451.1592, -398.7762, -514.8105, -248.9840])\n",
      "expected_state_action_values  tensor([[-505.2857, -647.0839],\n",
      "        [-346.9700, -444.0330],\n",
      "        [-309.6878, -395.8710],\n",
      "        [-394.1486, -504.9745],\n",
      "        [-189.9243, -243.3126]])\n",
      "1\n",
      "reward -50.39405533434282\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-52.6400],\n",
      "        [-25.3215],\n",
      "        [-28.1379],\n",
      "        [-10.7009],\n",
      "        [-41.6451]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [4, 8],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-452.9994, -472.5096, -480.5319, -569.9008, -507.7368, -689.6948,\n",
      "         -648.8604, -574.8102, -740.0329],\n",
      "        [-245.2170, -256.2538, -260.7073, -310.1766, -276.0506, -374.4358,\n",
      "         -351.6874, -311.9859, -401.6119],\n",
      "        [-272.7519, -284.5427, -289.8769, -344.7074, -306.7486, -416.3664,\n",
      "         -391.1277, -346.7867, -446.2025],\n",
      "        [ -79.9491,  -83.4618,  -84.5828, -100.2685,  -89.3980, -121.6839,\n",
      "         -114.6584, -101.3449, -130.0242],\n",
      "        [-400.1962, -417.1936, -424.6677, -504.1319, -448.9215, -609.7624,\n",
      "         -573.3440, -508.0041, -653.9986]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-480.5319, -574.8102],\n",
      "        [-260.7073, -351.6874],\n",
      "        [-289.8769, -391.1277],\n",
      "        [ -89.3980, -130.0242],\n",
      "        [-417.1936, -573.3440]], grad_fn=<GatherBackward>)\n",
      "A [[-484.22287 -504.85693 -513.39886 -608.5157  -542.2262  -736.839\n",
      "  -693.42334 -614.09296 -790.5276 ]\n",
      " [-218.03177 -227.43958 -231.2731  -274.52652 -244.46713 -332.22525\n",
      "  -312.4328  -276.75912 -356.0014 ]\n",
      " [-229.35089 -239.34192 -243.69199 -289.8881  -258.0331  -350.08276\n",
      "  -328.92084 -291.61307 -375.38297]\n",
      " [-103.89047 -108.30982 -109.97242 -130.3875  -116.1934  -158.15053\n",
      "  -148.9319  -131.69151 -169.06514]\n",
      " [-372.35855 -388.36868 -395.19522 -469.27933 -417.91632 -567.436\n",
      "  -533.5249  -472.7995  -608.8164 ]]\n",
      "J tensor([-484.2229, -218.0318, -229.3509, -103.8905, -372.3586])\n",
      "P tensor([-614.0930, -276.7591, -291.6131, -131.6915, -472.7995])\n",
      "expected_state_action_values  tensor([[-488.4405, -605.3236],\n",
      "        [-221.5501, -274.4047],\n",
      "        [-234.5537, -290.5897],\n",
      "        [-104.2023, -129.2233],\n",
      "        [-376.7677, -467.1646]])\n",
      "1\n",
      "reward -52.86274615074272\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-28.3724],\n",
      "        [-43.9882],\n",
      "        [-47.6074],\n",
      "        [-51.1810],\n",
      "        [-31.9069]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-205.5368, -203.8540, -208.2341, -259.4157, -248.1717, -313.4169,\n",
      "         -285.6093, -270.3107, -317.8050],\n",
      "        [-353.6420, -350.3756, -358.2797, -446.1339, -426.8196, -539.0217,\n",
      "         -491.2292, -464.9030, -546.8696],\n",
      "        [-357.6970, -354.2551, -362.3277, -451.0612, -431.5154, -545.1357,\n",
      "         -496.8210, -470.1247, -552.8613],\n",
      "        [-428.3464, -424.5886, -434.0834, -540.2909, -516.9417, -652.8384,\n",
      "         -594.9935, -563.1721, -662.3348],\n",
      "        [-252.3668, -250.3062, -256.0915, -319.3665, -305.3578, -385.3485,\n",
      "         -350.8811, -332.3190, -390.6684]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-205.5368, -285.6093],\n",
      "        [-358.2797, -491.2292],\n",
      "        [-362.3277, -496.8210],\n",
      "        [-428.3464, -563.1721],\n",
      "        [-256.0915, -350.8811]], grad_fn=<GatherBackward>)\n",
      "A [[-213.23604 -211.43169 -216.05702 -268.89255 -257.2844  -325.08148\n",
      "  -296.34067 -280.37094 -329.3216 ]\n",
      " [-374.45236 -370.86258 -379.14493 -471.65536 -451.35202 -570.3367\n",
      "  -519.9959  -491.92316 -578.4472 ]\n",
      " [-386.45963 -382.90482 -391.56573 -487.44736 -466.33817 -589.02094\n",
      "  -536.8017  -508.0377  -597.465  ]\n",
      " [-439.5558  -435.20404 -444.8883  -553.0299  -529.33685 -669.117\n",
      "  -610.2603  -577.15173 -678.5797 ]\n",
      " [-233.1204  -231.09122 -236.41115 -294.66077 -281.7804  -355.77945\n",
      "  -324.06375 -306.78604 -360.5491 ]]\n",
      "J tensor([-211.4317, -370.8626, -382.9048, -435.2040, -231.0912])\n",
      "P tensor([-280.3709, -491.9232, -508.0377, -577.1517, -306.7860])\n",
      "expected_state_action_values  tensor([[-218.6609, -280.7062],\n",
      "        [-377.7645, -486.7190],\n",
      "        [-392.2217, -504.8414],\n",
      "        [-442.8646, -570.6176],\n",
      "        [-239.8890, -308.0143]])\n",
      "5\n",
      "reward -48.145053745589856\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.7497, 20.6804, 11.0180,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -7.8685],\n",
      "        [-28.1379],\n",
      "        [-54.7144],\n",
      "        [-14.0164],\n",
      "        [-25.6968]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[ -80.8330,  -75.9174,  -80.8697,  -95.8934,  -91.9125, -116.3802,\n",
      "         -103.0388, -103.7312, -117.5664],\n",
      "        [-286.8248, -268.9216, -288.3305, -342.9964, -327.9501, -414.2890,\n",
      "         -365.4636, -369.2350, -419.4756],\n",
      "        [-518.4366, -485.8945, -520.2764, -617.3494, -590.8976, -747.0571,\n",
      "         -659.9567, -666.1406, -757.2081],\n",
      "        [-190.9609, -178.8474, -191.4025, -227.1770, -217.3926, -275.1897,\n",
      "         -243.1329, -245.2085, -278.3826],\n",
      "        [-185.9997, -174.7085, -186.9990, -222.7867, -212.9956, -268.8374,\n",
      "         -237.0677, -239.6382, -272.4403]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -95.8934, -103.7312],\n",
      "        [-288.3305, -365.4636],\n",
      "        [-520.2764, -659.9567],\n",
      "        [-178.8474, -243.1329],\n",
      "        [-222.7867, -237.0677]], grad_fn=<GatherBackward>)\n",
      "A [[ -84.13697   -78.971016  -84.2059    -99.86376   -95.67682  -121.18653\n",
      "  -107.24848  -107.987885 -122.33674 ]\n",
      " [-240.97911  -226.01508  -242.18565  -288.2065   -275.62607  -348.04077\n",
      "  -307.0784   -310.22235  -352.612   ]\n",
      " [-502.37198  -470.78088  -504.09314  -597.9417   -572.3847   -723.77985\n",
      "  -639.512    -645.3844   -733.444   ]\n",
      " [-146.89372  -137.71938  -147.23503  -174.8927   -167.36684  -211.7663\n",
      "  -187.0851   -188.70633  -214.27325 ]\n",
      " [-209.37201  -196.61763  -210.46869  -250.5079   -239.52377  -302.48895\n",
      "  -266.8093   -269.6451   -306.3935  ]]\n",
      "J tensor([ -78.9710, -226.0151, -470.7809, -137.7194, -196.6176])\n",
      "P tensor([-107.2485, -307.0784, -639.5120, -187.0851, -266.8093])\n",
      "expected_state_action_values  tensor([[ -78.9424, -104.3921],\n",
      "        [-231.5515, -304.5085],\n",
      "        [-478.4172, -630.2752],\n",
      "        [-137.9638, -182.3929],\n",
      "        [-202.6526, -265.8251]])\n",
      "5\n",
      "reward -46.52179547431805\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 9.9864,  6.4395, 17.3845,  9.2981,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-54.0051],\n",
      "        [-45.2304],\n",
      "        [-32.9790],\n",
      "        [-25.6968],\n",
      "        [ -6.7672]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [3, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-390.1925, -346.5017, -373.9361, -446.6921, -444.7943, -562.2090,\n",
      "         -480.7630, -517.5906, -569.9671],\n",
      "        [-395.5560, -351.2633, -379.1123, -453.2121, -451.1670, -570.1132,\n",
      "         -487.3417, -524.8309, -578.2155],\n",
      "        [-256.9088, -228.1354, -246.2652, -294.8205, -293.3621, -370.5917,\n",
      "         -316.6192, -341.0496, -375.7782],\n",
      "        [-175.8530, -156.5194, -168.8762, -202.7369, -201.5608, -254.2473,\n",
      "         -216.9189, -233.9828, -257.8380],\n",
      "        [ -49.7382,  -44.4143,  -47.4731,  -56.7268,  -56.5978,  -71.6512,\n",
      "          -61.4505,  -65.9212,  -72.2755]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-390.1925, -517.5906],\n",
      "        [-351.2633, -487.3417],\n",
      "        [-256.9088, -341.0496],\n",
      "        [-202.7369, -216.9189],\n",
      "        [ -56.7268,  -65.9212]], grad_fn=<GatherBackward>)\n",
      "A [[-452.55496  -402.1949   -434.0952   -519.2345   -516.7868   -652.6542\n",
      "  -557.67224  -600.9138   -662.1425  ]\n",
      " [-401.416    -356.4111   -384.79776  -460.15158  -457.99622  -578.7056\n",
      "  -494.5831   -532.6979   -586.88324 ]\n",
      " [-277.79492  -246.78107  -266.54095  -319.34735  -317.6248   -401.0702\n",
      "  -342.436    -369.09406  -406.64825 ]\n",
      " [-197.64409  -175.87491  -189.77904  -227.58987  -226.3013   -285.62354\n",
      "  -243.7552   -262.87006  -289.50528 ]\n",
      " [ -77.02599   -68.567116  -73.58634   -87.88493   -87.63106  -110.90831\n",
      "   -95.038704 -102.05316  -112.10308 ]]\n",
      "J tensor([-402.1949, -356.4111, -246.7811, -175.8749,  -68.5671])\n",
      "P tensor([-557.6722, -494.5831, -342.4360, -243.7552,  -95.0387])\n",
      "expected_state_action_values  tensor([[-415.9805, -555.9101],\n",
      "        [-366.0004, -490.3552],\n",
      "        [-255.0820, -341.1714],\n",
      "        [-183.9842, -245.0765],\n",
      "        [ -68.4777,  -92.3021]])\n",
      "5\n",
      "reward -50.362626667139324\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-53.1408],\n",
      "        [ -4.6880],\n",
      "        [-11.8820],\n",
      "        [-47.6074],\n",
      "        [-38.6147]])\n",
      "action_batch tensor([[4, 7],\n",
      "        [3, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-531.9944, -473.0354, -483.7474, -553.9808, -574.8814, -727.1786,\n",
      "         -642.7238, -690.4789, -737.0497],\n",
      "        [ -48.9754,  -43.8212,  -44.4032,  -50.9441,  -52.9375,  -67.0403,\n",
      "          -59.3898,  -63.6023,  -67.5493],\n",
      "        [-116.1912, -103.3618, -105.5208, -120.9237, -125.4884, -158.9408,\n",
      "         -140.5576, -150.7833, -160.5277],\n",
      "        [-419.2350, -372.8438, -381.6089, -437.9180, -454.0298, -573.8984,\n",
      "         -506.6953, -544.7889, -581.6095],\n",
      "        [-326.4336, -290.4316, -297.0757, -340.7966, -353.3822, -446.8049,\n",
      "         -394.5627, -424.1470, -452.6292]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-574.8814, -690.4789],\n",
      "        [ -50.9441,  -63.6023],\n",
      "        [-103.3618, -140.5576],\n",
      "        [-381.6089, -506.6953],\n",
      "        [-326.4336, -424.1470]], grad_fn=<GatherBackward>)\n",
      "A [[-540.3759   -480.1627   -491.27448  -562.46924  -583.6259   -738.5892\n",
      "  -652.79816  -701.16895  -748.1812  ]\n",
      " [ -59.805485  -53.4476    -54.228745  -62.13245   -64.57385   -81.78935\n",
      "   -72.46038   -77.61721   -82.49878 ]\n",
      " [-140.09515  -124.597374 -127.25905  -146.0213   -151.43405  -191.7042\n",
      "  -169.38448  -181.86043  -193.88965 ]\n",
      " [-453.32352  -403.31638  -412.73404  -473.62576  -491.06314  -620.6015\n",
      "  -547.9126   -589.19525  -629.0385  ]\n",
      " [-355.51544  -316.24323  -323.63895  -371.36157  -384.9822   -486.7547\n",
      "  -429.7245   -462.03134  -492.9981  ]]\n",
      "J tensor([-480.1627,  -53.4476, -124.5974, -403.3164, -316.2432])\n",
      "P tensor([-652.7982,  -72.4604, -169.3845, -547.9126, -429.7245])\n",
      "expected_state_action_values  tensor([[-485.2872, -640.6591],\n",
      "        [ -52.7908,  -69.9023],\n",
      "        [-124.0196, -164.3280],\n",
      "        [-410.5922, -540.7288],\n",
      "        [-323.2336, -425.3667]])\n",
      "5\n",
      "reward -45.600435782649015\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.2009],\n",
      "        [-28.3724],\n",
      "        [-50.3626],\n",
      "        [-42.7328],\n",
      "        [-36.4907]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-307.2780, -302.3713, -307.6320, -349.0064, -324.5953, -441.4380,\n",
      "         -403.4516, -406.6238, -446.1849],\n",
      "        [-231.7247, -228.1569, -232.1613, -263.8574, -245.2802, -333.3056,\n",
      "         -304.3978, -306.9708, -336.9158],\n",
      "        [-470.0877, -461.9968, -470.6598, -534.2281, -496.7443, -675.4057,\n",
      "         -617.1083, -622.0025, -682.9644],\n",
      "        [-402.2751, -395.3873, -402.7931, -457.1286, -425.0940, -578.0010,\n",
      "         -528.1777, -532.3049, -584.2838],\n",
      "        [-251.4115, -247.4217, -252.0767, -286.5820, -266.3205, -361.8586,\n",
      "         -330.3627, -333.2077, -365.5262]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-307.6320, -403.4516],\n",
      "        [-231.7247, -304.3978],\n",
      "        [-461.9968, -617.1083],\n",
      "        [-395.3873, -528.1777],\n",
      "        [-252.0767, -330.3627]], grad_fn=<GatherBackward>)\n",
      "A [[-279.07974 -275.28415 -280.09857 -318.70395 -296.18655 -401.9011\n",
      "  -366.78076 -370.30185 -406.60513]\n",
      " [-241.70099 -237.92126 -242.18044 -274.98373 -255.66444 -347.5822\n",
      "  -317.5363  -320.12152 -351.0451 ]\n",
      " [-494.48624 -486.1419  -495.24954 -562.2767  -522.79486 -710.63116\n",
      "  -649.19934 -654.4896  -718.6985 ]\n",
      " [-403.02454 -396.30276 -403.72855 -458.51178 -426.23596 -579.39636\n",
      "  -529.2017  -533.5782  -585.7594 ]\n",
      " [-292.56268 -287.82233 -293.48593 -333.8683  -310.1405  -421.3165\n",
      "  -384.45474 -387.89813 -425.56967]]\n",
      "J tensor([-275.2841, -237.9213, -486.1419, -396.3028, -287.8223])\n",
      "P tensor([-366.7808, -317.5363, -649.1993, -529.2017, -384.4547])\n",
      "expected_state_action_values  tensor([[-282.9567, -365.3036],\n",
      "        [-242.5015, -314.1551],\n",
      "        [-487.8903, -634.6420],\n",
      "        [-399.4053, -519.0143],\n",
      "        [-295.5308, -382.4999]])\n",
      "reward -57.28065899912928\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 9.9864,  0.0000, 18.4464, 14.4922,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395, 17.3845,  9.2981,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  0.0000, 18.4464, 14.4922,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-44.9251],\n",
      "        [-38.6657],\n",
      "        [-45.2304],\n",
      "        [-45.1085],\n",
      "        [-25.6968]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[-478.3810, -469.3142, -435.5464, -517.2672, -481.0161, -653.8032,\n",
      "         -615.5746, -602.2772, -660.8300],\n",
      "        [-344.6445, -337.7865, -313.7082, -372.7639, -346.5014, -471.1998,\n",
      "         -443.5266, -433.8552, -475.8084],\n",
      "        [-464.3945, -455.2359, -422.4561, -501.3932, -466.2847, -634.2932,\n",
      "         -597.3683, -584.1982, -640.8970],\n",
      "        [-452.3371, -443.9130, -411.7971, -488.9816, -454.8041, -618.0987,\n",
      "         -582.0637, -569.4669, -624.9086],\n",
      "        [-210.0887, -206.3647, -191.4411, -228.0584, -211.8728, -287.7140,\n",
      "         -270.5444, -264.9388, -290.6961]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-435.5464, -615.5746],\n",
      "        [-313.7082, -443.5266],\n",
      "        [-455.2359, -597.3683],\n",
      "        [-411.7971, -582.0637],\n",
      "        [-228.0584, -270.5444]], grad_fn=<GatherBackward>)\n",
      "A [[-452.33713 -443.913   -411.7971  -488.9816  -454.80414 -618.0987\n",
      "  -582.0637  -569.4669  -624.90857]\n",
      " [-388.5871  -380.88422 -353.75528 -420.37326 -390.7554  -531.2917\n",
      "  -500.06433 -489.2151  -536.6177 ]\n",
      " [-472.4854  -463.1189  -429.88083 -510.3464  -474.5418  -645.4824\n",
      "  -607.8023  -594.461   -652.15894]\n",
      " [-459.0811  -450.02118 -417.592   -495.2955  -460.71207 -626.8716\n",
      "  -590.55676 -577.39905 -633.1746 ]\n",
      " [-236.98271 -232.7309  -215.9248  -256.98874 -238.7705  -324.41904\n",
      "  -305.12595 -298.7471  -327.6376 ]]\n",
      "J tensor([-411.7971, -353.7553, -429.8808, -417.5920, -215.9248])\n",
      "P tensor([-569.4669, -489.2151, -594.4610, -577.3990, -298.7471])\n",
      "expected_state_action_values  tensor([[-415.5424, -557.4453],\n",
      "        [-357.0454, -478.9593],\n",
      "        [-432.1231, -580.2453],\n",
      "        [-420.9413, -564.7676],\n",
      "        [-220.0291, -294.5692]])\n",
      "5\n",
      "reward -57.392114771631356\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-41.6451],\n",
      "        [-47.4361],\n",
      "        [-32.9790],\n",
      "        [-32.6959],\n",
      "        [-39.9305]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [2, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-448.8806, -419.7156, -427.3299, -466.1213, -451.2458, -613.4194,\n",
      "         -560.3612, -564.9007, -620.1378],\n",
      "        [-518.8587, -485.1173, -493.6251, -537.6823, -520.8018, -708.3809,\n",
      "         -647.5423, -652.4664, -716.1359],\n",
      "        [-288.9910, -270.2803, -275.0682, -300.2506, -290.6363, -395.0449,\n",
      "         -360.8349, -363.7510, -399.3214],\n",
      "        [-351.8414, -329.2806, -334.9645, -365.2840, -353.6808, -480.8053,\n",
      "         -439.2800, -442.8499, -485.9965],\n",
      "        [-341.1448, -319.0773, -324.8223, -354.4287, -343.1393, -466.3221,\n",
      "         -425.9984, -429.4442, -471.3766]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-419.7156, -560.3612],\n",
      "        [-493.6251, -647.5423],\n",
      "        [-288.9910, -363.7510],\n",
      "        [-334.9645, -442.8499],\n",
      "        [-324.8223, -425.9984]], grad_fn=<GatherBackward>)\n",
      "A [[-418.78677 -391.76627 -398.73965 -435.07004 -421.2058  -572.37024\n",
      "  -522.8403  -527.1603  -578.8573 ]\n",
      " [-488.3842  -456.7808  -464.6147  -506.05502 -490.2393  -666.7097\n",
      "  -609.5217  -614.15704 -674.1782 ]\n",
      " [-314.87137 -294.59396 -299.95535 -327.66794 -317.045   -430.7506\n",
      "  -393.22867 -396.62408 -435.38806]\n",
      " [-323.87567 -303.10956 -308.16058 -335.78558 -325.2406  -442.3153\n",
      "  -404.31015 -407.4318  -447.07184]\n",
      " [-371.41708 -347.46033 -353.833   -386.24863 -373.84122 -507.9382\n",
      "  -463.848   -467.76114 -513.3961 ]]\n",
      "J tensor([-391.7663, -456.7808, -294.5940, -303.1096, -347.4603])\n",
      "P tensor([-522.8403, -609.5217, -393.2287, -404.3102, -463.8480])\n",
      "expected_state_action_values  tensor([[-394.2347, -512.2013],\n",
      "        [-458.5388, -596.0056],\n",
      "        [-298.1136, -386.8848],\n",
      "        [-305.4944, -396.5750],\n",
      "        [-352.6448, -457.3937]])\n",
      "reward -56.90305715760986\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-28.1379],\n",
      "        [-42.7328],\n",
      "        [-51.1810],\n",
      "        [-47.9592],\n",
      "        [-18.9634]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [0, 6],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[-307.6798, -261.2265, -267.3099, -305.7521, -295.7020, -401.4927,\n",
      "         -355.2213, -358.0905, -405.7778],\n",
      "        [-424.0673, -359.7802, -367.7789, -419.5822, -406.2834, -552.1618,\n",
      "         -489.2709, -492.6161, -558.5076],\n",
      "        [-506.1236, -429.9198, -439.3078, -501.4798, -485.4641, -659.3607,\n",
      "         -583.9717, -588.4033, -667.2510],\n",
      "        [-449.8325, -381.7800, -390.2238, -445.3319, -431.1156, -585.8590,\n",
      "         -518.9680, -522.6890, -592.6387],\n",
      "        [-185.1642, -157.5067, -160.7634, -183.9674, -177.9756, -241.5956,\n",
      "         -213.8162, -215.5356, -244.3028]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-267.3099, -355.2213],\n",
      "        [-359.7802, -489.2709],\n",
      "        [-506.1236, -588.4033],\n",
      "        [-449.8325, -518.9680],\n",
      "        [-183.9674, -213.8162]], grad_fn=<GatherBackward>)\n",
      "A [[-257.4865  -218.69453 -223.64925 -255.91359 -247.55946 -335.97287\n",
      "  -297.30865 -299.6883  -339.76508]\n",
      " [-423.99805 -359.87164 -367.87637 -420.01532 -406.5468  -552.36334\n",
      "  -489.19913 -492.78406 -558.77344]\n",
      " [-518.21814 -439.78055 -449.33392 -512.226   -496.07574 -674.44196\n",
      "  -597.7545  -601.7941  -682.2444 ]\n",
      " [-455.6777  -386.86588 -395.3874  -451.08926 -436.76233 -593.46625\n",
      "  -525.7857  -529.5469  -600.27216]\n",
      " [-182.59592 -155.22118 -158.46872 -181.24449 -175.34293 -238.17735\n",
      "  -210.82846 -212.44724 -240.70293]]\n",
      "J tensor([-218.6945, -359.8716, -439.7805, -386.8659, -155.2212])\n",
      "P tensor([-297.3087, -489.1991, -597.7545, -525.7857, -210.8285])\n",
      "expected_state_action_values  tensor([[-224.9630, -295.7157],\n",
      "        [-366.6173, -483.0120],\n",
      "        [-446.9835, -589.1601],\n",
      "        [-396.1385, -521.1663],\n",
      "        [-158.6624, -208.7090]])\n",
      "reward -57.45075251679578\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-51.1810],\n",
      "        [-51.4207],\n",
      "        [-33.5341],\n",
      "        [-38.6657],\n",
      "        [-35.2038]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [1, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-457.9770, -428.1936, -397.8620, -456.5768, -460.9468, -625.7067,\n",
      "         -536.8037, -576.3124, -633.6049],\n",
      "        [-500.9749, -467.7744, -434.6137, -497.8511, -502.9227, -683.5256,\n",
      "         -586.9544, -629.4811, -691.8522],\n",
      "        [-250.2286, -233.9725, -217.5247, -250.0245, -252.2507, -342.3261,\n",
      "         -293.5039, -315.1788, -346.2439],\n",
      "        [-297.3326, -277.7157, -258.2777, -296.6435, -299.3583, -406.4688,\n",
      "         -348.6262, -374.1923, -411.1688],\n",
      "        [-215.0150, -201.1809, -186.9247, -214.9355, -216.8742, -294.1925,\n",
      "         -252.2506, -270.8989, -297.6606]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-457.9770, -576.3124],\n",
      "        [-467.7744, -629.4811],\n",
      "        [-217.5247, -293.5039],\n",
      "        [-258.2777, -348.6262],\n",
      "        [-186.9247, -252.2506]], grad_fn=<GatherBackward>)\n",
      "A [[-469.9957  -438.96863 -407.83115 -467.3188  -472.02896 -641.4156\n",
      "  -550.7085  -590.7124  -649.24695]\n",
      " [-483.65143 -451.69885 -419.50656 -480.3766  -485.38986 -659.698\n",
      "  -566.64355 -607.6131  -667.8621 ]\n",
      " [-244.86588 -229.21237 -213.10045 -245.26842 -247.34344 -335.33673\n",
      "  -287.30875 -308.7874  -339.3026 ]\n",
      " [-334.9351  -312.8629  -290.9861  -334.2341  -337.2895  -457.88422\n",
      "  -392.69592 -421.55435 -463.30725]\n",
      " [-249.42812 -233.59471 -217.19981 -250.1786  -252.22977 -341.7741\n",
      "  -292.70444 -314.73352 -345.9072 ]]\n",
      "J tensor([-407.8311, -419.5066, -213.1004, -290.9861, -217.1998])\n",
      "P tensor([-550.7085, -566.6436, -287.3087, -392.6959, -292.7044])\n",
      "expected_state_action_values  tensor([[-418.2290, -546.8187],\n",
      "        [-428.9766, -561.3999],\n",
      "        [-225.3245, -292.1119],\n",
      "        [-300.5532, -392.0920],\n",
      "        [-230.6836, -298.6378]])\n",
      "5\n",
      "reward -60.746424317743276\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.2340,  6.4395, 14.7698, 11.9507,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [18.5462,  3.2197, 18.7128, 14.9720,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2688,  0.0000, 14.4961, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-39.9305],\n",
      "        [-60.7464],\n",
      "        [-50.3941],\n",
      "        [-33.9160],\n",
      "        [-30.9730]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [0, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-279.9524, -261.8547, -267.5891, -293.5539, -296.2851, -401.9685,\n",
      "         -355.8554, -358.6687, -407.0925],\n",
      "        [-468.2990, -438.0126, -447.6347, -490.8426, -495.4782, -672.1383,\n",
      "         -595.0627, -599.8608, -681.1793],\n",
      "        [-361.5919, -338.3746, -345.6516, -379.2821, -382.8232, -519.1280,\n",
      "         -459.5223, -463.3152, -526.2958],\n",
      "        [-264.4914, -247.6952, -253.1610, -278.0881, -280.5080, -380.2657,\n",
      "         -336.3691, -339.3280, -385.0685],\n",
      "        [-254.3359, -238.4696, -243.4874, -267.6100, -269.9377, -365.7148,\n",
      "         -323.4494, -326.4346, -370.6701]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-267.5891, -355.8554],\n",
      "        [-438.0126, -595.0627],\n",
      "        [-361.5919, -463.3152],\n",
      "        [-264.4914, -339.3280],\n",
      "        [-243.4874, -323.4494]], grad_fn=<GatherBackward>)\n",
      "A [[-302.95593 -283.43915 -289.76154 -318.041   -320.88904 -435.24606\n",
      "  -385.1491  -388.3518  -440.73535]\n",
      " [-495.22974 -463.1739  -473.39227 -519.11194 -524.0017  -710.806\n",
      "  -629.27466 -634.367   -720.4035 ]\n",
      " [-406.74194 -380.34756 -388.73206 -426.29715 -430.2455  -583.82983\n",
      "  -516.8215  -520.9631  -591.4643 ]\n",
      " [-270.2117  -252.81622 -258.1664  -283.00848 -285.6673  -387.81244\n",
      "  -343.3849  -346.05017 -392.6372 ]\n",
      " [-224.49118 -210.43297 -214.87885 -236.22273 -238.25096 -322.83212\n",
      "  -285.5079  -288.11307 -327.1209 ]]\n",
      "J tensor([-283.4391, -463.1739, -380.3476, -252.8162, -210.4330])\n",
      "P tensor([-385.1491, -629.2747, -516.8215, -343.3849, -285.5079])\n",
      "expected_state_action_values  tensor([[-295.0258, -386.5647],\n",
      "        [-477.6029, -627.0936],\n",
      "        [-392.7068, -515.5334],\n",
      "        [-261.4506, -342.9624],\n",
      "        [-220.3627, -287.9301]])\n",
      "5\n",
      "reward -67.39864016150275\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [18.5462,  3.2197, 18.7128, 14.9720,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  6.4395, 19.5042,  8.9866,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-48.4121],\n",
      "        [ -4.2900],\n",
      "        [-57.4508],\n",
      "        [-33.5341],\n",
      "        [-23.4019]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-423.8283, -395.9309, -402.9354, -421.9263, -426.1509, -578.9305,\n",
      "         -528.7575, -533.0687, -585.8484],\n",
      "        [ -56.9526,  -53.5071,  -53.9866,  -56.5042,  -57.1811,  -77.8073,\n",
      "          -71.2524,  -71.6409,  -78.3862],\n",
      "        [-540.2885, -504.3756, -513.4988, -537.6649, -543.0318, -737.8273,\n",
      "         -673.8859, -679.3065, -746.7963],\n",
      "        [-250.6020, -234.3727, -238.6182, -250.5476, -252.7909, -343.0343,\n",
      "         -312.9242, -315.7893, -346.9583],\n",
      "        [-201.6062, -188.7586, -192.0951, -202.0866, -203.7465, -276.2935,\n",
      "         -251.8155, -254.3282, -279.5251]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-402.9354, -528.7575],\n",
      "        [ -56.5042,  -71.6409],\n",
      "        [-513.4988, -673.8859],\n",
      "        [-238.6182, -312.9242],\n",
      "        [-192.0951, -251.8155]], grad_fn=<GatherBackward>)\n",
      "A [[-411.79565  -385.14853  -391.89804  -410.93005  -414.91196  -563.0122\n",
      "  -513.9307   -518.5218   -570.1445  ]\n",
      " [ -52.93433   -49.755955  -50.179504  -52.54738   -53.17284   -72.34619\n",
      "   -66.24772   -66.60784   -72.85759 ]\n",
      " [-519.3153   -485.31274  -494.04282  -517.9675   -522.96484  -709.8159\n",
      "  -647.94604  -653.6313   -718.8809  ]\n",
      " [-245.24016  -229.61331  -233.7755   -245.79114  -247.88368  -336.0437\n",
      "  -306.34686  -309.39804  -340.0156  ]\n",
      " [-202.41689  -189.29955  -192.5701   -202.15456  -203.96638  -276.97717\n",
      "  -252.70027  -254.93626  -280.06567 ]]\n",
      "J tensor([-385.1485,  -49.7560, -485.3127, -229.6133, -189.2995])\n",
      "P tensor([-513.9307,  -66.2477, -647.9460, -306.3469, -252.7003])\n",
      "expected_state_action_values  tensor([[-395.0457, -510.9497],\n",
      "        [ -49.0704,  -63.9130],\n",
      "        [-494.2322, -640.6022],\n",
      "        [-240.1861, -309.2462],\n",
      "        [-193.7715, -250.8321]])\n",
      "reward -71.02799099885807\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-51.4207],\n",
      "        [-35.2038],\n",
      "        [-42.7328],\n",
      "        [-50.3626],\n",
      "        [-36.9724]])\n",
      "action_batch tensor([[1, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-476.0074, -444.4915, -432.9127, -453.1803, -478.3987, -649.8416,\n",
      "         -575.8820, -580.0357, -658.1772],\n",
      "        [-203.7757, -190.7081, -185.7600, -195.2524, -205.8351, -279.0340,\n",
      "         -246.8841, -249.0328, -282.4946],\n",
      "        [-366.0237, -341.8086, -333.0659, -349.1014, -368.3883, -500.0973,\n",
      "         -442.9945, -446.3177, -506.4897],\n",
      "        [-427.3182, -398.9989, -388.8004, -407.6020, -430.0667, -583.8091,\n",
      "         -517.0738, -521.0186, -591.4885],\n",
      "        [-204.2258, -191.0634, -185.9792, -195.1400, -205.8656, -279.3285,\n",
      "         -247.3518, -249.3164, -282.7264]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-444.4915, -580.0357],\n",
      "        [-185.7600, -246.8841],\n",
      "        [-341.8086, -442.9945],\n",
      "        [-398.9989, -517.0738],\n",
      "        [-195.1400, -249.3164]], grad_fn=<GatherBackward>)\n",
      "A [[-459.71765 -429.38013 -418.01492 -437.42035 -461.88623 -627.4127\n",
      "  -556.15405 -560.0903  -635.59033]\n",
      " [-235.34041 -220.46396 -214.90799 -226.32272 -238.36568 -322.75333\n",
      "  -285.22885 -288.06976 -326.85397]\n",
      " [-364.57083 -340.6191  -331.90985 -348.20557 -367.26245 -498.4159\n",
      "  -441.2574  -444.80515 -504.84167]\n",
      " [-448.47778 -418.91208 -408.19717 -428.07538 -451.6205  -612.8756\n",
      "  -542.72437 -547.0032  -621.0473 ]\n",
      " [-277.47375 -259.6007  -253.00583 -265.8795  -280.25803 -379.92798\n",
      "  -336.0931  -339.08862 -384.72296]]\n",
      "J tensor([-418.0149, -214.9080, -331.9099, -408.1972, -253.0058])\n",
      "P tensor([-556.1541, -285.2289, -441.2574, -542.7244, -336.0931])\n",
      "expected_state_action_values  tensor([[-427.6341, -551.9594],\n",
      "        [-228.6210, -291.9098],\n",
      "        [-341.4517, -439.8644],\n",
      "        [-417.7401, -538.8145],\n",
      "        [-264.6776, -339.4562]])\n",
      "1\n",
      "reward -73.10391247478063\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395, 17.3845,  9.2981,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-24.8327],\n",
      "        [-54.0051],\n",
      "        [-26.2099],\n",
      "        [-41.6451],\n",
      "        [ -6.7672]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-222.3174, -197.7533, -211.5083, -221.8954, -224.0905, -304.1384,\n",
      "         -277.6476, -262.7760, -307.3747],\n",
      "        [-382.1622, -339.5089, -363.2732, -380.5973, -384.5549, -522.1475,\n",
      "         -476.9492, -451.1659, -528.1176],\n",
      "        [-196.2595, -174.5043, -186.7014, -196.1590, -197.9627, -268.6591,\n",
      "         -245.0884, -232.0379, -271.5363],\n",
      "        [-392.5367, -348.6326, -373.3074, -391.7375, -395.5398, -536.7661,\n",
      "         -489.9227, -463.6902, -543.0983],\n",
      "        [ -50.5268,  -45.1953,  -47.8948,  -50.1902,  -50.8022,  -69.0845,\n",
      "          -63.2450,  -59.7044,  -69.5275]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-211.5083, -262.7760],\n",
      "        [-382.1622, -451.1659],\n",
      "        [-174.5043, -245.0884],\n",
      "        [-348.6326, -489.9227],\n",
      "        [ -50.1902,  -59.7044]], grad_fn=<GatherBackward>)\n",
      "A [[-208.43929  -185.41667  -198.28287  -208.11884  -210.18785  -285.16547\n",
      "  -260.32812  -246.38898  -288.3384  ]\n",
      " [-441.86713  -392.8562   -420.40976  -441.11838  -445.46643  -604.26544\n",
      "  -551.54254  -522.16797  -611.64526 ]\n",
      " [-196.95042  -175.41733  -187.6512   -197.48244  -199.19739  -269.9626\n",
      "  -246.07509  -233.22958  -273.02274 ]\n",
      " [-367.08087  -326.21555  -349.1615   -366.5377   -370.1097   -502.03754\n",
      "  -458.19788  -433.75278  -508.18768 ]\n",
      " [ -77.30765   -68.90271   -73.30945   -76.78161   -77.67927  -105.6129\n",
      "   -96.631294  -91.25356  -106.50278 ]]\n",
      "J tensor([-185.4167, -392.8562, -175.4173, -326.2155,  -68.9027])\n",
      "P tensor([-246.3890, -522.1680, -233.2296, -433.7528,  -91.2536])\n",
      "expected_state_action_values  tensor([[-191.7077, -246.5827],\n",
      "        [-407.5757, -523.9562],\n",
      "        [-184.0855, -236.1165],\n",
      "        [-335.2390, -432.0226],\n",
      "        [ -68.7797,  -88.8955]])\n",
      "5\n",
      "reward -71.31223406041981\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [19.9728,  9.6592, 19.7430,  6.0170,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.7497, 20.6804, 11.0180,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.5628],\n",
      "        [-31.9069],\n",
      "        [-57.3921],\n",
      "        [-33.9160],\n",
      "        [-54.7144]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-241.5578, -195.0508, -210.0595, -241.4762, -233.0523, -315.9696,\n",
      "         -278.9021, -281.5860, -320.2478],\n",
      "        [-263.2833, -212.3906, -229.0094, -263.1695, -253.9692, -344.4292,\n",
      "         -304.0281, -306.8894, -348.8033],\n",
      "        [-525.2028, -422.5491, -455.6682, -521.9736, -504.1773, -685.1951,\n",
      "         -605.7386, -610.4152, -694.0679],\n",
      "        [-285.4023, -230.2154, -248.1821, -285.0716, -275.1558, -373.2273,\n",
      "         -329.5223, -332.5690, -378.0304],\n",
      "        [-480.9612, -387.4614, -417.4818, -478.2791, -462.0270, -627.5993,\n",
      "         -554.8087, -559.2817, -635.9401]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-241.5578, -278.9021],\n",
      "        [-229.0094, -304.0281],\n",
      "        [-422.5491, -605.7386],\n",
      "        [-285.4023, -332.5690],\n",
      "        [-417.4818, -554.8087]], grad_fn=<GatherBackward>)\n",
      "A [[-263.2833  -212.39058 -229.0094  -263.1695  -253.96916 -344.42917\n",
      "  -304.02808 -306.8894  -348.80334]\n",
      " [-243.63087 -196.44252 -211.78827 -243.2191  -234.75835 -318.57373\n",
      "  -281.30765 -283.8194  -322.48096]\n",
      " [-538.36664 -432.99124 -466.8625  -534.3917  -516.285   -702.011\n",
      "  -620.83203 -625.37933 -710.94073]\n",
      " [-290.3096  -234.00026 -252.02399 -288.92206 -279.0319  -379.0438\n",
      "  -334.9848  -337.73322 -383.84387]\n",
      " [-467.91187 -376.90582 -406.10632 -465.04684 -449.3074  -610.45715\n",
      "  -539.7694  -544.00684 -618.4073 ]]\n",
      "J tensor([-212.3906, -196.4425, -432.9912, -234.0003, -376.9058])\n",
      "P tensor([-304.0281, -281.3076, -620.8320, -334.9848, -539.7694])\n",
      "expected_state_action_values  tensor([[-226.7144, -309.1881],\n",
      "        [-208.7052, -285.0838],\n",
      "        [-447.0842, -616.1409],\n",
      "        [-244.5163, -335.4024],\n",
      "        [-393.9297, -540.5068]])\n",
      "reward -77.79196018134218\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 16.5628, 10.7409,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.2340,  6.4395, 14.7698, 11.9507,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.8479, 12.3229, 14.8634,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-47.5241],\n",
      "        [-38.6657],\n",
      "        [-52.8627],\n",
      "        [-49.9555],\n",
      "        [-23.4019]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [2, 6],\n",
      "        [1, 7],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-366.8024, -326.6242, -319.3195, -383.5139, -370.4771, -502.1245,\n",
      "         -456.0615, -461.7981, -509.7690],\n",
      "        [-266.4792, -237.1860, -232.0672, -279.1466, -269.4971, -365.1694,\n",
      "         -331.4478, -335.7057, -370.5305],\n",
      "        [-401.3448, -357.2598, -349.4066, -420.0028, -405.5883, -549.5793,\n",
      "         -498.9589, -505.3682, -558.1623],\n",
      "        [-369.7719, -329.3797, -322.2274, -387.8193, -374.3459, -506.8545,\n",
      "         -459.8714, -466.0864, -514.8293],\n",
      "        [-179.2728, -159.9676, -156.3780, -188.6349, -181.9405, -246.2027,\n",
      "         -223.1495, -226.3470, -249.8357]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-319.3195, -461.7981],\n",
      "        [-232.0672, -331.4478],\n",
      "        [-357.2598, -505.3682],\n",
      "        [-329.3797, -459.8714],\n",
      "        [-156.3780, -223.1495]], grad_fn=<GatherBackward>)\n",
      "A [[-377.17093 -335.68018 -328.17212 -393.91278 -380.58936 -516.06366\n",
      "  -468.8642  -474.58713 -523.8453 ]\n",
      " [-299.87286 -266.9258  -261.18985 -314.2046  -303.33957 -410.9388\n",
      "  -372.968   -377.80945 -417.09952]\n",
      " [-423.6916  -376.98212 -368.71848 -442.99243 -427.84933 -579.94586\n",
      "  -526.66077 -533.2687  -588.93225]\n",
      " [-389.0985  -346.44778 -338.93607 -407.79282 -393.6757  -533.1604\n",
      "  -483.83432 -490.25363 -541.5659 ]\n",
      " [-180.5886  -160.93954 -157.25322 -189.25876 -182.67296 -247.58012\n",
      "  -224.65569 -227.5918  -251.08911]]\n",
      "J tensor([-328.1721, -261.1898, -368.7185, -338.9361, -157.2532])\n",
      "P tensor([-468.8642, -372.9680, -526.6608, -483.8343, -224.6557])\n",
      "expected_state_action_values  tensor([[-342.8789, -469.5018],\n",
      "        [-273.7366, -374.3369],\n",
      "        [-384.7094, -526.8574],\n",
      "        [-354.9980, -485.4064],\n",
      "        [-164.9298, -225.5920]])\n",
      "reward -77.07478652771985\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  6.7497, 20.6804, 11.0180,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  7.3475,  9.7098,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-54.7144],\n",
      "        [-36.9097],\n",
      "        [-35.7833],\n",
      "        [-73.1039],\n",
      "        [-25.4330]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 6],\n",
      "        [2, 6],\n",
      "        [1, 7],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-464.2445, -433.1339, -422.5017, -484.6509, -468.3011, -635.1798,\n",
      "         -594.1323, -601.7827, -644.6307],\n",
      "        [-316.8186, -295.5837, -288.5888, -331.6238, -320.2362, -434.0764,\n",
      "         -405.7115, -411.1231, -440.3055],\n",
      "        [-245.0375, -229.4651, -223.5857, -257.3273, -248.4414, -336.1764,\n",
      "         -313.9937, -318.6219, -341.3769],\n",
      "        [-561.4744, -524.1691, -511.7251, -588.1248, -567.9457, -769.2861,\n",
      "         -718.9106, -728.8623, -781.1857],\n",
      "        [-190.5194, -178.2014, -173.7548, -200.0448, -193.0515, -261.4633,\n",
      "         -244.1472, -247.6608, -265.1502]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-422.5017, -594.1323],\n",
      "        [-316.8186, -405.7115],\n",
      "        [-223.5857, -313.9937],\n",
      "        [-524.1691, -728.8623],\n",
      "        [-173.7548, -247.6608]], grad_fn=<GatherBackward>)\n",
      "A [[-451.23755 -420.94534 -410.61206 -470.8115  -454.99118 -617.2638\n",
      "  -577.4882  -584.80994 -626.2844 ]\n",
      " [-296.19937 -276.43634 -269.86612 -310.17554 -299.50598 -405.91785\n",
      "  -379.3533  -384.46408 -411.72275]\n",
      " [-263.97626 -246.92789 -240.74681 -277.03406 -267.46307 -362.04776\n",
      "  -338.1899  -343.0662  -367.61014]\n",
      " [-556.71924 -519.8635  -507.63358 -583.8094  -563.65027 -763.1504\n",
      "  -712.94604 -723.0475  -775.0435 ]\n",
      " [-190.40508 -178.05513 -173.68819 -200.09537 -193.05405 -261.40533\n",
      "  -244.02232 -247.57652 -265.1045 ]]\n",
      "J tensor([-410.6121, -269.8661, -240.7468, -507.6336, -173.6882])\n",
      "P tensor([-577.4882, -379.3533, -338.1899, -712.9460, -244.0223])\n",
      "expected_state_action_values  tensor([[-424.2653, -574.4538],\n",
      "        [-279.7892, -378.3277],\n",
      "        [-252.4554, -340.1542],\n",
      "        [-529.9741, -714.7554],\n",
      "        [-181.7524, -245.0531]])\n",
      "5\n",
      "reward -81.82066691078164\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  6.4395, 19.5042,  8.9866,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[17.1196,  6.4395, 16.5628, 10.7409,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [19.9728,  9.6592, 19.7430,  6.0170,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.7128, 18.8469,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-48.1451],\n",
      "        [-56.9031],\n",
      "        [-71.0280],\n",
      "        [-45.6004],\n",
      "        [-19.3299]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[-406.2937, -415.9683, -405.0983, -446.1887, -430.7949, -583.8787,\n",
      "         -530.0038, -536.9901, -592.9965],\n",
      "        [-473.5557, -484.2267, -471.6111, -518.6779, -500.9687, -679.7505,\n",
      "         -617.4408, -625.0744, -690.0256],\n",
      "        [-499.9093, -512.1810, -498.9586, -550.1012, -530.9910, -719.0085,\n",
      "         -652.3404, -661.3664, -730.5928],\n",
      "        [-404.8376, -414.5949, -403.6600, -444.6893, -429.3828, -581.7935,\n",
      "         -528.1161, -535.1264, -591.1271],\n",
      "        [-124.4735, -127.6216, -123.9904, -136.6368, -131.9224, -178.9688,\n",
      "         -162.4852, -164.5343, -181.4328]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-415.9683, -530.0038],\n",
      "        [-471.6111, -625.0744],\n",
      "        [-498.9586, -652.3404],\n",
      "        [-414.5949, -528.1161],\n",
      "        [-136.6368, -162.4852]], grad_fn=<GatherBackward>)\n",
      "A [[-397.66382 -406.85864 -396.19168 -436.04526 -421.09988 -571.0965\n",
      "  -518.6087  -525.18494 -579.8748 ]\n",
      " [-461.63727 -472.36133 -460.00665 -506.17776 -488.88123 -662.93616\n",
      "  -602.059   -609.70685 -673.18713]\n",
      " [-509.06104 -521.83734 -508.348   -560.7133  -541.1668  -732.4772\n",
      "  -664.3972  -673.8166  -744.42584]\n",
      " [-399.0828  -408.23505 -397.3586  -437.01282 -422.18896 -572.742\n",
      "  -520.3435  -526.7446  -581.64716]\n",
      " [-144.20923 -148.15906 -144.01599 -159.09273 -153.49675 -207.76689\n",
      "  -188.38335 -191.08665 -210.9036 ]]\n",
      "J tensor([-396.1917, -460.0067, -508.3480, -397.3586, -144.0160])\n",
      "P tensor([-518.6087, -602.0590, -664.3972, -520.3435, -188.3833])\n",
      "expected_state_action_values  tensor([[-404.7176, -514.8929],\n",
      "        [-470.9090, -598.7562],\n",
      "        [-528.5412, -668.9855],\n",
      "        [-403.2232, -513.9096],\n",
      "        [-148.9443, -188.8749]])\n",
      "5\n",
      "reward -80.06947140934474\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  7.3475,  9.7098,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 18.1602, 26.5328,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-11.8820],\n",
      "        [-36.4907],\n",
      "        [-46.9426],\n",
      "        [-36.9097],\n",
      "        [-71.3122]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [0, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -90.7424,  -88.9958,  -94.0460, -103.4754,  -96.0364, -130.3891,\n",
      "         -115.0516, -116.2825, -131.9786],\n",
      "        [-192.1726, -188.5676, -199.9030, -220.6387, -204.5035, -276.7899,\n",
      "         -243.6322, -246.9329, -280.9572],\n",
      "        [-353.8507, -346.8868, -367.8597, -405.8922, -376.2540, -509.1942,\n",
      "         -448.2286, -454.3215, -517.6535],\n",
      "        [-272.7485, -267.2973, -283.5215, -312.7777, -289.9499, -392.5214,\n",
      "         -345.5828, -350.1647, -398.7222],\n",
      "        [-478.1019, -469.0672, -497.5854, -549.4236, -509.2325, -688.4894,\n",
      "         -605.7869, -614.4366, -700.3184]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -88.9958, -115.0516],\n",
      "        [-199.9030, -243.6322],\n",
      "        [-353.8507, -454.3215],\n",
      "        [-272.7485, -345.5828],\n",
      "        [-469.0672, -605.7869]], grad_fn=<GatherBackward>)\n",
      "A [[-108.71813 -106.59405 -112.69905 -124.19386 -115.18713 -156.27943\n",
      "  -137.7553  -139.36038 -158.45541]\n",
      " [-222.83775 -218.54953 -231.94702 -256.21738 -237.36342 -321.17743\n",
      "  -282.5216  -286.472   -325.99304]\n",
      " [-335.9114  -329.49982 -349.25314 -385.41818 -357.32132 -483.39902\n",
      "  -425.53775 -431.3819  -491.6382 ]\n",
      " [-254.74948 -249.75171 -264.88068 -292.2798  -270.93234 -366.71622\n",
      "  -322.82462 -327.15497 -372.48892]\n",
      " [-477.63867 -468.802   -497.21643 -549.22833 -509.0569  -687.96234\n",
      "  -605.2511  -614.0282  -700.08215]]\n",
      "J tensor([-106.5940, -218.5495, -329.4998, -249.7517, -468.8020])\n",
      "P tensor([-137.7553, -282.5216, -425.5378, -322.8246, -605.2511])\n",
      "expected_state_action_values  tensor([[-107.8166, -135.8617],\n",
      "        [-233.1853, -290.7601],\n",
      "        [-343.4925, -429.9266],\n",
      "        [-261.6863, -327.4519],\n",
      "        [-493.2340, -616.0382]])\n",
      "1\n",
      "reward -73.4551477185554\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-27.9424],\n",
      "        [-19.2270],\n",
      "        [-46.5218],\n",
      "        [-10.7610],\n",
      "        [-25.3215]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-194.5202, -211.6737, -222.4954, -235.7592, -219.3081, -295.3982,\n",
      "         -268.2658, -256.2167, -300.3090],\n",
      "        [-106.0037, -115.0763, -120.6387, -127.4381, -118.6743, -160.4222,\n",
      "         -146.0066, -139.0641, -162.8124],\n",
      "        [-371.3545, -402.4900, -423.0169, -446.4521, -415.7901, -561.5333,\n",
      "         -511.0027, -486.9697, -571.0875],\n",
      "        [ -88.3311,  -95.9131, -100.5031, -106.0143,  -98.7843, -133.6295,\n",
      "         -121.7338, -115.8455, -135.3940],\n",
      "        [-198.1217, -215.6546, -226.5186, -240.0098, -223.2648, -300.7676,\n",
      "         -273.1396, -260.8974, -305.9185]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-222.4954, -268.2658],\n",
      "        [-115.0763, -146.0066],\n",
      "        [-402.4900, -511.0027],\n",
      "        [ -95.9131, -121.7338],\n",
      "        [-226.5186, -273.1396]], grad_fn=<GatherBackward>)\n",
      "A [[-174.48189  -189.84302  -199.43198  -211.23715  -196.56865  -264.8099\n",
      "  -240.59937  -229.7001   -269.27417 ]\n",
      " [-149.74864  -162.41591  -170.50667  -180.01353  -167.60483  -226.58046\n",
      "  -206.18227  -196.41502  -229.97891 ]\n",
      " [-360.69012  -391.16455  -411.0524   -434.0977   -404.26553  -545.6378\n",
      "  -496.42938  -473.24097  -555.1965  ]\n",
      " [ -88.807755  -96.42911  -101.04371  -106.58046   -99.312935 -134.34589\n",
      "  -122.388115 -116.46739  -136.12268 ]\n",
      " [-177.91196  -193.22285  -202.83168  -214.287    -199.49736  -269.38687\n",
      "  -245.02644  -233.61562  -273.68307 ]]\n",
      "J tensor([-174.4819, -149.7486, -360.6901,  -88.8078, -177.9120])\n",
      "P tensor([-229.7001, -196.4150, -473.2410, -116.4674, -233.6156])\n",
      "expected_state_action_values  tensor([[-184.9761, -234.6725],\n",
      "        [-154.0008, -196.0005],\n",
      "        [-371.1429, -472.4387],\n",
      "        [ -90.6880, -115.5816],\n",
      "        [-185.4422, -235.5755]])\n",
      "1\n",
      "reward -74.52638850715086\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-38.6657],\n",
      "        [-45.2304],\n",
      "        [-56.4395],\n",
      "        [-53.1408],\n",
      "        [-31.9069]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [4, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-226.0247, -234.9256, -248.1810, -273.0020, -254.0800, -342.5498,\n",
      "         -302.3176, -297.0426, -348.4253],\n",
      "        [-303.4545, -315.4116, -332.8413, -365.5841, -340.4359, -459.1992,\n",
      "         -405.5668, -398.3397, -467.4624],\n",
      "        [-372.4120, -386.4875, -407.8906, -447.1431, -416.5564, -562.6653,\n",
      "         -497.3919, -488.0289, -572.3988],\n",
      "        [-365.6931, -379.8593, -400.6497, -439.3575, -409.3704, -552.5911,\n",
      "         -488.4810, -479.4247, -562.5859],\n",
      "        [-202.9471, -211.3721, -223.2480, -245.9499, -228.8038, -308.0838,\n",
      "         -271.6610, -267.2217, -313.4395]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-248.1810, -302.3176],\n",
      "        [-315.4116, -405.5668],\n",
      "        [-407.8906, -488.0289],\n",
      "        [-409.3704, -479.4247],\n",
      "        [-223.2480, -271.6610]], grad_fn=<GatherBackward>)\n",
      "A [[-254.30792 -264.34592 -279.28622 -307.2492  -285.95074 -385.42886\n",
      "  -340.13348 -334.2502  -392.16913]\n",
      " [-308.49304 -320.6099  -338.45068 -371.87665 -346.23306 -466.97156\n",
      "  -412.33646 -405.0437  -475.33035]\n",
      " [-373.55563 -388.22775 -409.7556  -449.93976 -419.0058  -565.18677\n",
      "  -499.20422 -490.30942 -575.4042 ]\n",
      " [-372.41196 -386.48752 -407.89056 -447.1431  -416.55637 -562.66534\n",
      "  -497.39188 -488.0289  -572.3988 ]\n",
      " [-188.18463 -195.86774 -206.84679 -227.71556 -211.87978 -285.49326\n",
      "  -251.84773 -247.59673 -290.31976]]\n",
      "J tensor([-254.3079, -308.4930, -373.5556, -372.4120, -188.1846])\n",
      "P tensor([-334.2502, -405.0437, -490.3094, -488.0289, -247.5967])\n",
      "expected_state_action_values  tensor([[-267.5428, -339.4909],\n",
      "        [-322.8741, -409.7697],\n",
      "        [-392.6396, -497.7180],\n",
      "        [-388.3116, -492.3668],\n",
      "        [-201.2731, -254.7440]])\n",
      "show\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "reward -3.5828422639440864\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 9.9864,  6.4395, 17.3845,  9.2981,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-54.0051],\n",
      "        [-52.6400],\n",
      "        [-30.9730],\n",
      "        [-28.1379],\n",
      "        [-35.2038]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [2, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-315.0809, -341.7774, -331.9050, -378.7891, -326.0442, -476.4806,\n",
      "         -433.6195, -426.3847, -484.4508],\n",
      "        [-364.8789, -396.0107, -384.4256, -438.9471, -377.8342, -551.8374,\n",
      "         -502.1043, -493.9088, -561.5980],\n",
      "        [-223.3072, -242.9844, -235.8875, -270.2515, -232.4316, -338.7545,\n",
      "         -307.7106, -303.1983, -344.8063],\n",
      "        [-219.9683, -238.8606, -232.3332, -266.1156, -228.7642, -333.7701,\n",
      "         -303.1429, -298.5341, -339.1061],\n",
      "        [-171.3207, -186.2605, -180.8859, -207.1521, -178.1813, -259.8592,\n",
      "         -236.1342, -232.5148, -264.1888]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-315.0809, -426.3847],\n",
      "        [-384.4256, -493.9088],\n",
      "        [-235.8875, -307.7106],\n",
      "        [-232.3332, -303.1429],\n",
      "        [-180.8859, -236.1342]], grad_fn=<GatherBackward>)\n",
      "A [[-363.3137  -394.49796 -383.14734 -437.93762 -376.8138  -550.0349\n",
      "  -500.14157 -492.25845 -559.7068 ]\n",
      " [-390.41403 -423.4792  -411.06656 -469.01355 -403.78616 -590.06\n",
      "  -537.0888  -528.1039  -600.4111 ]\n",
      " [-197.4786  -214.83707 -208.57152 -239.0055  -205.54105 -299.60587\n",
      "  -272.1364  -268.12054 -304.87726]\n",
      " [-186.30508 -202.41812 -196.73604 -225.45184 -193.87657 -282.67783\n",
      "  -256.78632 -252.88135 -287.4445 ]\n",
      " [-197.02971 -214.47824 -208.46179 -239.16    -205.57358 -299.4096\n",
      "  -271.74155 -267.91806 -304.47916]]\n",
      "J tensor([-363.3137, -390.4140, -197.4786, -186.3051, -197.0297])\n",
      "P tensor([-492.2585, -528.1039, -268.1205, -252.8813, -267.9181])\n",
      "expected_state_action_values  tensor([[-380.9874, -497.0377],\n",
      "        [-404.0126, -527.9335],\n",
      "        [-208.7038, -272.2815],\n",
      "        [-195.8125, -255.7311],\n",
      "        [-212.5305, -276.3300]])\n",
      "reward -3.7268138561220767\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -6.7672],\n",
      "        [-47.6074],\n",
      "        [-30.9730],\n",
      "        [-42.7328],\n",
      "        [-57.2807]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[ -47.4954,  -49.3073,  -49.3632,  -54.1205,  -46.6633,  -68.3513,\n",
      "          -60.5562,  -63.0322,  -69.1026],\n",
      "        [-338.0790, -348.6892, -352.5027, -386.9530, -332.9162, -486.4987,\n",
      "         -429.6496, -448.5220, -494.5145],\n",
      "        [-246.9138, -255.3499, -257.9377, -283.7381, -244.0422, -355.9736,\n",
      "         -314.0613, -328.3040, -362.0642],\n",
      "        [-340.4557, -350.9561, -354.7864, -389.1500, -334.9296, -489.5994,\n",
      "         -432.6334, -451.3950, -497.6275],\n",
      "        [-410.0009, -422.2845, -426.7885, -467.6171, -402.5506, -588.9576,\n",
      "         -520.7108, -542.9776, -598.6650]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -54.1205,  -63.0322],\n",
      "        [-352.5027, -429.6496],\n",
      "        [-257.9377, -314.0613],\n",
      "        [-350.9561, -432.6334],\n",
      "        [-426.7885, -542.9776]], grad_fn=<GatherBackward>)\n",
      "A [[ -72.00504   -74.4937    -74.85704   -82.02565   -70.68915  -103.519516\n",
      "   -91.661476  -95.449135 -104.868225]\n",
      " [-363.73224  -375.29517  -379.34805  -416.42056  -358.2774   -523.4569\n",
      "  -462.267    -482.65424  -532.1627  ]\n",
      " [-218.07764  -225.48724  -227.78127  -250.61574  -215.53607  -314.43588\n",
      "  -277.40228  -289.95587  -319.73364 ]\n",
      " [-338.07904  -348.68918  -352.50266  -386.953    -332.91617  -486.49866\n",
      "  -429.64957  -448.52197  -494.51447 ]\n",
      " [-474.31744  -488.56262  -494.11746  -541.77545  -466.20667  -681.8602\n",
      "  -602.505    -628.56     -692.9641  ]]\n",
      "J tensor([ -70.6891, -358.2774, -215.5361, -332.9162, -466.2067])\n",
      "P tensor([ -91.6615, -462.2670, -277.4023, -429.6496, -602.5050])\n",
      "expected_state_action_values  tensor([[ -70.3875,  -89.2626],\n",
      "        [-370.0571, -463.6477],\n",
      "        [-224.9555, -280.6351],\n",
      "        [-342.3574, -429.4174],\n",
      "        [-476.8667, -599.5352]])\n",
      "reward -4.687999665149713\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-31.9069],\n",
      "        [-47.4361],\n",
      "        [-28.3724],\n",
      "        [-28.1379],\n",
      "        [-24.1008]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [0, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-243.7654, -241.1050, -264.5464, -291.2534, -240.6978, -351.6349,\n",
      "         -300.8428, -333.6105, -356.8460],\n",
      "        [-427.7480, -422.2304, -462.8835, -507.8857, -420.0828, -615.0009,\n",
      "         -527.1581, -583.5539, -624.3996],\n",
      "        [-201.0744, -198.8967, -217.8609, -239.5960, -198.1006, -289.6631,\n",
      "         -248.0412, -274.8442, -294.0479],\n",
      "        [-249.8569, -246.8927, -271.1006, -298.4377, -246.5735, -360.4163,\n",
      "         -308.3319, -341.8427, -365.5236],\n",
      "        [-183.1098, -181.3305, -198.7419, -218.9580, -180.9117, -264.2820,\n",
      "         -226.0222, -250.7354, -268.1665]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-264.5464, -300.8428],\n",
      "        [-462.8835, -527.1581],\n",
      "        [-201.0744, -248.0412],\n",
      "        [-271.1006, -308.3319],\n",
      "        [-198.7419, -226.0222]], grad_fn=<GatherBackward>)\n",
      "A [[-225.889   -223.30473 -244.98456 -269.5476  -222.78421 -325.67532\n",
      "  -278.74493 -308.95062 -330.36694]\n",
      " [-403.39114 -398.35052 -436.5127  -478.9435  -396.20663 -579.93134\n",
      "  -497.15265 -550.3506  -588.9762 ]\n",
      " [-208.34363 -206.00108 -225.74112 -247.99214 -205.05737 -300.04614\n",
      "  -257.0341  -284.6885  -304.25754]\n",
      " [-211.71483 -209.31927 -229.67535 -252.94951 -209.06268 -305.38937\n",
      "  -261.30444 -289.6993  -309.96762]\n",
      " [-174.08643 -172.5513  -189.00972 -208.36885 -172.19418 -251.33356\n",
      "  -214.92323 -238.50436 -255.24052]]\n",
      "J tensor([-222.7842, -396.2066, -205.0574, -209.0627, -172.1942])\n",
      "P tensor([-278.7449, -497.1526, -257.0341, -261.3044, -214.9232])\n",
      "expected_state_action_values  tensor([[-232.4127, -282.7773],\n",
      "        [-404.0221, -494.8735],\n",
      "        [-212.9240, -259.7031],\n",
      "        [-216.2943, -263.3120],\n",
      "        [-179.0756, -217.5317]])\n",
      "reward -4.290023652316296\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-38.6147],\n",
      "        [ -4.6880],\n",
      "        [-57.2807],\n",
      "        [ -4.2900],\n",
      "        [-41.6451]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [4, 6],\n",
      "        [2, 7],\n",
      "        [4, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-270.8189, -255.0793, -269.1872, -307.3890, -254.1487, -371.6754,\n",
      "         -308.6283, -352.6413, -377.4273],\n",
      "        [ -51.0123,  -48.4189,  -50.5522,  -57.8710,  -47.8986,  -70.1060,\n",
      "          -58.2934,  -66.5227,  -71.0072],\n",
      "        [-426.8578, -401.3484, -423.7162, -483.1208, -399.6407, -584.8831,\n",
      "         -486.1534, -554.9197, -594.2759],\n",
      "        [ -59.8118,  -56.6899,  -59.2610,  -67.7342,  -56.0720,  -82.1054,\n",
      "          -68.3021,  -77.9180,  -83.1993],\n",
      "        [-366.5327, -344.9976, -364.4029, -416.2346, -344.1155, -503.0634,\n",
      "         -417.6460, -477.2751, -511.0823]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-270.8189, -352.6413],\n",
      "        [ -47.8986,  -58.2934],\n",
      "        [-423.7162, -554.9197],\n",
      "        [ -56.0720,  -68.3021],\n",
      "        [-344.9976, -417.6460]], grad_fn=<GatherBackward>)\n",
      "A [[-293.98352  -276.82623  -292.3122   -333.87125  -275.97485  -403.59824\n",
      "  -335.03192  -382.88483  -409.72443 ]\n",
      " [ -59.81183   -56.68988   -59.260975  -67.73424   -56.071995  -82.10536\n",
      "   -68.30212   -77.91796   -83.19932 ]\n",
      " [-492.33545  -462.9052   -489.07507  -558.0249   -461.42725  -675.0898\n",
      "  -560.78925  -640.43353  -685.77936 ]\n",
      " [ -56.168392  -53.2653    -55.655132  -63.650414  -52.68783   -77.13708\n",
      "   -64.15805   -73.1998    -78.15121 ]\n",
      " [-345.25784  -325.1784   -343.3239   -392.31628  -324.37698  -473.95752\n",
      "  -393.44293  -449.72858  -481.76126 ]]\n",
      "J tensor([-275.9749,  -56.0720, -461.4272,  -52.6878, -324.3770])\n",
      "P tensor([-335.0319,  -68.3021, -560.7892,  -64.1581, -393.4429])\n",
      "expected_state_action_values  tensor([[-286.9920, -340.1434],\n",
      "        [ -55.1528,  -66.1599],\n",
      "        [-472.5652, -561.9910],\n",
      "        [ -51.7091,  -62.0323],\n",
      "        [-333.5843, -395.7437]])\n",
      "5\n",
      "reward -6.76724704198075\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.7128, 18.8469,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.2038],\n",
      "        [-50.7651],\n",
      "        [-73.1039],\n",
      "        [-27.4673],\n",
      "        [-71.0280]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [1, 7],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-203.3384, -174.5721, -200.6712, -221.5511, -176.4176, -267.3193,\n",
      "         -214.3726, -260.4693, -270.9290],\n",
      "        [-428.7086, -367.4572, -422.1683, -464.5178, -370.1319, -561.9349,\n",
      "         -451.4226, -547.6761, -570.0190],\n",
      "        [-572.9810, -491.2711, -565.3580, -623.5717, -496.5408, -752.4913,\n",
      "         -603.5040, -733.3392, -763.6384],\n",
      "        [-221.5913, -190.0944, -218.7129, -241.6124, -192.3517, -291.3935,\n",
      "         -233.5958, -283.8747, -295.3847],\n",
      "        [-564.4391, -483.7220, -556.6801, -613.7328, -488.7399, -740.9940,\n",
      "         -594.4608, -722.0791, -751.8384]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-200.6712, -214.3726],\n",
      "        [-422.1683, -451.4226],\n",
      "        [-491.2711, -733.3392],\n",
      "        [-190.0944, -233.5958],\n",
      "        [-556.6801, -594.4608]], grad_fn=<GatherBackward>)\n",
      "A [[-230.17361 -197.77472 -227.56903 -251.69006 -200.28905 -303.09073\n",
      "  -242.70459 -295.33328 -307.23532]\n",
      " [-442.18835 -379.15234 -435.64755 -479.59186 -382.10684 -579.84344\n",
      "  -465.65015 -565.16113 -588.29553]\n",
      " [-565.00635 -484.54883 -557.7633  -615.59326 -490.11212 -742.3973\n",
      "  -595.1638  -723.49805 -753.4705 ]\n",
      " [-254.60402 -218.2563  -250.90454 -276.71747 -220.36688 -334.27515\n",
      "  -268.23016 -325.6561  -338.95074]\n",
      " [-572.98096 -491.27115 -565.358   -623.57166 -496.5408  -752.4913\n",
      "  -603.50397 -733.33923 -763.63837]]\n",
      "J tensor([-197.7747, -379.1523, -484.5488, -218.2563, -491.2711])\n",
      "P tensor([-242.7046, -465.6501, -595.1638, -268.2302, -603.5040])\n",
      "expected_state_action_values  tensor([[-213.2010, -253.6379],\n",
      "        [-392.0021, -469.8502],\n",
      "        [-509.1978, -608.7513],\n",
      "        [-223.8979, -268.8744],\n",
      "        [-513.1720, -614.1816]])\n",
      "reward -7.868501523473141\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[21.3995,  6.4395, 18.7128, 18.8469,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-67.3986],\n",
      "        [ -7.8685],\n",
      "        [-26.2099],\n",
      "        [-73.1039],\n",
      "        [-18.9634]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [4, 6],\n",
      "        [1, 6],\n",
      "        [1, 7],\n",
      "        [3, 6]])\n",
      "policy_net1(state_batch) tensor([[-555.7638, -498.4435, -526.9510, -601.2156, -479.1841, -728.1978,\n",
      "         -604.0328, -690.1121, -738.3585],\n",
      "        [ -90.3161,  -81.3218,  -85.3770,  -97.3960,  -77.7073, -118.3032,\n",
      "          -98.3206, -112.1028, -119.6810],\n",
      "        [-201.5719, -180.9547, -191.1217, -218.3093, -173.9055, -264.4294,\n",
      "         -219.2337, -250.4857, -267.6255],\n",
      "        [-600.1351, -538.4718, -569.4800, -650.2816, -518.1474, -786.9294,\n",
      "         -652.3416, -745.7775, -797.9524],\n",
      "        [-167.0280, -150.2026, -158.4776, -181.2549, -144.4065, -219.2978,\n",
      "         -181.7398, -207.7883, -222.1092]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-498.4435, -604.0328],\n",
      "        [ -77.7073,  -98.3206],\n",
      "        [-180.9547, -219.2337],\n",
      "        [-538.4718, -745.7775],\n",
      "        [-181.2549, -181.7398]], grad_fn=<GatherBackward>)\n",
      "A [[-590.58185  -529.66437  -560.17505  -639.3882   -509.5014   -774.12396\n",
      "  -641.9036   -733.58746  -784.83307 ]\n",
      " [ -92.78904   -83.488754  -87.73732  -100.09126   -79.82185  -121.58016\n",
      "  -101.008    -115.17657  -122.89056 ]\n",
      " [-199.3756   -179.27658  -189.3354   -216.6158   -172.51033  -261.90955\n",
      "  -216.91995  -248.15857  -265.21823 ]\n",
      " [-592.90857  -532.1159   -562.8836   -643.1489   -512.3883   -777.8307\n",
      "  -644.5547   -737.15027  -788.8059  ]\n",
      " [-164.48492  -147.80823  -155.99043  -178.30452  -142.0433   -215.88507\n",
      "  -178.95609  -204.5141   -218.50296 ]]\n",
      "J tensor([-509.5014,  -79.8219, -172.5103, -512.3883, -142.0433])\n",
      "P tensor([-641.9036, -101.0080, -216.9200, -644.5547, -178.9561])\n",
      "expected_state_action_values  tensor([[-525.9499, -645.1119],\n",
      "        [ -79.7082,  -98.7757],\n",
      "        [-181.4692, -221.4379],\n",
      "        [-534.2534, -653.2031],\n",
      "        [-146.8023, -180.0238]])\n",
      "reward -10.700904491424406\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.7497, 20.6804, 11.0180,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-51.4207],\n",
      "        [-54.7144],\n",
      "        [-48.4121],\n",
      "        [-73.4551],\n",
      "        [-25.4330]])\n",
      "action_batch tensor([[1, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-500.7715, -467.4886, -473.7746, -515.7702, -465.7364, -653.5602,\n",
      "         -562.1674, -602.3096, -661.7498],\n",
      "        [-490.7084, -458.3342, -464.5807, -506.2130, -456.9799, -640.8702,\n",
      "         -550.9684, -590.6349, -649.0104],\n",
      "        [-421.3630, -393.4809, -398.8729, -434.4930, -392.2794, -550.2618,\n",
      "         -473.1719, -507.0978, -557.0108],\n",
      "        [-643.1022, -601.1362, -609.9544, -666.1338, -600.9607, -841.3100,\n",
      "         -722.3543, -775.3997, -852.6069],\n",
      "        [-195.0952, -182.6817, -185.0784, -202.4355, -182.5261, -255.5909,\n",
      "         -219.3155, -235.4989, -258.6067]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-467.4886, -602.3096],\n",
      "        [-464.5807, -550.9684],\n",
      "        [-398.8729, -473.1719],\n",
      "        [-643.1022, -775.3997],\n",
      "        [-185.0784, -235.4989]], grad_fn=<GatherBackward>)\n",
      "A [[-483.9068  -451.84796 -457.73236 -498.15118 -449.92575 -631.3725\n",
      "  -543.2233  -581.9361  -639.42004]\n",
      " [-479.1319  -447.46603 -453.55945 -494.0008  -446.01154 -625.62787\n",
      "  -537.98334 -576.5905  -633.42017]\n",
      " [-406.91315 -380.45288 -385.605   -420.62747 -379.6702  -531.9083\n",
      "  -457.0769  -490.28937 -538.82733]\n",
      " [-605.52466 -566.0168  -574.23663 -627.22577 -565.83185 -792.1452\n",
      "  -680.102   -730.0652  -802.8925 ]\n",
      " [-194.70445 -182.27972 -184.75005 -202.2012  -182.27629 -255.17548\n",
      "  -218.89061 -235.08841 -258.19794]]\n",
      "J tensor([-449.9258, -446.0115, -379.6702, -565.8318, -182.2763])\n",
      "P tensor([-543.2233, -537.9833, -457.0769, -680.1020, -218.8906])\n",
      "expected_state_action_values  tensor([[-456.3539, -540.3217],\n",
      "        [-456.1248, -538.8994],\n",
      "        [-390.1152, -459.7812],\n",
      "        [-582.7038, -685.5469],\n",
      "        [-189.4817, -222.4346]])\n",
      "5\n",
      "reward -10.760997101723085\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-50.7651],\n",
      "        [-81.8207],\n",
      "        [-14.0164],\n",
      "        [ -3.5828],\n",
      "        [-32.9790]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 6],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-402.6178, -377.1959, -383.3035, -434.2784, -392.1600, -549.8113,\n",
      "         -458.7064, -492.0092, -557.2270],\n",
      "        [-536.5515, -503.5255, -512.6920, -583.5804, -526.2433, -735.3635,\n",
      "         -611.8287, -658.0662, -746.1395],\n",
      "        [-168.4692, -157.7628, -160.2835, -181.7689, -164.0411, -230.2675,\n",
      "         -192.0582, -205.8910, -232.8119],\n",
      "        [ -27.9605,  -26.5752,  -26.5738,  -30.3206,  -27.4238,  -38.4383,\n",
      "          -32.1349,  -34.3751,  -38.5819],\n",
      "        [-238.3232, -223.3308, -227.0667, -257.9098, -232.7184, -326.0183,\n",
      "         -271.6914, -291.6153, -330.2816]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-383.3035, -458.7064],\n",
      "        [-536.5515, -611.8287],\n",
      "        [-157.7628, -192.0582],\n",
      "        [ -27.9605,  -34.3751],\n",
      "        [-238.3232, -291.6153]], grad_fn=<GatherBackward>)\n",
      "A [[-414.7849   -388.75293  -395.08398  -447.86917  -404.36954  -566.6788\n",
      "  -472.61905  -507.1305   -574.4301  ]\n",
      " [-580.03217  -543.9516   -553.89246  -629.9174   -568.1114   -794.45636\n",
      "  -661.2746   -710.87714  -805.762   ]\n",
      " [-130.57445  -122.45721  -124.25022  -141.05508  -127.309105 -178.57861\n",
      "  -148.92688  -159.6977   -180.61876 ]\n",
      " [ -49.339153  -46.630833  -46.86761   -53.327766  -48.183853  -67.57034\n",
      "   -56.416943  -60.453625  -68.24743 ]\n",
      " [-253.01506  -237.18593  -241.3127   -274.34616  -247.422    -346.45126\n",
      "  -288.49203  -309.87198  -350.903   ]]\n",
      "J tensor([-388.7529, -543.9516, -122.4572,  -46.6308, -237.1859])\n",
      "P tensor([-472.6190, -661.2746, -148.9269,  -56.4169, -288.4920])\n",
      "expected_state_action_values  tensor([[-400.6427, -476.1222],\n",
      "        [-571.3771, -676.9678],\n",
      "        [-124.2278, -148.0506],\n",
      "        [ -45.5506,  -54.3581],\n",
      "        [-246.4463, -292.6218]])\n",
      "5\n",
      "reward -11.881967070464844\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-51.1810],\n",
      "        [-24.7273],\n",
      "        [ -3.7268],\n",
      "        [-26.8229],\n",
      "        [ -4.6880]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [0, 7],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-449.8264, -385.9755, -426.2538, -464.9462, -419.6239, -588.0999,\n",
      "         -505.2892, -541.9769, -595.6876],\n",
      "        [-218.8247, -187.7661, -207.1886, -225.9956, -203.9448, -286.1188,\n",
      "         -245.8777, -263.5775, -289.3727],\n",
      "        [ -47.1385,  -40.7639,  -44.4949,  -48.5587,  -43.9250,  -61.6782,\n",
      "          -53.1635,  -56.8316,  -62.0914],\n",
      "        [-211.8296, -181.9971, -200.9869, -219.7922, -198.2578, -277.5080,\n",
      "         -238.1956, -255.6954, -280.8859],\n",
      "        [ -48.6131,  -42.0270,  -45.8862,  -50.0637,  -45.2870,  -63.5949,\n",
      "          -54.8174,  -58.5992,  -64.0324]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-449.8264, -541.9769],\n",
      "        [-218.8247, -263.5775],\n",
      "        [ -48.5587,  -56.8316],\n",
      "        [-200.9869, -238.1956],\n",
      "        [ -50.0637,  -58.5992]], grad_fn=<GatherBackward>)\n",
      "A [[-467.58463  -400.79602  -442.55063  -482.00134  -435.2033   -610.60675\n",
      "  -525.0784   -562.66046  -618.2559  ]\n",
      " [-217.81378  -187.06992  -206.3564   -225.32138  -203.33487  -284.94867\n",
      "  -244.78497  -262.55936  -288.47028 ]\n",
      " [ -48.613094  -42.02701   -45.886208  -50.063736  -45.287025  -63.594856\n",
      "   -54.817413  -58.599155  -64.0324  ]\n",
      " [-209.7183   -180.1207   -198.88805  -217.36859  -196.10643  -274.6176\n",
      "  -235.79738  -253.02336  -277.91232 ]\n",
      " [ -58.45802   -50.45982   -55.17471   -60.111973  -54.380035  -76.39071\n",
      "   -65.859116  -70.3997    -76.99122 ]]\n",
      "J tensor([-400.7960, -187.0699,  -42.0270, -180.1207,  -50.4598])\n",
      "P tensor([-525.0784, -244.7850,  -54.8174, -235.7974,  -65.8591])\n",
      "expected_state_action_values  tensor([[-411.8974, -523.7516],\n",
      "        [-193.0902, -245.0338],\n",
      "        [ -41.5511,  -53.0625],\n",
      "        [-188.9315, -239.0405],\n",
      "        [ -50.1018,  -63.9612]])\n",
      "5\n",
      "reward -19.227034259031687\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-42.7328],\n",
      "        [-46.9426],\n",
      "        [-39.9305],\n",
      "        [-26.2099],\n",
      "        [-53.5095]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-352.3320, -315.6185, -335.4679, -364.9422, -343.3649, -481.2559,\n",
      "         -426.0238, -430.5935, -487.5744],\n",
      "        [-391.2942, -350.7462, -373.0156, -406.5089, -382.1784, -535.1663,\n",
      "         -473.2272, -478.8040, -542.3795],\n",
      "        [-280.7271, -251.6663, -267.4937, -291.3561, -274.0285, -383.8246,\n",
      "         -339.5799, -343.4091, -388.8329],\n",
      "        [-179.5101, -161.0552, -171.0529, -186.4809, -175.2842, -245.6416,\n",
      "         -217.2020, -219.7069, -248.5442],\n",
      "        [-446.3175, -400.0279, -424.7760, -461.5728, -434.4437, -609.1304,\n",
      "         -539.4634, -545.1729, -617.3815]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-315.6185, -426.0238],\n",
      "        [-391.2942, -478.8040],\n",
      "        [-267.4937, -339.5799],\n",
      "        [-161.0552, -217.2020],\n",
      "        [-424.7760, -539.4634]], grad_fn=<GatherBackward>)\n",
      "A [[-346.27594 -310.33856 -329.87424 -359.17154 -337.77917 -473.29352\n",
      "  -418.72348 -423.4472  -479.52164]\n",
      " [-372.28903 -333.9081  -354.93832 -386.87616 -363.7645  -509.19513\n",
      "  -450.27365 -455.64398 -516.2764 ]\n",
      " [-299.72665 -268.73828 -285.77887 -311.43268 -292.80142 -410.03943\n",
      "  -362.60336 -366.84186 -415.29086]\n",
      " [-176.0863  -158.27736 -168.0875  -183.59886 -172.4757  -241.34044\n",
      "  -213.17285 -215.91396 -244.32545]\n",
      " [-455.65115 -408.10886 -433.58078 -471.08997 -443.34552 -621.8439\n",
      "  -550.7092  -556.4388  -630.0009 ]]\n",
      "J tensor([-310.3386, -333.9081, -268.7383, -158.2774, -408.1089])\n",
      "P tensor([-418.7235, -450.2737, -362.6034, -213.1729, -550.7092])\n",
      "expected_state_action_values  tensor([[-322.0375, -419.5840],\n",
      "        [-347.4599, -452.1889],\n",
      "        [-281.7950, -366.2736],\n",
      "        [-168.6595, -218.0655],\n",
      "        [-420.8075, -549.1478]])\n",
      "reward -14.016359617389949\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[15.2340,  6.4395, 14.7698, 11.9507,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2688,  0.0000, 14.4961, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-50.3941],\n",
      "        [-41.6451],\n",
      "        [-40.3589],\n",
      "        [-24.1008],\n",
      "        [-26.2099]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-349.1620, -344.4676, -335.4527, -378.8710, -357.9389, -499.4995,\n",
      "         -456.0503, -434.8184, -507.4212],\n",
      "        [-349.8451, -344.8124, -335.9778, -379.1816, -358.1841, -500.3333,\n",
      "         -456.8632, -435.4170, -507.7135],\n",
      "        [-289.9738, -285.8220, -278.5035, -314.2921, -296.9101, -414.7692,\n",
      "         -378.7704, -360.9400, -420.6810],\n",
      "        [-169.6338, -167.7356, -163.2651, -184.8526, -174.4311, -243.3053,\n",
      "         -221.8218, -211.7400, -246.7192],\n",
      "        [-173.0611, -170.7910, -166.2547, -187.8835, -177.3886, -247.8085,\n",
      "         -226.1514, -215.6018, -251.1471]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-349.1620, -434.8184],\n",
      "        [-344.8124, -456.8632],\n",
      "        [-285.8220, -378.7704],\n",
      "        [-163.2651, -221.8218],\n",
      "        [-170.7910, -226.1514]], grad_fn=<GatherBackward>)\n",
      "A [[-390.07816 -384.51056 -374.6643  -422.86548 -399.45663 -557.886\n",
      "  -509.39755 -485.53568 -566.25684]\n",
      " [-329.53098 -325.0029  -316.54    -357.40298 -337.63235 -471.37894\n",
      "  -430.39014 -410.28586 -478.5868 ]\n",
      " [-299.12045 -295.1702  -287.53732 -324.81705 -306.81802 -428.14993\n",
      "  -390.83505 -372.68015 -434.61276]\n",
      " [-161.78944 -160.13705 -155.7683  -176.49817 -166.56856 -232.1258\n",
      "  -211.60884 -202.06454 -235.60579]\n",
      " [-169.63379 -167.73564 -163.26508 -184.85262 -174.43112 -243.30534\n",
      "  -221.8218  -211.73997 -246.7192 ]]\n",
      "J tensor([-374.6643, -316.5400, -287.5373, -155.7683, -163.2651])\n",
      "P tensor([-485.5357, -410.2859, -372.6801, -202.0645, -211.7400])\n",
      "expected_state_action_values  tensor([[-387.5919, -487.3761],\n",
      "        [-326.5310, -410.9023],\n",
      "        [-299.1425, -375.7710],\n",
      "        [-164.2923, -205.9589],\n",
      "        [-173.1485, -216.7759]])\n",
      "reward -19.32986484994728\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-25.6968],\n",
      "        [-11.8820],\n",
      "        [-35.7833],\n",
      "        [-52.6400],\n",
      "        [-35.2038]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-155.8240, -153.7667, -148.8851, -163.1298, -153.0411, -213.8606,\n",
      "         -188.4835, -191.5473, -217.3509],\n",
      "        [-105.2395, -103.6401, -100.0961, -109.1246, -102.5501, -143.9079,\n",
      "         -127.2029, -128.8443, -145.8790],\n",
      "        [-222.6379, -219.8782, -212.8005, -232.7977, -218.5131, -305.3053,\n",
      "         -269.2126, -273.6260, -310.5363],\n",
      "        [-408.0530, -401.5620, -388.8887, -423.7262, -398.2070, -557.7388,\n",
      "         -492.8027, -499.7668, -567.1844],\n",
      "        [-186.3524, -183.6716, -177.9305, -194.5040, -182.6120, -255.4117,\n",
      "         -225.3564, -228.7875, -259.4491]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-163.1298, -188.4835],\n",
      "        [-103.6401, -127.2029],\n",
      "        [-212.8005, -269.2126],\n",
      "        [-388.8887, -499.7668],\n",
      "        [-177.9305, -225.3564]], grad_fn=<GatherBackward>)\n",
      "A [[-172.41943  -170.06326  -164.70319  -180.22423  -169.09421  -236.50714\n",
      "  -208.5096   -211.82503  -240.17804 ]\n",
      " [-118.228096 -116.34983  -112.4788   -122.66894  -115.25729  -161.68933\n",
      "  -142.88602  -144.75244  -163.97215 ]\n",
      " [-241.80832  -238.52673  -230.9984   -252.63535  -237.14088  -331.4652\n",
      "  -292.33298  -297.00317  -337.11148 ]\n",
      " [-439.25604  -432.04825  -418.3906   -455.50443  -428.1788   -600.0207\n",
      "  -530.3801   -537.6433   -610.105   ]\n",
      " [-208.95845  -206.1842   -199.91951  -218.99438  -205.4104   -286.92453\n",
      "  -252.80542  -257.0215   -291.5096  ]]\n",
      "J tensor([-164.7032, -112.4788, -230.9984, -418.3906, -199.9195])\n",
      "P tensor([-208.5096, -142.8860, -292.3330, -530.3801, -252.8054])\n",
      "expected_state_action_values  tensor([[-173.9297, -213.3554],\n",
      "        [-113.1129, -140.4794],\n",
      "        [-243.6819, -298.8830],\n",
      "        [-429.1915, -529.9821],\n",
      "        [-215.1313, -262.7287]])\n",
      "reward -18.96337005029194\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 0.0000,  0.0000,  2.2900,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -4.2900],\n",
      "        [-42.7328],\n",
      "        [-30.9730],\n",
      "        [-50.7651],\n",
      "        [-73.4551]])\n",
      "action_batch tensor([[4, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[ -62.5864,  -64.4098,  -61.8045,  -67.5934,  -61.0114,  -85.6062,\n",
      "          -77.9100,  -78.9393,  -86.7331],\n",
      "        [-363.5872, -371.8555, -359.9675, -393.3044, -354.6169, -497.0335,\n",
      "         -451.8048, -458.2966, -504.8274],\n",
      "        [-256.0922, -262.7309, -254.1454, -278.5391, -250.9221, -350.9638,\n",
      "         -318.5033, -323.6994, -356.6959],\n",
      "        [-416.6425, -426.3048, -412.3362, -450.1462, -405.9741, -569.1891,\n",
      "         -517.5712, -524.9617, -578.3096],\n",
      "        [-602.3724, -616.8967, -597.6188, -654.3685, -589.5616, -824.8881,\n",
      "         -748.8156, -760.7524, -838.6079]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -61.0114,  -77.9100],\n",
      "        [-371.8555, -451.8048],\n",
      "        [-254.1454, -318.5033],\n",
      "        [-412.3362, -517.5712],\n",
      "        [-602.3724, -760.7524]], grad_fn=<GatherBackward>)\n",
      "A [[ -58.718212  -60.459167  -57.989586  -63.45678   -57.272976  -80.34753\n",
      "   -73.11269   -74.08613   -81.39099 ]\n",
      " [-357.91632  -366.23166  -354.5357   -387.68484  -349.40762  -489.59256\n",
      "  -444.78876  -451.41467  -497.2906  ]\n",
      " [-226.35509  -232.18103  -224.60085  -246.20558  -221.77905  -310.24265\n",
      "  -281.5382   -286.10104  -315.22604 ]\n",
      " [-429.3782   -439.5184   -425.1467   -464.38034  -418.74762  -586.843\n",
      "  -533.4646   -541.27356  -596.36176 ]\n",
      " [-567.12695  -580.8024   -562.5751   -616.0994   -555.05347  -776.6166\n",
      "  -704.9511   -716.21106  -789.6445  ]]\n",
      "J tensor([ -57.2730, -349.4076, -221.7791, -418.7476, -555.0535])\n",
      "P tensor([ -73.1127, -444.7888, -281.5382, -533.4646, -704.9511])\n",
      "expected_state_action_values  tensor([[ -55.8357,  -70.0914],\n",
      "        [-357.1997, -443.0427],\n",
      "        [-230.5742, -284.3574],\n",
      "        [-427.6379, -530.8832],\n",
      "        [-573.0032, -707.9111]])\n",
      "reward -22.213307605700276\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.8479, 12.3229, 14.8634,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-25.6968],\n",
      "        [-49.9555],\n",
      "        [-18.9634],\n",
      "        [-46.5218],\n",
      "        [-24.7273]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [1, 6],\n",
      "        [3, 6],\n",
      "        [1, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-149.4036, -154.3449, -161.0424, -170.5892, -141.7619, -214.6201,\n",
      "         -189.1805, -192.2476, -218.1265],\n",
      "        [-346.2211, -356.7595, -372.7699, -393.7299, -327.3318, -496.2443,\n",
      "         -437.8983, -444.5610, -504.5299],\n",
      "        [-139.9776, -144.5669, -150.7171, -159.4176, -132.5309, -200.8571,\n",
      "         -177.1947, -179.9294, -204.0443],\n",
      "        [-398.1645, -409.4769, -427.7330, -450.6191, -374.8714, -569.4080,\n",
      "         -503.1929, -510.0455, -578.6731],\n",
      "        [-187.2128, -192.9089, -201.2506, -212.2525, -176.5160, -268.0735,\n",
      "         -236.7736, -240.1128, -272.1252]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-170.5892, -189.1805],\n",
      "        [-356.7595, -437.8983],\n",
      "        [-159.4176, -177.1947],\n",
      "        [-409.4769, -503.1929],\n",
      "        [-187.2128, -240.1128]], grad_fn=<GatherBackward>)\n",
      "A [[-165.26796 -170.65054 -178.094   -188.41637 -156.572   -237.27438\n",
      "  -209.21515 -212.5344  -240.96149]\n",
      " [-367.05615 -378.057   -395.03632 -417.09705 -346.79962 -525.899\n",
      "  -464.1749  -471.11023 -534.7092 ]\n",
      " [-137.91699 -142.31728 -148.41188 -156.86601 -130.40309 -197.81213\n",
      "  -174.55577 -177.16272 -200.79608]\n",
      " [-386.7653  -397.9831  -415.6643  -438.19806 -364.53668 -553.3339\n",
      "  -488.86584 -495.70312 -562.6232 ]\n",
      " [-186.33994 -192.21172 -200.4591  -211.66296 -176.03502 -266.9997\n",
      "  -235.73099 -239.20967 -271.32593]]\n",
      "J tensor([-156.5720, -346.7996, -130.4031, -364.5367, -176.0350])\n",
      "P tensor([-209.2151, -464.1749, -174.5558, -488.8658, -235.7310])\n",
      "expected_state_action_values  tensor([[-166.6116, -213.9904],\n",
      "        [-362.0752, -467.7129],\n",
      "        [-136.3261, -176.0636],\n",
      "        [-374.6048, -486.5010],\n",
      "        [-183.1588, -236.8852]])\n",
      "reward -27.46733434227647\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  7.3475,  9.7098,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  4.7618, 18.3990, 31.2316,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-51.1810],\n",
      "        [-25.3215],\n",
      "        [-34.3329],\n",
      "        [-77.0748],\n",
      "        [-10.7610]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-347.9285, -361.0032, -392.5201, -397.5786, -344.5976, -522.3779,\n",
      "         -448.0786, -454.4559, -531.5414],\n",
      "        [-183.8040, -191.1598, -207.8145, -211.1897, -182.8691, -276.7734,\n",
      "         -237.0114, -240.7425, -281.4690],\n",
      "        [-238.5065, -247.3490, -269.2025, -272.9861, -236.5150, -358.4697,\n",
      "         -307.3434, -311.7231, -364.4109],\n",
      "        [-465.5558, -483.6392, -526.5255, -534.9174, -463.2942, -700.6379,\n",
      "         -600.0236, -609.5741, -713.7080],\n",
      "        [ -91.3667,  -94.8305, -102.7506, -104.0111,  -90.1911, -137.0778,\n",
      "         -117.7359, -119.1672, -139.0389]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-347.9285, -454.4559],\n",
      "        [-207.8145, -237.0114],\n",
      "        [-269.2025, -307.3434],\n",
      "        [-526.5255, -600.0236],\n",
      "        [ -94.8305, -117.7359]], grad_fn=<GatherBackward>)\n",
      "A [[-363.92328  -377.05432  -409.91888  -414.47787  -359.4041   -545.56024\n",
      "  -468.41656  -474.56314  -554.8904  ]\n",
      " [-169.59976  -175.9587   -191.15498  -193.5902   -167.7762   -254.64175\n",
      "  -218.48221  -221.43776  -258.6563  ]\n",
      " [-239.289    -248.14813  -269.90604  -273.52365  -237.08134  -359.3761\n",
      "  -308.2832   -312.56207  -365.49765 ]\n",
      " [-454.5017   -472.258    -514.20685  -522.64966  -452.64246  -684.25\n",
      "  -585.8757   -595.3256   -697.129   ]\n",
      " [ -91.853165  -95.33358  -103.29618  -104.55859   -90.666985 -137.80276\n",
      "  -118.3608   -119.79838  -139.7764  ]]\n",
      "J tensor([-359.4041, -167.7762, -237.0813, -452.6425,  -90.6670])\n",
      "P tensor([-468.4166, -218.4822, -308.2832, -585.8757, -118.3608])\n",
      "expected_state_action_values  tensor([[-374.6447, -472.7559],\n",
      "        [-176.3201, -221.9555],\n",
      "        [-247.7061, -311.7878],\n",
      "        [-484.4530, -604.3629],\n",
      "        [ -92.3613, -117.2857]])\n",
      "reward -30.556983617060453\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  7.3475,  9.7098,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-28.1379],\n",
      "        [-36.9097],\n",
      "        [-35.2009],\n",
      "        [-10.7610],\n",
      "        [-46.5218]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-222.1087, -210.0186, -229.7762, -241.4177, -208.8941, -317.7514,\n",
      "         -280.4815, -284.3379, -322.6455],\n",
      "        [-278.9723, -263.5949, -288.2379, -302.4064, -261.8546, -398.4868,\n",
      "         -352.0772, -356.6601, -405.0043],\n",
      "        [-244.9787, -231.7134, -252.7678, -264.6405, -229.3267, -349.3687,\n",
      "         -309.0186, -312.8295, -355.1277],\n",
      "        [ -92.3436,  -87.3473,  -95.1094,  -99.5332,  -86.2931, -131.7111,\n",
      "         -116.6252, -117.8469, -133.3849],\n",
      "        [-393.6518, -371.5070, -406.0402, -425.0947, -368.2975, -561.2142,\n",
      "         -496.3691, -502.3445, -570.5782]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-229.7762, -280.4815],\n",
      "        [-278.9723, -352.0772],\n",
      "        [-252.7678, -309.0186],\n",
      "        [ -87.3473, -116.6252],\n",
      "        [-371.5070, -496.3691]], grad_fn=<GatherBackward>)\n",
      "A [[-192.28801  -181.94344  -198.88028  -209.0679   -180.98264  -275.0673\n",
      "  -242.85974  -246.1961   -279.59283 ]\n",
      " [-259.69156  -245.46881  -268.38156  -281.64722  -243.86378  -371.04614\n",
      "  -327.78873  -332.10754  -377.0888  ]\n",
      " [-211.5638   -200.77536  -219.01872  -230.25645  -199.34302  -302.71243\n",
      "  -267.19205  -271.14203  -308.04532 ]\n",
      " [ -92.87487   -87.8484    -95.65539  -100.100395  -86.7857   -132.46432\n",
      "  -117.293884 -118.521805 -134.15085 ]\n",
      " [-383.3173   -361.96835  -395.54843  -414.40036  -359.02612  -546.70386\n",
      "  -483.41818  -489.41467  -556.1187  ]]\n",
      "J tensor([-180.9826, -243.8638, -199.3430,  -86.7857, -359.0261])\n",
      "P tensor([-242.8597, -327.7887, -267.1920, -117.2939, -483.4182])\n",
      "expected_state_action_values  tensor([[-191.0223, -246.7117],\n",
      "        [-256.3871, -331.9196],\n",
      "        [-214.6096, -275.6738],\n",
      "        [ -88.8681, -116.3255],\n",
      "        [-369.6453, -481.5981]])\n",
      "5\n",
      "reward -29.9633356789192\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 2.8533,  3.2197,  2.6279,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 16.5628, 10.7409,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  8.5990,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 1.4266,  1.7539,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.2340,  6.4395, 14.7698, 11.9507,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.7128, 18.8469,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-10.7009],\n",
      "        [-52.8627],\n",
      "        [-22.2133],\n",
      "        [-14.0164],\n",
      "        [-71.0280]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 7],\n",
      "        [4, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[ -70.7848,  -67.3299,  -73.4637,  -80.0420,  -69.3885, -105.7840,\n",
      "          -90.9616,  -94.6762, -107.3261],\n",
      "        [-341.5253, -323.5203, -355.6527, -387.5532, -335.6359, -510.6069,\n",
      "         -438.4211, -457.1199, -519.6848],\n",
      "        [-120.8803, -114.8599, -125.9646, -137.4761, -119.0351, -181.0588,\n",
      "         -155.3911, -162.0692, -183.8896],\n",
      "        [-140.8621, -133.3899, -146.3920, -159.3699, -138.0317, -210.4685,\n",
      "         -180.8471, -188.3061, -213.6449],\n",
      "        [-445.4060, -422.0179, -464.1692, -506.0502, -438.2204, -666.2222,\n",
      "         -571.8667, -596.4992, -678.3497]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -73.4637,  -90.9616],\n",
      "        [-323.5203, -457.1199],\n",
      "        [-119.0351, -155.3911],\n",
      "        [-133.3899, -180.8471],\n",
      "        [-464.1692, -571.8667]], grad_fn=<GatherBackward>)\n",
      "A [[ -89.14057   -84.597244  -92.55523  -100.809166  -87.352005 -133.2023\n",
      "  -114.50198  -119.185196 -135.14752 ]\n",
      " [-362.04575  -342.77206  -376.84787  -410.41296  -355.479    -541.0209\n",
      "  -464.67734  -484.32715  -550.5691  ]\n",
      " [-133.13261  -126.69054  -139.04941  -152.20445  -131.71135  -199.83357\n",
      "  -171.2519   -178.91402  -203.28076 ]\n",
      " [-109.70614  -104.073296 -114.036606 -124.301056 -107.67452  -164.02953\n",
      "  -140.91609  -146.78192  -166.58095 ]\n",
      " [-451.06165  -427.6353   -470.3289   -513.04095  -444.21933  -675.01184\n",
      "  -579.2312   -604.4224   -687.4404  ]]\n",
      "J tensor([ -84.5972, -342.7721, -126.6905, -104.0733, -427.6353])\n",
      "P tensor([-114.5020, -464.6773, -171.2519, -140.9161, -579.2312])\n",
      "expected_state_action_values  tensor([[ -86.8384, -113.7527],\n",
      "        [-361.3576, -471.0723],\n",
      "        [-136.2348, -176.3400],\n",
      "        [-107.6823, -140.8408],\n",
      "        [-455.8997, -592.3361]])\n",
      "5\n",
      "reward -28.382236371250027\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-47.6074],\n",
      "        [-50.3626],\n",
      "        [-19.2270],\n",
      "        [-46.9426],\n",
      "        [-51.4207]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [1, 7]])\n",
      "policy_net1(state_batch) tensor([[-306.4754, -303.2807, -306.8421, -347.5206, -325.7870, -458.2015,\n",
      "         -404.9275, -422.1305, -465.9193],\n",
      "        [-365.5580, -361.4957, -365.7465, -413.9481, -388.1945, -546.1039,\n",
      "         -482.8355, -503.1336, -555.5111],\n",
      "        [-105.6550, -104.6150, -105.5326, -119.4941, -112.0695, -157.8946,\n",
      "         -139.6482, -145.3806, -160.1779],\n",
      "        [-346.1272, -342.6117, -346.8082, -393.1529, -368.4477, -517.8585,\n",
      "         -457.4155, -477.0959, -526.7631],\n",
      "        [-404.7242, -400.1770, -404.6901, -457.3429, -429.0443, -604.0690,\n",
      "         -534.3975, -556.5996, -614.2086]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-306.8421, -404.9275],\n",
      "        [-361.4957, -482.8355],\n",
      "        [-104.6150, -139.6482],\n",
      "        [-346.1272, -477.0959],\n",
      "        [-400.1770, -556.5996]], grad_fn=<GatherBackward>)\n",
      "A [[-328.70312 -325.42172 -329.19238 -372.83603 -349.51993 -491.48483\n",
      "  -434.31488 -452.85187 -499.84277]\n",
      " [-382.07224 -377.98462 -382.4263  -432.97293 -405.99518 -570.9565\n",
      "  -504.70706 -526.0712  -580.8963 ]\n",
      " [-147.91011 -146.29129 -147.82306 -167.25601 -156.82564 -220.99722\n",
      "  -195.4283  -203.48619 -224.19916]\n",
      " [-330.27975 -327.12997 -330.97235 -375.2727  -351.73517 -494.1759\n",
      "  -436.50653 -455.35553 -502.90237]\n",
      " [-392.52222 -388.2122  -392.40073 -443.31158 -415.9825  -585.6661\n",
      "  -518.2515  -539.71716 -595.64606]]\n",
      "J tensor([-325.4217, -377.9846, -146.2913, -327.1300, -388.2122])\n",
      "P tensor([-434.3149, -504.7071, -195.4283, -436.5065, -518.2515])\n",
      "expected_state_action_values  tensor([[-340.4870, -438.4908],\n",
      "        [-390.5488, -504.5990],\n",
      "        [-150.8892, -195.1125],\n",
      "        [-341.3596, -439.7985],\n",
      "        [-400.8117, -517.8470]])\n",
      "1\n",
      "reward -24.733855773960688\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  9.6592, 19.7430,  6.0170,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  6.4395, 19.5042,  8.9866,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  9.6592, 19.7430,  6.0170,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-27.9424],\n",
      "        [-26.8229],\n",
      "        [-18.9634],\n",
      "        [-57.3921],\n",
      "        [-56.9031]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [3, 6],\n",
      "        [1, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-185.0588, -201.4079, -203.0966, -221.2245, -207.6223, -291.4217,\n",
      "         -264.7634, -261.0960, -295.7808],\n",
      "        [-168.4295, -183.2832, -184.7014, -201.1047, -188.8183, -265.0591,\n",
      "         -240.9281, -237.4929, -269.1064],\n",
      "        [-130.2088, -141.7684, -142.6523, -155.3428, -145.8528, -204.8544,\n",
      "         -186.2129, -183.5305, -207.9498],\n",
      "        [-431.6108, -467.6859, -471.8067, -511.7433, -480.8529, -676.7314,\n",
      "         -616.0624, -606.2313, -687.2043],\n",
      "        [-444.4529, -481.3554, -485.5440, -526.2325, -494.5730, -696.4184,\n",
      "         -634.2158, -623.8492, -707.0353]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-203.0966, -264.7634],\n",
      "        [-184.7014, -240.9281],\n",
      "        [-155.3428, -186.2129],\n",
      "        [-467.6859, -616.0624],\n",
      "        [-485.5440, -623.8492]], grad_fn=<GatherBackward>)\n",
      "A [[-168.42953 -183.28319 -184.7014  -201.10468 -188.81834 -265.05914\n",
      "  -240.92805 -237.49294 -269.10645]\n",
      " [-166.95692 -181.5898  -182.97495 -199.09903 -186.97185 -262.59067\n",
      "  -238.76364 -235.27032 -266.5498 ]\n",
      " [-127.90005 -139.12851 -140.0349  -152.3847  -143.07652 -201.12616\n",
      "  -182.86588 -180.14912 -204.007  ]\n",
      " [-444.45288 -481.3554  -485.544   -526.23254 -494.57303 -696.4184\n",
      "  -634.21576 -623.84924 -707.03534]\n",
      " [-436.09335 -472.65582 -476.7106  -516.9435  -485.83252 -683.66205\n",
      "  -622.4803  -612.519   -694.34546]]\n",
      "J tensor([-168.4295, -166.9569, -127.9000, -444.4529, -436.0934])\n",
      "P tensor([-237.4929, -235.2703, -180.1491, -623.8492, -612.5190])\n",
      "expected_state_action_values  tensor([[-179.5290, -241.6860],\n",
      "        [-177.0841, -238.5661],\n",
      "        [-134.0734, -181.0976],\n",
      "        [-457.3997, -618.8564],\n",
      "        [-449.3871, -608.1702]])\n",
      "1\n",
      "reward -31.63862360544532\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395, 17.3845,  9.2981,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-47.9592],\n",
      "        [ -7.8685],\n",
      "        [-39.9305],\n",
      "        [-54.0051],\n",
      "        [-18.9634]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [4, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [4, 6]])\n",
      "policy_net1(state_batch) tensor([[-307.7728, -320.4354, -323.7792, -350.0143, -343.5227, -482.9317,\n",
      "         -427.4685, -420.1835, -490.7894],\n",
      "        [ -69.0653,  -72.2218,  -72.4137,  -78.3229,  -76.9674, -108.3139,\n",
      "          -96.0099,  -94.2438, -109.8857],\n",
      "        [-234.6737, -244.4685, -247.0228, -267.2973, -262.3169, -368.5244,\n",
      "         -326.1302, -320.6362, -374.4975],\n",
      "        [-299.3400, -311.6576, -314.7723, -339.8283, -333.6959, -469.3569,\n",
      "         -415.7143, -408.4333, -476.7924],\n",
      "        [-122.9119, -128.4773, -129.5261, -140.4221, -137.7487, -193.3771,\n",
      "         -171.0133, -168.2704, -196.3442]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-307.7728, -427.4685],\n",
      "        [ -76.9674,  -96.0099],\n",
      "        [-247.0228, -326.1302],\n",
      "        [-299.3400, -408.4333],\n",
      "        [-137.7487, -171.0133]], grad_fn=<GatherBackward>)\n",
      "A [[-310.09988  -322.97726  -326.3208   -352.63376  -346.15613  -486.59012\n",
      "  -430.7758   -423.4262   -494.42682 ]\n",
      " [ -70.02539   -73.17198   -73.43928   -79.42735   -78.0188   -109.85773\n",
      "   -97.347824  -95.556435 -111.33475 ]\n",
      " [-249.39096  -259.8696   -262.712    -284.4198   -279.01123  -391.90256\n",
      "  -346.65952  -340.9582   -398.15045 ]\n",
      " [-341.9145   -356.3719   -359.98834  -389.3059   -382.05542  -536.7647\n",
      "  -474.99863  -467.1453   -545.7317  ]\n",
      " [-120.72621  -126.07051  -127.13826  -137.72339  -135.10785  -189.84154\n",
      "  -167.93057  -165.1539   -192.59418 ]]\n",
      "J tensor([-310.0999,  -70.0254, -249.3910, -341.9145, -120.7262])\n",
      "P tensor([-423.4262,  -95.5564, -340.9582, -467.1453, -165.1539])\n",
      "expected_state_action_values  tensor([[-327.0490, -429.0428],\n",
      "        [ -70.8913,  -93.8693],\n",
      "        [-264.3824, -346.7929],\n",
      "        [-361.7281, -474.4359],\n",
      "        [-127.6170, -167.6019]])\n",
      "reward -24.28428961511929\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [19.9728,  6.4395, 23.7921, 22.3220,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 16.5628, 10.7409,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.2340,  6.4395, 14.7698, 11.9507,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-73.1039],\n",
      "        [-74.5264],\n",
      "        [-52.8627],\n",
      "        [-47.4361],\n",
      "        [-25.3215]])\n",
      "action_batch tensor([[1, 7],\n",
      "        [0, 7],\n",
      "        [1, 7],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-494.8163, -491.1791, -516.3923, -537.4234, -487.9341, -740.5817,\n",
      "         -673.4206, -663.5939, -752.7432],\n",
      "        [-506.8116, -503.2077, -529.0798, -550.9927, -500.1651, -758.8286,\n",
      "         -689.8093, -679.9457, -771.5314],\n",
      "        [-373.7711, -370.7279, -389.5989, -405.0810, -367.8521, -558.9291,\n",
      "         -508.4913, -500.7273, -567.7829],\n",
      "        [-393.2949, -389.9414, -409.5082, -425.0301, -386.1815, -587.3677,\n",
      "         -534.8004, -526.2650, -596.5347],\n",
      "        [-198.7251, -197.7281, -207.6051, -216.5242, -196.4384, -297.9832,\n",
      "         -270.6941, -266.9718, -302.5946]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-491.1791, -663.5939],\n",
      "        [-506.8116, -679.9457],\n",
      "        [-370.7279, -500.7273],\n",
      "        [-409.5082, -534.8004],\n",
      "        [-207.6051, -270.6941]], grad_fn=<GatherBackward>)\n",
      "A [[-487.23926 -483.82715 -508.77576 -529.8882  -480.98157 -729.6725\n",
      "  -663.2626  -653.8183  -741.739  ]\n",
      " [-527.06903 -523.08575 -549.79047 -571.8625  -519.28595 -788.4567\n",
      "  -717.13745 -706.5095  -801.4387 ]\n",
      " [-395.6334  -392.21506 -412.21344 -428.36533 -389.048   -591.3603\n",
      "  -538.1325  -529.7585  -600.6571 ]\n",
      " [-372.79446 -369.78613 -388.15298 -402.86984 -366.1096  -556.7047\n",
      "  -506.93362 -498.86502 -565.59296]\n",
      " [-182.0417  -180.72475 -189.6291  -197.13614 -179.00447 -272.24844\n",
      "  -247.71239 -243.85097 -276.15146]]\n",
      "J tensor([-480.9816, -519.2859, -389.0480, -366.1096, -179.0045])\n",
      "P tensor([-653.8183, -706.5095, -529.7585, -498.8650, -243.8510])\n",
      "expected_state_action_values  tensor([[-505.9873, -661.5404],\n",
      "        [-541.8837, -710.3849],\n",
      "        [-403.0059, -529.6454],\n",
      "        [-376.9347, -496.4146],\n",
      "        [-186.4255, -244.7874]])\n",
      "reward -25.749370643181575\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[19.9728,  9.6592, 19.7430,  6.0170,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 14.3632, 15.1425,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.1460, 11.5169,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.7497, 20.6804, 11.0180,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.2406, 12.5768,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-57.3921],\n",
      "        [ -6.7672],\n",
      "        [-80.0695],\n",
      "        [-52.2115],\n",
      "        [-28.3822]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-488.4149, -482.4156, -465.7061, -502.7308, -455.6448, -694.8322,\n",
      "         -614.8111, -640.4077, -707.4734],\n",
      "        [ -54.6014,  -54.4088,  -51.9005,  -56.1827,  -50.9703,  -77.7736,\n",
      "          -68.8815,  -71.6842,  -78.9687],\n",
      "        [-582.8229, -577.1005, -557.3778, -603.7203, -546.7458, -831.3231,\n",
      "         -734.3702, -766.4906, -847.4941],\n",
      "        [-435.6518, -430.7491, -415.5246, -448.4329, -406.5213, -619.7811,\n",
      "         -548.4818, -571.3946, -631.0947],\n",
      "        [-187.3380, -186.0099, -179.3981, -194.8032, -176.2687, -267.8290,\n",
      "         -236.3215, -246.8972, -272.6436]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-482.4156, -614.8111],\n",
      "        [ -54.4088,  -68.8815],\n",
      "        [-577.1005, -734.3702],\n",
      "        [-415.5246, -571.3946],\n",
      "        [-186.0099, -236.3215]], grad_fn=<GatherBackward>)\n",
      "A [[-502.39084  -496.00708  -478.77573  -516.4273   -468.15308  -714.3111\n",
      "  -632.2796   -658.34375  -727.1468  ]\n",
      " [ -80.7265    -80.15476   -76.77993   -82.99135   -75.284386 -114.87677\n",
      "  -101.742744 -105.8813   -116.77552 ]\n",
      " [-585.30273  -579.3563   -559.46014  -605.5339   -548.5047   -834.41046\n",
      "  -737.36334  -769.3305   -850.5059  ]\n",
      " [-385.5078   -381.54684  -368.22318  -398.25168  -360.79855  -549.3221\n",
      "  -485.62015  -506.4246   -559.52405 ]\n",
      " [-172.2771   -171.02516  -164.91533  -178.95421  -161.96202  -246.21295\n",
      "  -217.32938  -226.9623   -250.50125 ]]\n",
      "J tensor([-468.1531,  -75.2844, -548.5047, -360.7986, -161.9620])\n",
      "P tensor([-632.2796, -101.7427, -737.3633, -485.6201, -217.3294])\n",
      "expected_state_action_values  tensor([[-478.7299, -626.4437],\n",
      "        [ -74.5232,  -98.3357],\n",
      "        [-573.7237, -743.6965],\n",
      "        [-376.9302, -489.2697],\n",
      "        [-174.1480, -223.9787]])\n",
      "1\n",
      "reward -26.35642317595378\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  4.7618, 18.3990, 31.2316,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 18.1602, 26.5328,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  4.7618, 18.3990, 31.2316,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-80.0695],\n",
      "        [-77.7920],\n",
      "        [-11.8820],\n",
      "        [-77.0748],\n",
      "        [-24.8327]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-557.8008, -527.9063, -511.4864, -576.0260, -521.6565, -795.0437,\n",
      "         -722.2211, -711.8091, -808.8994],\n",
      "        [-508.3262, -481.1522, -466.0141, -524.7706, -475.3048, -724.3956,\n",
      "         -658.1304, -648.5926, -737.1630],\n",
      "        [-100.5845,  -95.2268,  -91.7752, -103.0910,  -93.4459, -143.0466,\n",
      "         -130.2051, -127.9711, -144.9827],\n",
      "        [-528.0613, -500.0463, -484.3849, -545.7828, -494.2270, -752.9118,\n",
      "         -683.8017, -674.1487, -766.2458],\n",
      "        [-193.1841, -182.8949, -176.7928, -198.6157, -179.9806, -274.9944,\n",
      "         -250.1214, -246.1539, -279.0790]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-527.9063, -722.2211],\n",
      "        [-466.0141, -658.1304],\n",
      "        [ -95.2268, -130.2051],\n",
      "        [-484.3849, -683.8017],\n",
      "        [-176.7928, -246.1539]], grad_fn=<GatherBackward>)\n",
      "A [[-560.60565  -530.37805  -513.78253  -578.1687   -523.7139   -798.5916\n",
      "  -725.7094   -714.98047  -812.3767  ]\n",
      " [-528.0613   -500.04626  -484.38486  -545.78284  -494.22702  -752.9118\n",
      "  -683.8017   -674.14874  -766.24585 ]\n",
      " [-113.37625  -107.24829  -103.478226 -116.27675  -105.38159  -161.26108\n",
      "  -146.75247  -144.25464  -163.51198 ]\n",
      " [-517.35846  -489.98883  -474.70322  -535.1122   -484.53333  -737.873\n",
      "  -670.0385   -660.6925   -751.0545  ]\n",
      " [-185.00832  -175.17229  -169.2967   -190.3042   -172.47484  -263.37607\n",
      "  -239.5567   -235.76611  -267.4619  ]]\n",
      "J tensor([-513.7825, -484.3849, -103.4782, -474.7032, -169.2967])\n",
      "P tensor([-714.9805, -674.1487, -144.2546, -660.6925, -235.7661])\n",
      "expected_state_action_values  tensor([[-542.4738, -723.5519],\n",
      "        [-513.7383, -684.5258],\n",
      "        [-105.0124, -141.7112],\n",
      "        [-504.3077, -671.6980],\n",
      "        [-177.1997, -237.0222]])\n",
      "reward -22.498344769605364\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[21.3995,  6.4395, 18.7128, 18.8469,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.8479, 12.3229, 14.8634,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 14.3632, 15.1425,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  4.7618, 18.3990, 31.2316,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-67.3986],\n",
      "        [-50.3005],\n",
      "        [-23.4019],\n",
      "        [-28.1379],\n",
      "        [-77.0748]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-503.2253, -495.5452, -479.4042, -517.2557, -468.8206, -715.9888,\n",
      "         -668.8330, -622.6421, -727.6566],\n",
      "        [-387.1954, -381.5353, -369.0918, -398.4974, -361.0559, -551.2983,\n",
      "         -514.7812, -479.4011, -560.0203],\n",
      "        [-174.4487, -172.2241, -166.4747, -180.2945, -163.1780, -248.9584,\n",
      "         -232.1550, -216.4138, -252.6505],\n",
      "        [-239.3306, -235.8387, -228.3191, -246.8921, -223.5074, -341.2686,\n",
      "         -318.4031, -296.6140, -346.1508],\n",
      "        [-556.0136, -548.4152, -530.8289, -574.2396, -520.0571, -792.7112,\n",
      "         -739.5465, -689.4557, -806.1545]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-495.5452, -668.8330],\n",
      "        [-387.1954, -479.4011],\n",
      "        [-166.4747, -232.1550],\n",
      "        [-228.3191, -318.4031],\n",
      "        [-530.8289, -739.5465]], grad_fn=<GatherBackward>)\n",
      "A [[-532.03613 -523.9374  -507.0597  -547.3507  -495.9661  -757.3058\n",
      "  -707.2113  -658.52716 -769.55316]\n",
      " [-370.93643 -365.4969  -353.63916 -382.0654  -346.0771  -528.33185\n",
      "  -493.1838  -459.3776  -536.73065]\n",
      " [-178.07343 -175.55124 -169.62675 -183.26337 -165.98012 -253.66495\n",
      "  -236.81335 -220.48645 -257.28903]\n",
      " [-207.38478 -204.47235 -197.79688 -213.99272 -193.80103 -295.69376\n",
      "  -275.93924 -257.05228 -300.1958 ]\n",
      " [-544.6224  -537.26447 -520.0963  -562.8656  -509.72556 -776.69183\n",
      "  -724.4978  -675.5319  -789.97626]]\n",
      "J tensor([-495.9661, -346.0771, -165.9801, -193.8010, -509.7256])\n",
      "P tensor([-658.5272, -459.3776, -220.4865, -257.0523, -675.5319])\n",
      "expected_state_action_values  tensor([[-513.7681, -660.0731],\n",
      "        [-361.7699, -463.7404],\n",
      "        [-172.7840, -221.8397],\n",
      "        [-202.5589, -259.4850],\n",
      "        [-535.8278, -685.0535]])\n",
      "reward -25.03607551187796\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  3.2406, 11.1610,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  8.5990,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-40.3589],\n",
      "        [-27.9424],\n",
      "        [-27.4673],\n",
      "        [-34.1165],\n",
      "        [-32.9790]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [4, 6],\n",
      "        [0, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-274.6917, -294.4371, -284.5214, -295.6036, -267.8613, -409.1792,\n",
      "         -372.1674, -345.3214, -415.6180],\n",
      "        [-190.7462, -205.0562, -198.1240, -206.5449, -186.9620, -284.9963,\n",
      "         -258.7838, -240.5431, -289.4407],\n",
      "        [-143.7761, -154.7538, -149.3314, -155.8380, -141.0733, -214.9172,\n",
      "         -195.1203, -181.4158, -218.3499],\n",
      "        [-229.9229, -246.6886, -238.3658, -248.1433, -224.6998, -342.9519,\n",
      "         -311.6300, -289.3988, -348.4248],\n",
      "        [-218.1227, -233.9038, -225.8813, -234.9155, -212.8379, -325.0126,\n",
      "         -295.5338, -274.2767, -330.2719]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-294.4371, -372.1674],\n",
      "        [-198.1240, -258.7838],\n",
      "        [-141.0733, -195.1203],\n",
      "        [-229.9229, -311.6300],\n",
      "        [-218.1227, -274.2767]], grad_fn=<GatherBackward>)\n",
      "A [[-285.5959  -306.47922 -296.0809  -307.93805 -279.01865 -425.72815\n",
      "  -387.06708 -359.3833  -432.7991 ]\n",
      " [-174.27151 -187.31537 -180.87134 -188.46985 -170.67764 -260.2113\n",
      "  -236.39651 -219.64124 -264.34814]\n",
      " [-155.18591 -167.10663 -161.31805 -168.37427 -152.35915 -232.10759\n",
      "  -210.63557 -195.92694 -235.70844]\n",
      " [-243.27692 -260.78394 -252.07732 -262.11496 -237.44215 -362.6147\n",
      "  -329.68954 -305.97565 -368.2375 ]\n",
      " [-229.9229  -246.68858 -238.36577 -248.14333 -224.69983 -342.95193\n",
      "  -311.63004 -289.39883 -348.42477]]\n",
      "J tensor([-279.0186, -170.6776, -152.3591, -237.4422, -224.6998])\n",
      "P tensor([-359.3833, -219.6412, -195.9269, -305.9756, -289.3988])\n",
      "expected_state_action_values  tensor([[-291.4756, -363.8039],\n",
      "        [-181.5523, -225.6195],\n",
      "        [-164.5906, -203.8016],\n",
      "        [-247.8144, -309.4946],\n",
      "        [-235.2088, -293.4380]])\n",
      "reward -28.10141792176816\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  4.7618, 18.3990, 31.2316,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 10.5880, 11.7574,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 18.1602, 26.5328,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.2270],\n",
      "        [-37.9897],\n",
      "        [-39.9305],\n",
      "        [-52.6400],\n",
      "        [-77.7920]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [4, 7],\n",
      "        [2, 6],\n",
      "        [2, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-110.2173, -108.4305, -104.5654, -113.2833, -110.5514, -156.8023,\n",
      "         -138.5606, -136.0969, -159.1409],\n",
      "        [-278.0087, -273.6037, -264.6567, -287.0672, -280.0250, -396.0347,\n",
      "         -349.5922, -343.9842, -402.9879],\n",
      "        [-258.7231, -254.4494, -246.1228, -266.7919, -260.3002, -368.3649,\n",
      "         -325.2862, -319.9061, -374.6942],\n",
      "        [-385.3641, -379.0520, -366.3554, -396.4411, -387.0192, -547.9758,\n",
      "         -484.2149, -476.0689, -557.7654],\n",
      "        [-487.7211, -480.1706, -464.8312, -504.7652, -492.1662, -695.2965,\n",
      "         -613.3243, -604.0143, -708.2787]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-108.4305, -138.5606],\n",
      "        [-280.0250, -343.9842],\n",
      "        [-246.1228, -325.2862],\n",
      "        [-366.3554, -476.0689],\n",
      "        [-464.8312, -613.3243]], grad_fn=<GatherBackward>)\n",
      "A [[-153.35696 -150.71217 -145.58736 -157.59602 -153.77374 -218.13612\n",
      "  -192.74077 -189.33545 -221.39865]\n",
      " [-276.02118 -271.34244 -262.53036 -284.4076  -277.4887  -392.89322\n",
      "  -346.98892 -341.17755 -399.45605]\n",
      " [-274.00058 -269.54138 -260.8378  -282.89218 -275.90207 -390.3578\n",
      "  -344.5499  -338.99063 -396.9694 ]\n",
      " [-415.17868 -408.15726 -394.47168 -426.50998 -416.48813 -590.00055\n",
      "  -521.5611  -512.5672  -600.4639 ]\n",
      " [-505.89017 -498.2865  -482.43976 -524.2126  -510.99728 -721.59326\n",
      "  -636.2848  -626.88275 -735.13116]]\n",
      "J tensor([-145.5874, -262.5304, -260.8378, -394.4717, -482.4398])\n",
      "P tensor([-189.3354, -341.1776, -338.9906, -512.5672, -626.8828])\n",
      "expected_state_action_values  tensor([[-150.2556, -189.6289],\n",
      "        [-274.2670, -345.0495],\n",
      "        [-274.6845, -345.0221],\n",
      "        [-407.6645, -513.9504],\n",
      "        [-511.9877, -641.9865]])\n",
      "reward -29.308831841271527\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3717,  6.4395,  5.6897,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3428,  6.4395,  5.9774, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 10.5880, 11.7574,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-11.8820],\n",
      "        [-25.3215],\n",
      "        [-24.2843],\n",
      "        [-37.9897],\n",
      "        [-50.7651]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [4, 7],\n",
      "        [4, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-102.6758, -105.0678, -101.2013, -105.4201,  -95.4826, -146.0589,\n",
      "         -132.6697, -130.4827, -148.0916],\n",
      "        [-208.5315, -213.7422, -206.5248, -215.9256, -195.3099, -297.5741,\n",
      "         -269.6545, -265.9884, -302.4748],\n",
      "        [-202.4674, -207.5682, -200.5448, -209.7094, -189.6774, -288.9642,\n",
      "         -261.8304, -258.2952, -293.7166],\n",
      "        [-292.3092, -299.0280, -289.1078, -301.6443, -273.0836, -416.4021,\n",
      "         -377.7785, -372.2115, -423.4157],\n",
      "        [-392.5607, -401.1843, -387.5515, -403.2272, -365.3450, -558.0008,\n",
      "         -506.8733, -498.8626, -567.4484]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-105.0678, -132.6697],\n",
      "        [-206.5248, -269.6545],\n",
      "        [-189.6774, -258.2952],\n",
      "        [-273.0836, -372.2115],\n",
      "        [-387.5515, -506.8733]], grad_fn=<GatherBackward>)\n",
      "A [[-115.56042  -118.16481  -113.932365 -118.72587  -107.5158   -164.41241\n",
      "  -149.30783  -146.86508  -166.76666 ]\n",
      " [-190.90685  -195.27069  -188.56096  -196.49878  -177.89816  -271.7478\n",
      "  -246.6513   -242.84563  -275.91577 ]\n",
      " [-182.43987  -186.62025  -180.22409  -187.88612  -170.0762   -259.76752\n",
      "  -235.73528  -232.12363  -263.72906 ]\n",
      " [-290.0667   -296.4188   -286.6459   -298.72427  -270.47403  -412.89792\n",
      "  -374.76715  -368.99957  -419.51813 ]\n",
      " [-404.4622   -413.52838  -399.50394  -415.90213  -376.7697   -575.1775\n",
      "  -522.3244   -514.24927  -585.0323  ]]\n",
      "J tensor([-107.5158, -177.8982, -170.0762, -270.4740, -376.7697])\n",
      "P tensor([-146.8651, -242.8456, -232.1236, -368.9996, -514.2493])\n",
      "expected_state_action_values  tensor([[-108.6462, -144.0605],\n",
      "        [-185.4298, -243.8826],\n",
      "        [-177.3529, -233.1955],\n",
      "        [-281.4163, -370.0893],\n",
      "        [-389.8578, -513.5894]])\n",
      "1\n",
      "reward -28.582777594007826\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[17.1196,  6.4395, 16.5628, 10.7409,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.4549,  4.3198,  7.2281,  8.3536,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.2107,  6.4395,  6.5373,  6.9140,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[15.2340,  6.4395, 14.7698, 11.9507,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  5.0432,  7.1056,  5.8941,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.5108,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-52.8627],\n",
      "        [-26.3564],\n",
      "        [-46.5218],\n",
      "        [-39.9305],\n",
      "        [-28.1014]])\n",
      "action_batch tensor([[1, 7],\n",
      "        [1, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-395.5096, -419.3865, -405.4731, -406.3543, -396.6925, -562.3713,\n",
      "         -525.1563, -488.7404, -571.7105],\n",
      "        [-197.3957, -209.5740, -202.3212, -202.8382, -197.9891, -280.7848,\n",
      "         -262.1859, -244.0112, -285.1854],\n",
      "        [-414.6026, -439.1443, -424.5745, -424.9268, -415.0067, -588.8968,\n",
      "         -550.2791, -511.7305, -598.4742],\n",
      "        [-273.7052, -290.3434, -280.6362, -281.3622, -274.6818, -389.3173,\n",
      "         -363.5592, -338.3366, -395.6600],\n",
      "        [-169.0554, -179.8174, -173.5833, -174.5176, -170.2037, -240.9498,\n",
      "         -224.7269, -209.4162, -244.8779]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-419.3865, -488.7404],\n",
      "        [-209.5740, -244.0112],\n",
      "        [-439.1443, -550.2791],\n",
      "        [-280.6362, -363.5592],\n",
      "        [-173.5833, -209.4162]], grad_fn=<GatherBackward>)\n",
      "A [[-418.08353 -443.11667 -428.44778 -429.14465 -419.0119  -594.22186\n",
      "  -555.03973 -516.3992  -604.0127 ]\n",
      " [-174.83621 -185.92766 -179.42271 -180.15431 -175.76561 -249.00018\n",
      "  -232.34947 -216.43124 -252.9691 ]\n",
      " [-404.76932 -428.956   -414.6684  -415.30222 -405.56796 -575.1533\n",
      "  -537.31757 -499.84088 -584.79315]\n",
      " [-289.26987 -306.9344  -296.7892  -297.70932 -290.5348  -411.6971\n",
      "  -384.30182 -357.77026 -418.30984]\n",
      " [-192.39435 -204.29517 -197.30745 -197.93831 -193.13007 -273.84143\n",
      "  -255.60017 -237.95105 -278.03677]]\n",
      "J tensor([-418.0835, -174.8362, -404.7693, -289.2699, -192.3943])\n",
      "P tensor([-516.3992, -216.4312, -499.8409, -357.7703, -237.9510])\n",
      "expected_state_action_values  tensor([[-429.1379, -517.6220],\n",
      "        [-183.7090, -221.1445],\n",
      "        [-410.8142, -496.3786],\n",
      "        [-300.2734, -361.9238],\n",
      "        [-201.2563, -242.2574]])\n",
      "1\n",
      "reward -23.46328087547835\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  5.8472,  5.9286,  9.1005,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 16.5628, 10.7409,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.1372,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [15.2340,  6.4395, 14.7698, 11.9507,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-14.0164],\n",
      "        [-30.4570],\n",
      "        [-28.5828],\n",
      "        [-52.8627],\n",
      "        [-81.8207]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [1, 7],\n",
      "        [0, 6]])\n",
      "policy_net1(state_batch) tensor([[-157.1332, -160.7503, -166.8333, -162.0281, -157.7565, -223.8705,\n",
      "         -203.1565, -200.0049, -227.2459],\n",
      "        [-199.0048, -204.3690, -212.4143, -207.3941, -201.5918, -284.7766,\n",
      "         -257.7336, -254.5742, -289.6107],\n",
      "        [-189.4717, -194.2778, -201.8058, -196.6795, -191.2749, -270.6659,\n",
      "         -245.1826, -241.8987, -275.1548],\n",
      "        [-386.1876, -395.2203, -410.7859, -399.3529, -388.7925, -550.5334,\n",
      "         -499.2985, -492.1274, -560.2156],\n",
      "        [-514.5003, -527.6285, -548.8014, -535.2625, -520.6226, -735.2481,\n",
      "         -665.7975, -657.4518, -749.1875]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-160.7503, -203.1565],\n",
      "        [-212.4143, -257.7336],\n",
      "        [-189.4717, -241.8987],\n",
      "        [-395.2203, -492.1274],\n",
      "        [-514.5003, -665.7975]], grad_fn=<GatherBackward>)\n",
      "A [[-123.300156 -126.32346  -130.93198  -127.30935  -123.94521  -175.77057\n",
      "  -159.47258  -157.05376  -178.49258 ]\n",
      " [-194.89232  -199.73041  -207.53441  -202.04813  -196.58412  -278.27353\n",
      "  -252.21414  -248.70566  -282.7508  ]\n",
      " [-180.43121  -185.1405   -192.2139   -187.34575  -182.20602  -257.77994\n",
      "  -233.50597  -230.41869  -262.09048 ]\n",
      " [-407.8527   -417.19086  -433.65524  -421.34915  -410.28043  -581.16815\n",
      "  -527.2271   -519.49146  -591.31165 ]\n",
      " [-553.724    -567.44135  -590.2627   -575.1465   -559.5416   -790.79004\n",
      "  -716.37555  -707.04877  -805.4356  ]]\n",
      "J tensor([-123.3002, -194.8923, -180.4312, -407.8527, -553.7240])\n",
      "P tensor([-157.0538, -248.7057, -230.4187, -519.4915, -707.0488])\n",
      "expected_state_action_values  tensor([[-124.9865, -155.3647],\n",
      "        [-205.8600, -254.2921],\n",
      "        [-190.9709, -235.9596],\n",
      "        [-419.9302, -520.4050],\n",
      "        [-580.1722, -718.1646]])\n",
      "reward -30.790073106536905\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  3.2197,  5.5108,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  6.3968,  6.6019,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 10.1104,  9.0302,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-25.0361],\n",
      "        [-45.6004],\n",
      "        [-28.3724],\n",
      "        [-47.5241],\n",
      "        [-40.9402]])\n",
      "action_batch tensor([[4, 7],\n",
      "        [1, 6],\n",
      "        [0, 6],\n",
      "        [2, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-167.0584, -170.2193, -164.3065, -165.6008, -161.1646, -228.2238,\n",
      "         -212.3962, -209.7341, -231.9006],\n",
      "        [-449.3992, -456.5987, -441.6378, -444.2271, -432.6308, -612.9284,\n",
      "         -570.9063, -563.2871, -623.4623],\n",
      "        [-204.9653, -208.6461, -201.5081, -202.9553, -197.5714, -279.8421,\n",
      "         -260.5163, -257.1686, -284.4497],\n",
      "        [-383.2849, -389.6023, -376.7556, -378.7753, -368.9211, -522.7859,\n",
      "         -487.0059, -480.4824, -531.4128],\n",
      "        [-329.4454, -334.7387, -323.8697, -325.8636, -317.2754, -449.5666,\n",
      "         -418.6573, -413.0766, -456.8487]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-161.1646, -209.7341],\n",
      "        [-456.5987, -570.9063],\n",
      "        [-204.9653, -260.5163],\n",
      "        [-376.7556, -480.4824],\n",
      "        [-334.7387, -418.6573]], grad_fn=<GatherBackward>)\n",
      "A [[-180.69717 -184.1563  -177.88805 -179.54387 -174.64532 -247.08806\n",
      "  -229.79568 -227.06227 -251.1812 ]\n",
      " [-447.42487 -454.12946 -439.14673 -440.96606 -429.70618 -609.4847\n",
      "  -568.1666  -560.0764  -619.686  ]\n",
      " [-206.83269 -210.48085 -203.35808 -204.55392 -199.17827 -282.3121\n",
      "  -262.92453 -259.43542 -286.6198 ]\n",
      " [-395.86923 -402.18256 -388.9294  -390.7694  -380.68338 -539.69037\n",
      "  -502.90787 -495.991   -548.5207 ]\n",
      " [-366.418   -372.2014  -359.99005 -361.90857 -352.53635 -499.63968\n",
      "  -465.52472 -459.14453 -507.94766]]\n",
      "J tensor([-174.6453, -429.7062, -199.1783, -380.6834, -352.5363])\n",
      "P tensor([-227.0623, -560.0764, -259.4354, -495.9910, -459.1445])\n",
      "expected_state_action_values  tensor([[-182.2169, -229.3921],\n",
      "        [-432.3360, -549.6692],\n",
      "        [-207.6328, -261.8643],\n",
      "        [-390.1391, -493.9159],\n",
      "        [-358.2230, -454.1703]])\n",
      "1\n",
      "reward -32.9641407443207\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  6.9588,  7.0048,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.1372,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 10.5880, 11.7574,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  5.8472,  5.9286,  9.1005,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 6.2107,  6.4395,  6.5373,  6.9140,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-23.4633],\n",
      "        [-27.4673],\n",
      "        [-29.3088],\n",
      "        [-41.6451],\n",
      "        [-38.9782]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [1, 6],\n",
      "        [4, 7],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-214.7924, -202.2472, -210.0038, -204.4087, -212.6199, -281.6227,\n",
      "         -268.7740, -265.8214, -285.9594],\n",
      "        [-232.5260, -218.7091, -227.3378, -221.3249, -230.2489, -304.8716,\n",
      "         -291.0065, -287.7289, -309.6289],\n",
      "        [-223.0733, -209.8388, -217.8927, -211.7898, -220.4055, -292.2019,\n",
      "         -279.0452, -275.7784, -296.5801],\n",
      "        [-408.6922, -383.9173, -399.0470, -387.4767, -403.4406, -534.8527,\n",
      "         -511.0321, -504.8501, -543.3733],\n",
      "        [-336.5819, -316.3767, -328.8948, -319.6542, -332.7149, -440.8473,\n",
      "         -421.0426, -416.1177, -447.7614]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-214.7924, -265.8214],\n",
      "        [-218.7091, -291.0065],\n",
      "        [-220.4055, -275.7784],\n",
      "        [-383.9173, -511.0321],\n",
      "        [-328.8948, -421.0426]], grad_fn=<GatherBackward>)\n",
      "A [[-190.53102 -179.46626 -186.14102 -181.02422 -188.40448 -249.62975\n",
      "  -238.37971 -235.66168 -253.51938]\n",
      " [-269.01776 -252.82095 -262.63065 -255.22511 -265.69443 -352.19342\n",
      "  -336.43945 -332.39633 -357.77783]\n",
      " [-225.14491 -211.85968 -220.09023 -214.21165 -222.80887 -295.1714\n",
      "  -281.7085  -278.57123 -299.6775 ]\n",
      " [-390.4963  -367.02826 -381.34732 -370.44824 -385.71295 -511.12363\n",
      "  -488.31876 -482.51785 -519.52936]\n",
      " [-343.2537  -322.76648 -335.41565 -326.02075 -339.3922  -449.54782\n",
      "  -429.38852 -424.39786 -456.8469 ]]\n",
      "J tensor([-179.4663, -252.8210, -211.8597, -367.0283, -322.7665])\n",
      "P tensor([-235.6617, -332.3963, -278.5712, -482.5179, -424.3979])\n",
      "expected_state_action_values  tensor([[-184.9829, -235.5588],\n",
      "        [-255.0061, -326.6240],\n",
      "        [-219.9825, -280.0229],\n",
      "        [-371.9705, -475.9111],\n",
      "        [-329.4680, -420.9362]])\n",
      "5\n",
      "reward -30.411895747894754\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.7497, 20.6804, 11.0180,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 18.1602, 26.5328,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  3.2406, 11.1610,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[18.5462,  3.2197, 18.7128, 14.9720,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  8.5990,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-60.7464],\n",
      "        [-54.7144],\n",
      "        [-36.4907],\n",
      "        [-71.3122],\n",
      "        [-27.4673]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [4, 6]])\n",
      "policy_net1(state_batch) tensor([[-493.9462, -465.8946, -520.4110, -489.0439, -477.1577, -674.1050,\n",
      "         -627.8474, -642.5875, -685.7024],\n",
      "        [-462.3970, -436.0499, -486.8671, -457.0464, -446.0363, -630.6488,\n",
      "         -587.5960, -601.1490, -641.1914],\n",
      "        [-212.1185, -200.2906, -223.6587, -210.4785, -205.2085, -289.9208,\n",
      "         -269.8263, -276.2611, -294.3618],\n",
      "        [-548.7773, -518.0026, -578.9969, -545.0520, -531.4152, -749.9548,\n",
      "         -697.8361, -714.8968, -763.0262],\n",
      "        [-164.0104, -155.2427, -173.1671, -163.4100, -159.2206, -224.5515,\n",
      "         -208.7665, -214.0141, -228.2038]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-465.8946, -627.8474],\n",
      "        [-486.8671, -587.5960],\n",
      "        [-223.6587, -269.8263],\n",
      "        [-518.0026, -697.8361],\n",
      "        [-159.2206, -208.7665]], grad_fn=<GatherBackward>)\n",
      "A [[-522.56366 -492.85547 -550.5823  -517.4212  -504.8344  -713.1753\n",
      "  -664.21906 -679.82904 -725.4831 ]\n",
      " [-450.6664  -424.93127 -474.44614 -445.17896 -434.5283  -614.51807\n",
      "  -572.6915  -585.7771  -624.6301 ]\n",
      " [-241.39537 -227.82164 -254.69208 -239.88383 -233.74951 -330.15936\n",
      "  -307.10025 -314.54324 -335.1671 ]\n",
      " [-554.9714  -524.02435 -585.6416  -551.52167 -537.71796 -758.54913\n",
      "  -705.76    -723.1515  -772.08685]\n",
      " [-175.39317 -166.08543 -185.33023 -174.92505 -170.37476 -240.26624\n",
      "  -223.28558 -228.99718 -244.06937]]\n",
      "J tensor([-492.8555, -424.9313, -227.8216, -524.0244, -166.0854])\n",
      "P tensor([-664.2191, -572.6915, -307.1003, -705.7600, -223.2856])\n",
      "expected_state_action_values  tensor([[-504.3163, -658.5435],\n",
      "        [-437.1526, -570.1368],\n",
      "        [-241.5302, -312.8809],\n",
      "        [-542.9341, -706.4963],\n",
      "        [-176.9442, -228.4243]])\n",
      "reward -27.32718820212903\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.7497, 20.6804, 11.0180,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-10.7610],\n",
      "        [ -3.5828],\n",
      "        [-54.7144],\n",
      "        [-41.6451],\n",
      "        [-32.2255]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-105.7054, -103.6402, -107.4167, -104.1400, -108.4156, -144.1584,\n",
      "         -137.9188, -137.3064, -145.9080],\n",
      "        [ -31.1036,  -30.8672,  -31.5777,  -30.8006,  -32.0707,  -42.6012,\n",
      "          -40.7489,  -40.5612,  -42.9016],\n",
      "        [-483.9644, -473.9914, -492.8293, -478.1447, -497.4758, -660.0940,\n",
      "         -630.9666, -629.1867, -670.6894],\n",
      "        [-392.1478, -384.0394, -399.4556, -387.9359, -403.4447, -535.2067,\n",
      "         -511.3678, -510.0394, -543.6813],\n",
      "        [-183.5924, -180.4958, -187.4079, -182.6933, -189.7695, -251.2520,\n",
      "         -239.6891, -239.4803, -255.3018]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-103.6402, -137.9188],\n",
      "        [ -31.1036,  -40.5612],\n",
      "        [-492.8293, -630.9666],\n",
      "        [-384.0394, -511.3678],\n",
      "        [-187.4079, -239.6891]], grad_fn=<GatherBackward>)\n",
      "A [[-106.331474 -104.25255  -108.05196  -104.75184  -109.05409  -145.00809\n",
      "  -138.73372  -138.11696  -146.7715  ]\n",
      " [ -54.122444  -53.43498   -54.94559   -53.475533  -55.652214  -73.92499\n",
      "   -70.67532   -70.428345  -74.8694  ]\n",
      " [-472.85     -463.04675  -481.44498  -466.89423  -485.85544  -644.80316\n",
      "  -616.4746   -614.61646  -654.9989  ]\n",
      " [-374.8718   -367.32928  -381.9313   -371.07684  -385.91498  -511.7199\n",
      "  -488.88632  -487.7234   -520.08606 ]\n",
      " [-233.71992  -229.53566  -238.55539  -232.36598  -241.37671  -319.73233\n",
      "  -305.06058  -304.72598  -324.7804  ]]\n",
      "J tensor([-104.2525,  -53.4350, -463.0468, -367.3293, -229.5357])\n",
      "P tensor([-138.1170,  -70.4283, -614.6165, -487.7234, -304.7260])\n",
      "expected_state_action_values  tensor([[-104.5883, -135.0663],\n",
      "        [ -51.6743,  -66.9684],\n",
      "        [-471.4565, -607.8692],\n",
      "        [-372.2414, -480.5961],\n",
      "        [-238.8076, -306.4789]])\n",
      "5\n",
      "reward -29.739135179763025\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395, 17.3845,  9.2981,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  0.0000, 18.4464, 14.4922,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-27.4673],\n",
      "        [-32.6959],\n",
      "        [ -6.7672],\n",
      "        [-24.1008],\n",
      "        [-45.1085]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-222.5627, -201.6836, -210.4148, -212.2379, -220.5017, -292.0181,\n",
      "         -271.6955, -285.6103, -296.7342],\n",
      "        [-304.6832, -276.0297, -287.6552, -289.2789, -300.8388, -398.9828,\n",
      "         -371.6336, -390.3521, -405.4722],\n",
      "        [ -62.3166,  -56.8343,  -58.6383,  -59.0350,  -61.4496,  -81.6161,\n",
      "          -76.0947,  -79.8197,  -82.7431],\n",
      "        [-186.8922, -169.6718, -176.7745, -178.4008, -185.2596, -245.3632,\n",
      "         -228.1607, -240.0026, -249.2426],\n",
      "        [-390.2454, -353.7084, -368.6160, -370.8197, -385.7382, -511.0543,\n",
      "         -476.0669, -500.1813, -519.9765]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-201.6836, -271.6955],\n",
      "        [-287.6552, -390.3521],\n",
      "        [ -56.8343,  -76.0947],\n",
      "        [-176.7745, -228.1607],\n",
      "        [-368.6160, -476.0669]], grad_fn=<GatherBackward>)\n",
      "A [[-258.19858  -233.77242  -243.73116  -245.38564  -255.11557  -338.24954\n",
      "  -314.9707   -330.83466  -343.80475 ]\n",
      " [-285.5773   -258.72754  -269.4375   -270.69803  -281.6529   -373.7003\n",
      "  -348.27164  -365.64902  -379.781   ]\n",
      " [ -90.81641   -82.539406  -85.49323   -85.96934   -89.49083  -118.847664\n",
      "  -110.82161  -116.23894  -120.61249 ]\n",
      " [-182.69948  -166.00197  -172.85179  -174.56465  -181.28685  -239.90863\n",
      "  -223.06912  -234.72116  -243.93512 ]\n",
      " [-387.3491   -350.59225  -365.50366  -367.06207  -381.95578  -506.79776\n",
      "  -472.3793   -495.8637   -514.993   ]]\n",
      "J tensor([-233.7724, -258.7275,  -82.5394, -166.0020, -350.5923])\n",
      "P tensor([-314.9707, -348.2716, -110.8216, -223.0691, -472.3793])\n",
      "expected_state_action_values  tensor([[-237.8625, -310.9409],\n",
      "        [-265.5506, -346.1403],\n",
      "        [ -81.0527, -106.5067],\n",
      "        [-173.5026, -224.8630],\n",
      "        [-360.6415, -470.2498]])\n",
      "reward -31.92729203954438\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  1.7539,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 10.1104,  9.0302,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[ -7.8685],\n",
      "        [-40.4197],\n",
      "        [-33.5341],\n",
      "        [-10.7009],\n",
      "        [-53.1408]])\n",
      "action_batch tensor([[4, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [4, 8],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[ -87.4077,  -82.4989,  -79.3655,  -82.7772,  -86.1579, -114.3816,\n",
      "         -103.9255, -109.0076, -116.1548],\n",
      "        [-324.5989, -305.3841, -295.9072, -309.0052, -321.2142, -425.3782,\n",
      "         -385.8355, -405.5223, -432.8517],\n",
      "        [-221.7322, -208.6694, -202.2079, -211.3024, -219.5338, -290.8150,\n",
      "         -263.6647, -277.1482, -295.5039],\n",
      "        [ -80.7747,  -76.1723,  -73.3528,  -76.3823,  -79.5365, -105.6950,\n",
      "          -96.0978, -100.7023, -107.0589],\n",
      "        [-459.7741, -431.9505, -418.2629, -435.2768, -453.0491, -601.0563,\n",
      "         -546.0372, -573.0629, -611.5498]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -86.1579, -103.9255],\n",
      "        [-305.3841, -385.8355],\n",
      "        [-202.2079, -263.6647],\n",
      "        [ -79.5365, -107.0589],\n",
      "        [-453.0491, -573.0629]], grad_fn=<GatherBackward>)\n",
      "A [[ -86.810585  -81.891685  -78.83602   -82.22135   -85.54802  -113.633385\n",
      "  -103.218765 -108.26383  -115.274185]\n",
      " [-330.18643  -310.2758   -300.78763  -313.79953  -326.25043  -432.4573\n",
      "  -392.3943   -412.16092  -439.7093  ]\n",
      " [-213.08885  -200.80437  -194.58224  -203.68253  -211.48146  -279.831\n",
      "  -253.48712  -266.7186   -284.45255 ]\n",
      " [-102.00589   -96.00455   -92.67459   -96.504524 -100.44332  -133.47952\n",
      "  -121.30583  -127.14978  -135.24245 ]\n",
      " [-458.48273  -430.39395  -416.97272  -433.73376  -451.41028  -599.28033\n",
      "  -544.4565   -571.22864  -609.26154 ]]\n",
      "J tensor([ -78.8360, -300.7876, -194.5822,  -92.6746, -416.9727])\n",
      "P tensor([-103.2188, -392.3943, -253.4871, -121.3058, -544.4565])\n",
      "expected_state_action_values  tensor([[ -78.8209, -100.7654],\n",
      "        [-311.1285, -393.5745],\n",
      "        [-208.6581, -261.6725],\n",
      "        [ -94.1080, -119.8762],\n",
      "        [-428.4162, -543.1517]])\n",
      "reward -36.86387962053026\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.4549,  4.3198,  7.2281,  8.3536,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  7.3475,  9.7098,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395, 17.3845,  9.2981,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  5.0432,  7.1056,  5.8941,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  0.0000, 18.4464, 14.4922,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-26.3564],\n",
      "        [-31.9069],\n",
      "        [-73.1039],\n",
      "        [-36.9097],\n",
      "        [-45.1085]])\n",
      "action_batch tensor([[1, 7],\n",
      "        [2, 6],\n",
      "        [1, 7],\n",
      "        [0, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-203.7290, -198.9260, -192.1219, -193.3279, -188.5923, -266.6696,\n",
      "         -246.4517, -247.4988, -290.3520],\n",
      "        [-234.4653, -229.0587, -221.5584, -223.4095, -217.7599, -307.4276,\n",
      "         -283.8150, -285.3329, -334.7774],\n",
      "        [-544.0197, -530.8065, -513.7121, -517.2620, -504.4966, -712.3494,\n",
      "         -658.0693, -661.3297, -776.7773],\n",
      "        [-302.3377, -294.8254, -285.3092, -287.2222, -280.1283, -395.8732,\n",
      "         -365.7706, -367.3729, -431.1558],\n",
      "        [-363.7206, -355.1892, -343.2291, -345.2433, -336.9722, -475.9116,\n",
      "         -439.9746, -441.9589, -519.0252]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-198.9260, -247.4988],\n",
      "        [-221.5584, -283.8150],\n",
      "        [-530.8065, -661.3297],\n",
      "        [-302.3377, -365.7706],\n",
      "        [-343.2291, -439.9746]], grad_fn=<GatherBackward>)\n",
      "A [[-180.79135 -176.82167 -170.69356 -172.03317 -167.7363  -236.92033\n",
      "  -218.79881 -219.92957 -258.0254 ]\n",
      " [-217.1464  -212.02368 -205.05302 -206.59851 -201.42603 -284.5668\n",
      "  -262.8173  -264.08282 -309.74088]\n",
      " [-533.7809  -520.97955 -504.30078 -508.17828 -495.4918  -699.31604\n",
      "  -645.78625 -649.22534 -762.6402 ]\n",
      " [-281.4373  -274.54434 -265.64203 -267.49445 -260.8632  -368.5938\n",
      "  -340.52017 -342.06528 -401.4214 ]\n",
      " [-360.72235 -351.74197 -340.03534 -341.41437 -333.32437 -471.53928\n",
      "  -436.2035  -437.75317 -513.59705]]\n",
      "J tensor([-167.7363, -201.4260, -495.4918, -260.8632, -333.3244])\n",
      "P tensor([-218.7988, -262.8173, -645.7863, -340.5202, -436.2035])\n",
      "expected_state_action_values  tensor([[-177.3191, -223.2754],\n",
      "        [-213.1903, -268.4424],\n",
      "        [-519.0465, -654.3115],\n",
      "        [-271.6866, -343.3779],\n",
      "        [-345.1004, -437.6916]])\n",
      "reward -34.42259611357164\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  1.7539,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -7.8685],\n",
      "        [-32.6959],\n",
      "        [-40.3589],\n",
      "        [-42.7328],\n",
      "        [-11.8820]])\n",
      "action_batch tensor([[4, 6],\n",
      "        [2, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[ -77.9805,  -76.7908,  -79.0466,  -76.8763,  -75.0771, -106.2606,\n",
      "          -95.7848,  -95.9641, -115.6534],\n",
      "        [-258.6255, -253.8664, -263.0822, -255.7915, -249.5457, -352.6896,\n",
      "         -317.5423, -318.5984, -384.4177],\n",
      "        [-276.1889, -270.8013, -280.9644, -273.2354, -266.5483, -376.6963,\n",
      "         -339.1547, -340.2010, -410.4856],\n",
      "        [-327.8442, -321.3020, -333.3150, -323.9040, -316.1192, -446.8094,\n",
      "         -402.4793, -403.5725, -487.1583],\n",
      "        [ -96.8547,  -95.1301,  -98.2431,  -95.5026,  -93.2006, -132.0120,\n",
      "         -118.9415, -119.1589, -143.5286]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -75.0771,  -95.7848],\n",
      "        [-263.0822, -318.5984],\n",
      "        [-270.8013, -339.1547],\n",
      "        [-321.3020, -402.4793],\n",
      "        [ -95.1301, -118.9415]], grad_fn=<GatherBackward>)\n",
      "A [[ -77.42881   -76.20517   -78.50173   -76.342316  -74.52438  -105.54278\n",
      "   -95.110756  -95.28618  -114.74952 ]\n",
      " [-242.69925  -238.22318  -246.68921  -239.6      -233.8715   -330.70166\n",
      "  -297.92993  -298.769    -360.4568  ]\n",
      " [-289.5072   -284.1878   -294.76773  -286.99106  -279.915    -395.13315\n",
      "  -355.5929   -356.9496   -430.96295 ]\n",
      " [-318.07886  -311.9052   -323.56262  -314.726    -306.99673  -433.79596\n",
      "  -390.51175  -391.7976   -472.9728  ]\n",
      " [-109.19688  -107.1604   -110.79908  -107.750595 -105.13292  -148.8591\n",
      "  -134.0904   -134.35504  -161.91418 ]]\n",
      "J tensor([ -74.5244, -233.8715, -279.9150, -306.9967, -105.1329])\n",
      "P tensor([ -95.1108, -297.9299, -355.5929, -390.5117, -134.0904])\n",
      "expected_state_action_values  tensor([[ -74.9404,  -93.4682],\n",
      "        [-243.1802, -300.8328],\n",
      "        [-292.2824, -360.3925],\n",
      "        [-319.0299, -394.1934],\n",
      "        [-106.5016, -132.5633]])\n",
      "reward -36.30574799850308\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395,  4.8983,  9.9410,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.5108,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 6.6434,  6.4395,  5.6096, 12.2717,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  6.3968,  6.6019,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-30.4119],\n",
      "        [-34.1165],\n",
      "        [-32.9790],\n",
      "        [-25.6968],\n",
      "        [-25.0361]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 6],\n",
      "        [0, 7],\n",
      "        [3, 6],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-179.8776, -183.4714, -177.3170, -179.3786, -169.0829, -246.1489,\n",
      "         -215.2033, -216.1965, -268.4841],\n",
      "        [-219.8183, -223.6370, -216.2703, -218.2849, -205.9184, -300.2286,\n",
      "         -262.8100, -263.6331, -327.4424],\n",
      "        [-210.8064, -214.3494, -207.1641, -208.8622, -197.1570, -287.6098,\n",
      "         -251.9694, -252.5670, -313.7593],\n",
      "        [-139.3798, -142.1969, -137.2922, -138.9823, -131.0238, -190.7283,\n",
      "         -166.7686, -167.4979, -208.0726],\n",
      "        [-140.9111, -143.6944, -138.5956, -139.8722, -132.0010, -192.4527,\n",
      "         -168.5241, -169.0540, -209.8409]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-183.4714, -215.2033],\n",
      "        [-219.8183, -262.8100],\n",
      "        [-210.8064, -252.5670],\n",
      "        [-138.9823, -166.7686],\n",
      "        [-132.0010, -169.0540]], grad_fn=<GatherBackward>)\n",
      "A [[-186.56018 -190.07599 -183.71346 -185.64447 -175.05788 -255.0566\n",
      "  -223.12807 -223.99123 -278.1626 ]\n",
      " [-233.74565 -237.57985 -229.85155 -231.68425 -218.6706  -319.01215\n",
      "  -279.4523  -280.11707 -347.7742 ]\n",
      " [-219.81828 -223.63704 -216.27026 -218.2849  -205.91843 -300.2286\n",
      "  -262.81    -263.6331  -327.44244]\n",
      " [-150.89333 -153.88399 -148.60759 -150.21756 -141.63257 -206.37265\n",
      "  -180.5092  -181.2363  -224.95041]\n",
      " [-152.19672 -155.24059 -149.86131 -151.49332 -142.88301 -208.09499\n",
      "  -182.06462 -182.78534 -227.00934]]\n",
      "J tensor([-175.0579, -218.6706, -205.9184, -141.6326, -142.8830])\n",
      "P tensor([-223.1281, -279.4523, -262.8100, -180.5092, -182.0646])\n",
      "expected_state_action_values  tensor([[-187.9640, -231.2272],\n",
      "        [-230.9200, -285.6235],\n",
      "        [-218.3056, -269.5080],\n",
      "        [-153.1661, -188.1551],\n",
      "        [-153.6308, -188.8942]])\n",
      "reward -42.605209170021254\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  6.4395,  4.5453, 13.5254,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3717,  6.4395,  5.6897,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  6.9588,  7.0048,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3428,  6.4395,  5.9774, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-30.7901],\n",
      "        [-33.9160],\n",
      "        [-24.2843],\n",
      "        [-46.0041],\n",
      "        [-10.7610]])\n",
      "action_batch tensor([[4, 7],\n",
      "        [0, 7],\n",
      "        [4, 7],\n",
      "        [1, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-161.6471, -163.6316, -152.5512, -161.2233, -154.7836, -211.7796,\n",
      "         -190.5361, -191.2999, -230.8013],\n",
      "        [-247.9563, -250.6847, -234.2476, -247.6076, -237.6470, -324.9982,\n",
      "         -292.2900, -293.5384, -354.2319],\n",
      "        [-200.8537, -203.3762, -189.8734, -201.0182, -192.8153, -263.5279,\n",
      "         -236.8045, -238.0298, -287.2881],\n",
      "        [-357.7203, -361.2041, -337.4896, -356.0845, -341.9719, -468.1178,\n",
      "         -421.3817, -422.8362, -510.4981],\n",
      "        [ -96.1207,  -97.1133,  -90.4060,  -95.2189,  -91.5461, -125.6697,\n",
      "         -113.3204, -113.4343, -136.4835]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-154.7836, -191.2999],\n",
      "        [-247.9563, -293.5384],\n",
      "        [-192.8153, -238.0298],\n",
      "        [-361.2041, -422.8362],\n",
      "        [ -97.1133, -113.3204]], grad_fn=<GatherBackward>)\n",
      "A [[-166.45493  -168.86674  -157.6265   -167.29166  -160.31883  -218.81735\n",
      "  -196.37091  -197.65329  -238.53755 ]\n",
      " [-256.5383   -259.06274  -241.89162  -255.10759  -245.02637  -335.63058\n",
      "  -302.19797  -303.12888  -365.75357 ]\n",
      " [-180.54877  -182.4053   -170.2217   -179.64076  -172.49034  -236.33554\n",
      "  -212.7337   -213.39534  -257.32727 ]\n",
      " [-347.50598  -350.67682  -327.75812  -345.78174  -332.07114  -454.6935\n",
      "  -409.32993  -410.6395   -495.7318  ]\n",
      " [ -96.675735  -97.67236   -90.92731   -95.7637    -92.071175 -126.391365\n",
      "  -113.97306  -114.08681  -137.2706  ]]\n",
      "J tensor([-157.6265, -241.8916, -170.2217, -327.7581,  -90.9273])\n",
      "P tensor([-196.3709, -302.1980, -212.7337, -409.3299, -113.9731])\n",
      "expected_state_action_values  tensor([[-172.6539, -207.5239],\n",
      "        [-251.6185, -305.8942],\n",
      "        [-177.4838, -215.7446],\n",
      "        [-340.9864, -414.4010],\n",
      "        [ -92.5956, -113.3367]])\n",
      "reward -37.84399457614964\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  6.4395,  4.1460, 11.5169,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.8479, 12.3229, 14.8634,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  7.5863, 10.1950,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  6.4395,  3.2406, 12.5768,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 14.3632, 15.1425,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.9144,  5.0590,  7.1990,  9.5668,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-28.3822],\n",
      "        [-50.3005],\n",
      "        [-42.7328],\n",
      "        [ -7.8685],\n",
      "        [-31.9273]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [3, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-178.6556, -167.5818, -162.2710, -172.2481, -154.9270, -225.5414,\n",
      "         -202.7549, -197.8896, -245.6248],\n",
      "        [-368.7305, -344.6926, -334.0588, -353.0192, -317.9510, -463.9212,\n",
      "         -417.9955, -407.0637, -505.5591],\n",
      "        [-342.9983, -320.2280, -310.2833, -327.3725, -295.0533, -430.9876,\n",
      "         -388.7288, -378.1163, -469.5651],\n",
      "        [ -77.1934,  -72.3850,  -69.6259,  -73.4032,  -66.2779,  -96.9528,\n",
      "          -87.6099,  -85.0702, -105.2972],\n",
      "        [-207.0800, -193.8949, -187.6373, -198.5686, -178.7840, -260.7746,\n",
      "         -234.8208, -228.8050, -284.0688]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-167.5818, -202.7549],\n",
      "        [-368.7305, -407.0637],\n",
      "        [-320.2280, -388.7288],\n",
      "        [ -73.4032,  -85.0702],\n",
      "        [-187.6373, -234.8208]], grad_fn=<GatherBackward>)\n",
      "A [[-164.10643  -153.91399  -149.00136  -158.0346   -142.18478  -207.0978\n",
      "  -186.2591   -181.70442  -225.40364 ]\n",
      " [-351.51544  -328.5764   -318.50125  -336.83743  -303.27847  -442.4325\n",
      "  -398.47577  -388.1521   -482.1709  ]\n",
      " [-332.91196  -310.9755   -301.31232  -318.2107   -286.65118  -418.589\n",
      "  -377.29944  -367.2188   -456.06274 ]\n",
      " [ -76.69952   -71.87979   -69.1937    -72.9418    -65.831505  -96.36431\n",
      "   -87.05214   -84.52545  -104.53613 ]\n",
      " [-214.23737  -200.5838   -194.14107  -205.41208  -184.93385  -269.78894\n",
      "  -242.93274  -236.71002  -293.82388 ]]\n",
      "J tensor([-142.1848, -303.2785, -286.6512,  -65.8315, -184.9339])\n",
      "P tensor([-181.7044, -388.1521, -367.2188,  -84.5255, -236.7100])\n",
      "expected_state_action_values  tensor([[-156.3485, -191.9162],\n",
      "        [-323.2512, -399.6374],\n",
      "        [-300.7189, -373.2297],\n",
      "        [ -67.1169,  -83.9414],\n",
      "        [-198.3678, -244.9663]])\n",
      "reward -54.14953378821966\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.9144,  5.0590,  7.1990,  9.5668,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.1448, 10.2561,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 16.5628, 10.7409,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.3299],\n",
      "        [ -4.6880],\n",
      "        [-29.7391],\n",
      "        [-48.1451],\n",
      "        [ -6.7672]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [4, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-113.9460, -106.9973, -110.8092, -107.8861, -101.9083, -148.9520,\n",
      "         -130.8144, -126.9674, -161.9880],\n",
      "        [ -48.3758,  -45.7629,  -46.9195,  -45.8387,  -43.3318,  -63.2988,\n",
      "          -55.6043,  -53.9767,  -68.7942],\n",
      "        [-174.2348, -163.9234, -170.0500, -166.3086, -156.8262, -228.3887,\n",
      "         -200.0551, -194.7693, -249.0934],\n",
      "        [-376.5631, -352.8049, -366.7005, -357.0318, -337.0564, -492.1212,\n",
      "         -431.9106, -419.5734, -536.5746],\n",
      "        [ -47.4764,  -44.8675,  -46.0380,  -44.8104,  -42.4315,  -62.0458,\n",
      "          -54.6155,  -52.9185,  -67.2878]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-110.8092, -130.8144],\n",
      "        [ -43.3318,  -55.6043],\n",
      "        [-163.9234, -200.0551],\n",
      "        [-352.8049, -431.9106],\n",
      "        [ -44.8104,  -52.9185]], grad_fn=<GatherBackward>)\n",
      "A [[-129.29768  -121.67771  -126.09008  -123.16755  -116.21452  -169.40521\n",
      "  -148.50797  -144.45857  -184.50278 ]\n",
      " [ -56.565987  -53.42866   -54.86162   -53.493553  -50.585945  -73.934364\n",
      "   -64.98365   -63.055935  -80.393936]\n",
      " [-189.04562  -177.68678  -184.37657  -180.04733  -169.83336  -247.59291\n",
      "  -217.00418  -211.12527  -269.8849  ]\n",
      " [-374.83618  -350.93918  -364.73288  -354.7847   -335.04355  -489.52524\n",
      "  -429.849    -417.32266  -533.6298  ]\n",
      " [ -70.97765   -66.809586  -68.88024   -66.98392   -63.385403  -92.68526\n",
      "   -81.547295  -79.03687  -100.71631 ]]\n",
      "J tensor([-116.2145,  -50.5859, -169.8334, -335.0435,  -63.3854])\n",
      "P tensor([-144.4586,  -63.0559, -211.1253, -417.3227,  -79.0369])\n",
      "expected_state_action_values  tensor([[-123.9229, -149.3426],\n",
      "        [ -50.2153,  -61.4383],\n",
      "        [-182.5892, -219.7519],\n",
      "        [-349.6842, -423.7354],\n",
      "        [ -63.8141,  -77.9004]])\n",
      "5\n",
      "reward -52.210851612998326\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 14.3632, 15.1425,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 18.1602, 26.5328,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.7497, 20.6804, 11.0180,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -7.8685],\n",
      "        [-52.2115],\n",
      "        [-40.3589],\n",
      "        [-71.3122],\n",
      "        [-10.7009]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[ -70.8550,  -64.2314,  -71.1300,  -70.1900,  -67.4212,  -92.6141,\n",
      "          -79.2560,  -81.2151, -100.5983],\n",
      "        [-377.0582, -340.2123, -379.7422, -374.6757, -359.2399, -492.9425,\n",
      "         -420.9096, -432.3244, -537.1176],\n",
      "        [-261.4138, -235.8909, -263.4228, -260.3413, -249.4757, -342.1540,\n",
      "         -291.9443, -299.9792, -372.6298],\n",
      "        [-456.1340, -412.0511, -460.5352, -456.1166, -436.7392, -597.8458,\n",
      "         -509.4209, -524.3257, -652.1287],\n",
      "        [ -75.9986,  -68.8731,  -76.3133,  -75.4276,  -72.3552,  -99.4119,\n",
      "          -84.9479,  -87.1432, -108.0274]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -70.1900,  -81.2151],\n",
      "        [-379.7422, -432.3244],\n",
      "        [-235.8909, -291.9443],\n",
      "        [-412.0511, -509.4209],\n",
      "        [ -76.3133,  -84.9479]], grad_fn=<GatherBackward>)\n",
      "A [[ -70.29902   -63.684387  -70.58588   -69.64606   -66.869705  -91.91928\n",
      "   -78.63722   -80.57717   -99.719025]\n",
      " [-332.02942  -299.89792  -334.92773  -331.30975  -317.3733   -434.85968\n",
      "  -370.80655  -381.36273  -473.98605 ]\n",
      " [-273.2661   -246.89047  -275.61636  -272.72446  -261.2896   -357.92444\n",
      "  -305.23328  -313.89886  -390.18585 ]\n",
      " [-463.25317  -418.64505  -467.81537  -463.54166  -443.8429   -607.28906\n",
      "  -517.3934   -532.66504  -662.7591  ]\n",
      " [ -93.86656   -84.87817   -94.30143   -93.1713    -89.33903  -122.77876\n",
      "  -104.880104 -107.59647  -133.41003 ]]\n",
      "J tensor([ -63.6844, -299.8979, -246.8905, -418.6451,  -84.8782])\n",
      "P tensor([ -78.6372, -370.8065, -305.2333, -517.3934, -104.8801])\n",
      "expected_state_action_values  tensor([[ -65.1844,  -78.6420],\n",
      "        [-322.1196, -385.9374],\n",
      "        [-262.5603, -315.0688],\n",
      "        [-448.0928, -536.9662],\n",
      "        [ -87.0913, -105.0930]])\n",
      "1\n",
      "reward -58.42556499426876\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[19.9728,  6.4395, 23.7921, 22.3220,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3717,  6.4395,  5.6897,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3428,  6.4395,  5.9774, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-74.5264],\n",
      "        [-24.2843],\n",
      "        [-11.8820],\n",
      "        [-23.4019],\n",
      "        [-57.2807]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [4, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-502.8512, -471.4164, -490.7933, -478.8322, -480.4724, -658.4799,\n",
      "         -577.2756, -561.4276, -717.8268],\n",
      "        [-190.3380, -178.9031, -185.9252, -181.7378, -182.2289, -249.6579,\n",
      "         -218.6838, -212.8375, -271.7938],\n",
      "        [ -98.2473,  -92.1956,  -95.4630,  -92.8980,  -93.3548, -128.4210,\n",
      "         -112.8444, -109.4133, -139.4436],\n",
      "        [-157.8207, -148.2684, -154.1161, -150.7362, -151.1006, -207.0763,\n",
      "         -181.3518, -176.4668, -225.2773],\n",
      "        [-395.3705, -369.8244, -384.6350, -373.6776, -375.5917, -516.1797,\n",
      "         -453.5759, -440.0117, -562.1140]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-502.8512, -561.4276],\n",
      "        [-182.2289, -212.8375],\n",
      "        [ -92.1956, -112.8444],\n",
      "        [-154.1161, -181.3518],\n",
      "        [-384.6350, -440.0117]], grad_fn=<GatherBackward>)\n",
      "A [[-522.09564  -489.27173  -509.18835  -496.07928  -498.0352   -683.0756\n",
      "  -599.2408   -582.4276   -744.4367  ]\n",
      " [-171.56297  -160.88745  -167.1124   -162.77141  -163.42253  -224.4763\n",
      "  -196.99913  -191.30441  -244.06966 ]\n",
      " [-110.43204  -103.536125 -107.33612  -104.49491  -104.98216  -144.3717\n",
      "  -126.82373  -122.989586 -156.82808 ]\n",
      " [-161.6414   -151.62445  -157.53708  -153.63287  -154.17297  -211.6636\n",
      "  -185.64673  -180.36276  -230.13387 ]\n",
      " [-445.971    -417.13675  -434.19934  -422.1818   -424.1009   -582.6861\n",
      "  -511.68808  -496.6313   -634.3537  ]]\n",
      "J tensor([-489.2717, -160.8875, -103.5361, -151.6245, -417.1367])\n",
      "P tensor([-582.4276, -191.3044, -122.9896, -180.3628, -496.6313])\n",
      "expected_state_action_values  tensor([[-514.8709, -598.7112],\n",
      "        [-169.0830, -196.4583],\n",
      "        [-105.0645, -122.5726],\n",
      "        [-159.8639, -185.7284],\n",
      "        [-432.7037, -504.2488]])\n",
      "reward -55.07167439333817\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  4.8983,  9.9410,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.6434,  6.4395,  5.6096, 12.2717,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-30.9730],\n",
      "        [-25.3215],\n",
      "        [-45.2304],\n",
      "        [-47.4361],\n",
      "        [-30.4119]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-246.3593, -230.4868, -239.0334, -225.6645, -213.0587, -310.3820,\n",
      "         -279.4561, -272.2744, -337.9669],\n",
      "        [-215.7273, -201.8349, -209.3746, -197.7873, -186.6509, -271.9578,\n",
      "         -244.7484, -238.5049, -295.9005],\n",
      "        [-362.6839, -338.3591, -351.1159, -330.2672, -312.1510, -455.8238,\n",
      "         -411.0853, -399.7747, -496.0642],\n",
      "        [-424.9169, -396.2611, -411.1954, -386.3853, -365.3009, -533.7158,\n",
      "         -481.5347, -468.1130, -580.7803],\n",
      "        [-206.9504, -193.6416, -200.9392, -189.9620, -179.2091, -261.0280,\n",
      "         -234.8253, -228.8978, -283.9750]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-239.0334, -279.4561],\n",
      "        [-209.3746, -244.7484],\n",
      "        [-338.3591, -411.0853],\n",
      "        [-411.1954, -481.5347],\n",
      "        [-193.6416, -234.8253]], grad_fn=<GatherBackward>)\n",
      "A [[-217.25537 -203.22086 -210.75626 -199.0158  -187.88194 -273.7456\n",
      "  -246.46033 -240.0953  -297.9842 ]\n",
      " [-196.68398 -183.6669  -190.40053 -179.21837 -169.3292  -247.36803\n",
      "  -223.02734 -216.88794 -268.83737]\n",
      " [-366.33313 -341.71182 -354.71765 -333.77994 -315.40378 -460.53348\n",
      "  -415.23862 -403.86453 -501.13147]\n",
      " [-405.53592 -378.35266 -392.42932 -368.7587  -348.6962  -509.3292\n",
      "  -459.5812  -446.79364 -554.44666]\n",
      " [-214.2712  -200.2994  -207.85822 -196.29576 -185.25357 -270.0446\n",
      "  -243.07274 -236.7763  -293.7456 ]]\n",
      "J tensor([-187.8819, -169.3292, -315.4038, -348.6962, -185.2536])\n",
      "P tensor([-240.0953, -216.8879, -403.8645, -446.7936, -236.7763])\n",
      "expected_state_action_values  tensor([[-200.0668, -247.0588],\n",
      "        [-177.7178, -220.5206],\n",
      "        [-329.0938, -408.7085],\n",
      "        [-361.2627, -449.5504],\n",
      "        [-197.1401, -243.5106]])\n",
      "reward -60.087894809702945\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395,  4.8983,  9.9410,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.7128, 18.8469,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 6.6434,  6.4395,  5.6096, 12.2717,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-30.4119],\n",
      "        [-14.0164],\n",
      "        [-67.3986],\n",
      "        [-10.7610],\n",
      "        [-23.4019]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-195.5019, -176.6707, -183.6632, -179.6247, -169.4143, -246.6296,\n",
      "         -215.9707, -216.2754, -268.4683],\n",
      "        [-155.8049, -140.3581, -145.7421, -141.6435, -133.8856, -195.7945,\n",
      "         -172.0343, -171.6182, -212.5930],\n",
      "        [-477.7545, -430.3595, -447.8066, -436.0276, -411.9533, -600.7454,\n",
      "         -527.2495, -526.9363, -654.6346],\n",
      "        [-101.6118,  -91.7122,  -94.9403,  -92.3921,  -87.3379, -127.7201,\n",
      "         -112.2257, -111.9388, -138.6791],\n",
      "        [-164.1538, -148.2925, -154.1189, -150.7316, -142.1615, -207.0719,\n",
      "         -181.3509, -181.5346, -225.2713]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-176.6707, -215.9707],\n",
      "        [-145.7421, -171.6182],\n",
      "        [-430.3595, -527.2495],\n",
      "        [ -91.7122, -112.2257],\n",
      "        [-154.1189, -181.3509]], grad_fn=<GatherBackward>)\n",
      "A [[-202.87616  -183.1469   -190.40721  -186.01184  -175.50803  -255.71352\n",
      "  -224.06288  -224.21317  -278.32043 ]\n",
      " [-123.03476  -111.02831  -115.10102  -111.99758  -105.87515  -154.69392\n",
      "  -135.90645  -135.61932  -168.0367  ]\n",
      " [-502.2877   -452.46213  -471.0101   -458.87338  -433.39368  -631.88245\n",
      "  -554.36035  -554.2026   -688.46185 ]\n",
      " [-102.15594   -92.201614  -95.44798   -92.88134   -87.8018   -128.4\n",
      "  -112.825356 -112.53574  -139.4197  ]\n",
      " [-168.11307  -151.64795  -157.53925  -153.62741  -145.03618  -211.65797\n",
      "  -185.64493  -185.5413   -230.12643 ]]\n",
      "J tensor([-175.5080, -105.8752, -433.3937,  -87.8018, -145.0362])\n",
      "P tensor([-224.0629, -135.6193, -554.2026, -112.5357, -185.5413])\n",
      "expected_state_action_values  tensor([[-188.3691, -232.0685],\n",
      "        [-109.3040, -136.0737],\n",
      "        [-457.4529, -566.1810],\n",
      "        [ -89.7826, -112.0432],\n",
      "        [-153.9345, -190.3891]])\n",
      "reward -52.16329517854855\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3717,  6.4395,  5.6897,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[18.5462,  3.2197, 18.7128, 14.9720,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3428,  6.4395,  5.9774, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-60.7464],\n",
      "        [-50.3626],\n",
      "        [-24.2843],\n",
      "        [-33.9160],\n",
      "        [ -3.7268]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [4, 7],\n",
      "        [0, 7],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-473.2005, -441.4554, -428.7491, -431.5765, -407.8230, -595.0135,\n",
      "         -536.3809, -507.2675, -647.9222],\n",
      "        [-412.3017, -384.3484, -373.2397, -375.4623, -354.8598, -518.1451,\n",
      "         -467.2614, -441.6442, -564.0432],\n",
      "        [-208.6169, -195.2145, -189.3751, -191.3255, -180.5418, -263.0315,\n",
      "         -236.6810, -224.2167, -286.1793],\n",
      "        [-256.3599, -239.5250, -232.5714, -234.5842, -221.4942, -322.9134,\n",
      "         -290.8008, -275.2458, -351.2600],\n",
      "        [ -46.2485,  -43.5399,  -41.7166,  -42.0038,  -39.8031,  -58.1848,\n",
      "          -52.6167,  -49.5985,  -62.9549]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-441.4554, -536.3809],\n",
      "        [-384.3484, -467.2614],\n",
      "        [-180.5418, -224.2167],\n",
      "        [-256.3599, -275.2458],\n",
      "        [ -42.0038,  -49.5985]], grad_fn=<GatherBackward>)\n",
      "A [[-500.42593  -466.82043  -453.43466  -456.44916  -431.3142   -629.2607\n",
      "  -567.2327   -536.4604   -685.25214 ]\n",
      " [-431.29913  -402.20706  -390.58063  -393.05154  -371.4405   -542.1651\n",
      "  -488.82104  -462.15594  -590.2981  ]\n",
      " [-187.09625  -174.71388  -169.40169  -170.5719   -161.13385  -235.37079\n",
      "  -212.16034  -200.57216  -255.77078 ]\n",
      " [-265.1963   -247.5292   -240.14299  -241.64021  -228.33047  -333.45273\n",
      "  -300.63727  -284.2223   -362.6623  ]\n",
      " [ -47.60977   -44.807823  -42.94467   -43.22774   -40.963657  -59.886135\n",
      "   -54.157257  -51.050316  -64.80689 ]]\n",
      "J tensor([-431.3142, -371.4405, -161.1339, -228.3305,  -40.9637])\n",
      "P tensor([-536.4604, -462.1559, -200.5722, -284.2223,  -51.0503])\n",
      "expected_state_action_values  tensor([[-448.9292, -543.5607],\n",
      "        [-384.6591, -466.3030],\n",
      "        [-169.3047, -204.7992],\n",
      "        [-239.4135, -289.7161],\n",
      "        [ -40.5941,  -49.6721]])\n",
      "reward -52.865636465119394\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.1448, 10.2561,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  4.8983,  9.9410,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-42.7328],\n",
      "        [ -4.6880],\n",
      "        [-27.3272],\n",
      "        [-46.9426],\n",
      "        [-47.4361]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [3, 7],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-347.8436, -350.4758, -327.7460, -313.1950, -291.8621, -454.3516,\n",
      "         -420.9521, -399.0322, -494.9571],\n",
      "        [ -46.3967,  -47.1514,  -43.5523,  -41.6564,  -38.9140,  -60.6499,\n",
      "          -56.3209,  -53.2544,  -65.7057],\n",
      "        [-208.6137, -210.7507, -196.9771, -189.0083, -175.8467, -273.2627,\n",
      "         -252.6695, -239.9521, -297.5689],\n",
      "        [-382.6617, -385.8536, -360.9863, -345.6531, -321.8174, -500.4719,\n",
      "         -463.1740, -439.5192, -545.3859],\n",
      "        [-417.3632, -420.5582, -393.1234, -375.2798, -349.7777, -544.8422,\n",
      "         -504.9210, -478.5774, -593.5417]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-350.4758, -420.9521],\n",
      "        [ -41.6564,  -53.2544],\n",
      "        [-196.9771, -252.6695],\n",
      "        [-382.6617, -439.5192],\n",
      "        [-393.1234, -504.9210]], grad_fn=<GatherBackward>)\n",
      "A [[-338.38412  -341.1262   -318.995    -305.13168  -284.19543  -442.27905\n",
      "  -409.52173  -388.4107   -481.81396 ]\n",
      " [ -55.412277  -56.22269   -52.016983  -49.671482  -46.40669   -72.36116\n",
      "   -67.211914  -63.55117   -78.468346]\n",
      " [-195.13539  -197.34145  -184.27258  -176.88016  -164.60606  -255.61626\n",
      "  -236.36844  -224.52762  -278.56378 ]\n",
      " [-368.44513  -371.7168   -347.6058   -332.90964  -309.994    -481.88974\n",
      "  -445.98734  -423.27335  -525.37006 ]\n",
      " [-399.06818  -402.2876   -375.8797   -358.8294   -334.50394  -520.91595\n",
      "  -482.79498  -457.63025  -567.6843  ]]\n",
      "J tensor([-284.1954,  -46.4067, -164.6061, -309.9940, -334.5039])\n",
      "P tensor([-388.4107,  -63.5512, -224.5276, -423.2733, -457.6302])\n",
      "expected_state_action_values  tensor([[-298.5087, -392.3024],\n",
      "        [ -46.4540,  -61.8841],\n",
      "        [-175.4726, -229.4020],\n",
      "        [-325.9372, -427.8886],\n",
      "        [-348.4896, -459.3033]])\n",
      "reward -45.19118453294075\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 18.1602, 26.5328,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3717,  6.4395,  5.6897,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[18.5462,  3.2197, 18.7128, 14.9720,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.3428,  6.4395,  5.9774, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-60.7464],\n",
      "        [-71.3122],\n",
      "        [-24.2843],\n",
      "        [-10.7009],\n",
      "        [-24.1008]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [4, 7],\n",
      "        [4, 8],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-428.1925, -434.9368, -405.9696, -422.2884, -374.8931, -582.8448,\n",
      "         -525.9347, -497.7586, -635.8093],\n",
      "        [-472.5627, -480.4597, -448.7560, -467.7167, -414.9095, -644.2305,\n",
      "         -580.6822, -550.1833, -702.9241],\n",
      "        [-185.1241, -188.6809, -175.8882, -183.6572, -162.8263, -252.7431,\n",
      "         -227.6272, -215.8228, -275.4563],\n",
      "        [ -70.5423,  -71.8720,  -66.6159,  -69.1765,  -61.5153,  -95.9325,\n",
      "          -86.7631,  -81.8861, -104.1161],\n",
      "        [-153.5677, -156.5353, -145.8526, -152.2958, -135.0169, -209.6669,\n",
      "         -188.8419, -179.0104, -228.3604]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-434.9368, -525.9347],\n",
      "        [-480.4597, -580.6822],\n",
      "        [-162.8263, -215.8228],\n",
      "        [ -61.5153, -104.1161],\n",
      "        [-145.8526, -188.8419]], grad_fn=<GatherBackward>)\n",
      "A [[-452.9025   -460.00443  -429.41583  -446.7007   -396.55493  -616.49445\n",
      "  -556.27936  -526.49243  -672.55493 ]\n",
      " [-479.55408  -487.74536  -455.48056  -474.9377   -421.32382  -653.8786\n",
      "  -589.3072   -558.4802   -713.778   ]\n",
      " [-166.49834  -169.28262  -157.73398  -164.13489  -145.65912  -226.74127\n",
      "  -204.57286  -193.5496   -246.80298 ]\n",
      " [ -88.750046  -90.241234  -83.85869   -87.07731   -77.38402  -120.70162\n",
      "  -109.10917  -102.99955  -131.03477 ]\n",
      " [-150.88812  -153.94095  -143.34322  -149.79718  -132.82742  -206.05307\n",
      "  -185.56905  -175.97493  -224.66624 ]]\n",
      "J tensor([-396.5549, -421.3238, -145.6591,  -77.3840, -132.8274])\n",
      "P tensor([-526.4924, -558.4802, -193.5496, -102.9995, -175.9749])\n",
      "expected_state_action_values  tensor([[-417.6458, -534.5896],\n",
      "        [-450.5037, -573.9445],\n",
      "        [-155.3775, -198.4789],\n",
      "        [ -80.3465, -103.4005],\n",
      "        [-143.6455, -182.4782]])\n",
      "reward -44.46836553396387\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-32.2255],\n",
      "        [-24.1008],\n",
      "        [-19.3299],\n",
      "        [-57.2807],\n",
      "        [-56.4395]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [3, 6],\n",
      "        [2, 7],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-142.9190, -140.9259, -130.9187, -142.0146, -117.6729, -195.1989,\n",
      "         -171.1723, -161.9548, -198.8642],\n",
      "        [-145.1617, -143.0011, -132.9394, -144.0824, -119.3534, -198.2179,\n",
      "         -173.8383, -164.4093, -201.6928],\n",
      "        [-110.8456, -108.9192, -101.0854, -109.1691,  -90.5550, -150.8808,\n",
      "         -132.6474, -125.0934, -153.3522],\n",
      "        [-371.8365, -364.3271, -339.1973, -365.3147, -303.1434, -505.2079,\n",
      "         -444.4901, -419.0540, -514.6319],\n",
      "        [-377.5741, -369.9651, -344.4147, -370.5428, -307.4769, -512.8470,\n",
      "         -451.3205, -425.4010, -521.9884]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-130.9187, -171.1723],\n",
      "        [-132.9394, -173.8383],\n",
      "        [-109.1691, -132.6474],\n",
      "        [-339.1973, -419.0540],\n",
      "        [-344.4147, -425.4010]], grad_fn=<GatherBackward>)\n",
      "A [[-180.47568 -177.7255  -165.32101 -179.13889 -148.40433 -246.38795\n",
      "  -216.09111 -204.39444 -250.8946 ]\n",
      " [-142.919   -140.92586 -130.91872 -142.01456 -117.67289 -195.1989\n",
      "  -171.17226 -161.95479 -198.8642 ]\n",
      " [-126.19735 -124.29254 -115.4215  -125.03656 -103.64012 -172.17561\n",
      "  -151.10773 -142.81114 -175.25615]\n",
      " [-417.30472 -408.87885 -380.98877 -410.65875 -340.59097 -567.43896\n",
      "  -498.9248  -470.60147 -577.8261 ]\n",
      " [-378.70947 -371.59744 -345.939   -372.94925 -309.36084 -515.0882\n",
      "  -452.85727 -427.3412  -524.7117 ]]\n",
      "J tensor([-148.4043, -117.6729, -103.6401, -340.5910, -309.3608])\n",
      "P tensor([-204.3944, -161.9548, -142.8111, -470.6015, -427.3412])\n",
      "expected_state_action_values  tensor([[-165.7894, -216.1805],\n",
      "        [-130.0064, -169.8601],\n",
      "        [-112.6060, -147.8599],\n",
      "        [-363.8125, -480.8220],\n",
      "        [-334.8642, -441.0465]])\n",
      "reward -50.04153502594643\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3717,  6.4395,  5.6897,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.2107,  6.4395,  6.5373,  6.9140,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3428,  6.4395,  5.9774, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.5108,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-26.2099],\n",
      "        [-34.1165],\n",
      "        [-24.2843],\n",
      "        [-28.1014],\n",
      "        [-35.7833]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 6],\n",
      "        [4, 7],\n",
      "        [2, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-156.5271, -153.7936, -141.1816, -161.7955, -128.1260, -213.3351,\n",
      "         -192.3595, -182.0791, -216.7891],\n",
      "        [-223.2843, -219.2732, -201.5483, -231.1024, -182.9873, -304.3990,\n",
      "         -274.3753, -259.8302, -309.6880],\n",
      "        [-184.9946, -182.1505, -167.2478, -192.0748, -152.0499, -252.5658,\n",
      "         -227.4565, -215.6642, -257.0034],\n",
      "        [-155.3400, -152.9418, -140.3236, -161.1378, -127.6050, -211.9884,\n",
      "         -190.9917, -181.0101, -215.7322],\n",
      "        [-202.6002, -199.6954, -183.1813, -210.2540, -166.5307, -276.4684,\n",
      "         -249.0855, -236.2010, -281.6198]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-153.7936, -192.3595],\n",
      "        [-223.2843, -274.3753],\n",
      "        [-152.0499, -215.6642],\n",
      "        [-140.3236, -181.0101],\n",
      "        [-183.1813, -249.0855]], grad_fn=<GatherBackward>)\n",
      "A [[-153.443    -151.10034  -138.67192  -159.25842  -126.068115 -209.49615\n",
      "  -188.67892  -178.85983  -213.03279 ]\n",
      " [-236.90994  -232.43365  -213.74083  -244.77242  -193.87923  -322.7407\n",
      "  -291.10678  -275.4747   -328.18866 ]\n",
      " [-166.3739   -163.41713  -149.97818  -171.67444  -135.99014  -226.57089\n",
      "  -204.40947  -193.3984   -230.24406 ]\n",
      " [-174.37392  -171.3621   -157.32303  -180.23438  -142.73949  -237.62762\n",
      "  -214.28255  -202.8516   -241.54756 ]\n",
      " [-220.25961  -216.80148  -199.0351   -228.38742  -180.88383  -300.4517\n",
      "  -270.74152  -256.61942  -306.00842 ]]\n",
      "J tensor([-126.0681, -193.8792, -135.9901, -142.7395, -180.8838])\n",
      "P tensor([-178.8598, -275.4747, -193.3984, -202.8516, -256.6194])\n",
      "expected_state_action_values  tensor([[-139.6712, -187.1837],\n",
      "        [-208.6078, -282.0437],\n",
      "        [-146.6754, -198.3428],\n",
      "        [-156.5670, -210.6678],\n",
      "        [-198.5787, -266.7408]])\n",
      "reward -47.06794524248203\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  6.4395, 23.7921, 22.3220,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  5.0623, 10.1720,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.1460, 11.5169,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.3299],\n",
      "        [-50.3626],\n",
      "        [-47.9592],\n",
      "        [-74.5264],\n",
      "        [-24.7339]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [1, 6],\n",
      "        [0, 6],\n",
      "        [0, 7],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-111.8335, -110.4451, -109.0633, -120.0314,  -88.6524, -158.8113,\n",
      "         -147.3042, -131.6929, -161.0278],\n",
      "        [-362.3577, -357.1261, -353.8260, -389.0255, -287.3162, -514.2450,\n",
      "         -477.0137, -426.5883, -522.6938],\n",
      "        [-319.2326, -314.8162, -311.8388, -342.9030, -253.2095, -453.2097,\n",
      "         -420.3123, -375.9666, -460.4451],\n",
      "        [-483.0449, -476.8676, -472.7775, -521.0258, -384.6265, -686.8974,\n",
      "         -636.3281, -569.9243, -698.7120],\n",
      "        [-147.8547, -146.4850, -144.9820, -160.1759, -118.2029, -210.8276,\n",
      "         -195.0818, -174.9153, -213.9614]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-120.0314, -147.3042],\n",
      "        [-357.1261, -477.0137],\n",
      "        [-319.2326, -420.3123],\n",
      "        [-483.0449, -569.9243],\n",
      "        [-147.8547, -174.9153]], grad_fn=<GatherBackward>)\n",
      "A [[-128.92657  -127.610954 -126.0931   -139.15364  -102.73708  -183.50053\n",
      "  -169.95218  -152.2279   -186.32962 ]\n",
      " [-379.3881   -374.07034  -370.6079   -407.61594  -301.03543  -538.57996\n",
      "  -499.49057  -446.813    -547.5332  ]\n",
      " [-323.14215  -318.79648  -315.74924  -347.0708   -256.3363   -458.75473\n",
      "  -425.53204  -380.62903  -466.01437 ]\n",
      " [-501.35608  -494.73016  -490.31146  -539.65625  -398.46356  -712.2876\n",
      "  -660.2447   -591.01624  -724.3393  ]\n",
      " [-149.66621  -148.31679  -146.61626  -162.00604  -119.587776 -213.25851\n",
      "  -197.36522  -176.96577  -216.74527 ]]\n",
      "J tensor([-102.7371, -301.0354, -256.3363, -398.4636, -119.5878])\n",
      "P tensor([-152.2279, -446.8130, -380.6290, -591.0162, -176.9658])\n",
      "expected_state_action_values  tensor([[-111.7932, -156.3350],\n",
      "        [-321.2945, -452.4943],\n",
      "        [-278.6618, -390.5253],\n",
      "        [-433.1436, -606.4410],\n",
      "        [-132.3629, -184.0031]])\n",
      "1\n",
      "reward -49.34393215136876\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  6.9588,  7.0048,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  5.8472,  5.9286,  9.1005,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 18.7790, 15.3735,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-34.1165],\n",
      "        [-23.4633],\n",
      "        [-19.3299],\n",
      "        [-51.1810],\n",
      "        [-57.2807]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [0, 7],\n",
      "        [3, 6],\n",
      "        [0, 7],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-196.8219, -195.7715, -201.2457, -212.1189, -163.8497, -292.6830,\n",
      "         -264.2537, -249.8636, -297.4255],\n",
      "        [-147.9001, -147.4414, -151.3488, -159.6520, -123.3160, -220.1356,\n",
      "         -198.6801, -187.9757, -223.6137],\n",
      "        [-102.1952, -101.7001, -104.1765, -109.6115,  -84.7406, -151.7113,\n",
      "         -137.1717, -129.4969, -153.9162],\n",
      "        [-325.2724, -323.5283, -332.3989, -349.5640, -270.1975, -482.9267,\n",
      "         -436.4370, -412.4991, -491.1998],\n",
      "        [-345.8669, -343.0857, -352.5903, -370.0066, -286.1388, -512.4243,\n",
      "         -463.6738, -437.5499, -521.0461]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-196.8219, -264.2537],\n",
      "        [-147.9001, -187.9757],\n",
      "        [-109.6115, -137.1717],\n",
      "        [-325.2724, -412.4991],\n",
      "        [-352.5903, -437.5499]], grad_fn=<GatherBackward>)\n",
      "A [[-209.06592  -207.72653  -213.63733  -224.87471  -173.7644   -310.6354\n",
      "  -280.65952  -265.17654  -315.51236 ]\n",
      " [-132.90288  -132.5454   -135.85234  -143.15659  -110.6474   -197.60559\n",
      "  -178.48524  -168.77917  -200.79378 ]\n",
      " [-117.281265 -116.99956  -119.92934  -126.56663   -97.79749  -174.54427\n",
      "  -157.56177  -149.0495   -177.34445 ]\n",
      " [-339.4138   -337.073    -346.29007  -363.48212  -281.0523   -503.13882\n",
      "  -455.1358   -429.7016   -511.52628 ]\n",
      " [-387.36105  -384.2429   -395.2365   -415.08167  -320.84027  -574.3874\n",
      "  -519.4348   -490.38495  -583.83997 ]]\n",
      "J tensor([-173.7644, -110.6474,  -97.7975, -281.0523, -320.8403])\n",
      "P tensor([-265.1765, -168.7792, -149.0495, -429.7016, -490.3849])\n",
      "expected_state_action_values  tensor([[-190.5044, -272.7754],\n",
      "        [-123.0459, -175.3645],\n",
      "        [-107.3476, -153.4744],\n",
      "        [-304.1281, -437.9124],\n",
      "        [-346.0369, -498.6271]])\n",
      "reward -47.81611994771623\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.3899,  3.0863,  1.2602,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.3299],\n",
      "        [-56.4395],\n",
      "        [-47.6074],\n",
      "        [-32.6959],\n",
      "        [-48.4121]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 7],\n",
      "        [2, 6],\n",
      "        [2, 7],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[ -95.0697,  -99.6564,  -98.3901, -102.2706,  -83.4011, -148.2355,\n",
      "         -137.8488, -130.4189, -150.5239],\n",
      "        [-329.5921, -344.2249, -340.9738, -353.4285, -288.2142, -512.7398,\n",
      "         -477.0751, -451.2227, -521.7060],\n",
      "        [-256.4726, -268.3506, -265.8996, -276.5981, -225.4012, -400.0145,\n",
      "         -371.6503, -352.0102, -407.2733],\n",
      "        [-207.9096, -217.8059, -215.5581, -224.1265, -182.6723, -324.2547,\n",
      "         -301.3109, -285.4024, -330.0165],\n",
      "        [-285.3370, -298.4020, -295.6205, -306.9477, -250.2665, -444.5387,\n",
      "         -413.3496, -391.2498, -452.4531]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -98.3901, -137.8488],\n",
      "        [-340.9738, -451.2227],\n",
      "        [-265.8996, -371.6503],\n",
      "        [-215.5581, -285.4024],\n",
      "        [-295.6205, -413.3496]], grad_fn=<GatherBackward>)\n",
      "A [[-108.58756  -114.13056  -112.76012  -117.60724   -95.838585 -169.78662\n",
      "  -157.62906  -149.44333  -172.68005 ]\n",
      " [-331.18353  -346.44745  -343.1921   -356.49033  -290.616    -516.0385\n",
      "  -479.7177   -454.21417  -525.5034  ]\n",
      " [-275.054    -287.94504  -285.26486  -296.74368  -241.82487  -429.04953\n",
      "  -398.60526  -377.6197   -436.91913 ]\n",
      " [-196.39159  -205.7134   -203.41579  -211.25537  -172.26218  -305.98138\n",
      "  -284.50925  -269.35098  -311.43607 ]\n",
      " [-279.1416   -292.4525   -289.6477   -301.33548  -245.64372  -435.50922\n",
      "  -404.64316  -383.40768  -443.68823 ]]\n",
      "J tensor([ -95.8386, -290.6160, -241.8249, -172.2622, -245.6437])\n",
      "P tensor([-149.4433, -454.2142, -377.6197, -269.3510, -383.4077])\n",
      "expected_state_action_values  tensor([[-105.5846, -153.8289],\n",
      "        [-317.9939, -465.2322],\n",
      "        [-265.2498, -387.4651],\n",
      "        [-187.7318, -275.1117],\n",
      "        [-269.4914, -393.4789]])\n",
      "reward -43.69548310122947\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  2.6880, 13.7230,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.0140, 38.1314,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.6263,  6.4395,  3.2406, 11.1610,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.8479, 12.3229, 14.8634,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  7.5781, 35.2749,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-30.5570],\n",
      "        [-18.9634],\n",
      "        [-45.6004],\n",
      "        [-49.9555],\n",
      "        [-55.0717]])\n",
      "action_batch tensor([[4, 6],\n",
      "        [3, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-116.3802, -122.5732, -116.6952, -126.9420, -103.3332, -182.5329,\n",
      "         -173.4212, -165.0889, -185.7745],\n",
      "        [-104.1585, -109.4954, -104.0724, -112.9897,  -92.0496, -162.9217,\n",
      "         -154.9864, -147.3218, -165.8849],\n",
      "        [-310.3215, -324.6367, -309.5316, -334.6418, -272.8071, -483.7528,\n",
      "         -460.8794, -437.4984, -493.2303],\n",
      "        [-256.6988, -268.9391, -256.5351, -277.9301, -226.4097, -400.9711,\n",
      "         -381.6037, -362.6105, -408.6594],\n",
      "        [-196.0757, -207.5088, -197.9401, -216.6871, -176.1877, -309.2726,\n",
      "         -292.9268, -279.8959, -315.9554]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-103.3332, -173.4212],\n",
      "        [-112.9897, -154.9864],\n",
      "        [-324.6367, -460.8794],\n",
      "        [-268.9391, -381.6037],\n",
      "        [-176.1877, -279.8959]], grad_fn=<GatherBackward>)\n",
      "A [[-125.530876 -132.28392  -126.03717  -137.33716  -111.74959  -197.13026\n",
      "  -187.14813  -178.2969   -200.76315 ]\n",
      " [-100.338066 -105.37323  -100.1824   -108.66556   -88.51895  -156.8642\n",
      "  -149.26411  -141.80518  -159.55096 ]\n",
      " [-311.4107   -325.2882   -310.05783  -334.48178  -272.8082   -484.59695\n",
      "  -462.1288   -438.21274  -493.8393  ]\n",
      " [-275.4463   -288.38428  -275.10883  -297.90677  -242.71758  -430.02246\n",
      "  -409.35425  -388.86084  -438.30963 ]\n",
      " [-172.18098  -182.97127  -174.47932  -191.74478  -155.90735  -272.82773\n",
      "  -257.9323   -246.81972  -279.29196 ]]\n",
      "J tensor([-111.7496,  -88.5190, -272.8082, -242.7176, -155.9073])\n",
      "P tensor([-178.2969, -141.8052, -438.2127, -388.8608, -246.8197])\n",
      "expected_state_action_values  tensor([[-131.1316, -191.0242],\n",
      "        [ -98.6304, -146.5880],\n",
      "        [-291.1278, -439.9919],\n",
      "        [-268.4014, -399.9303],\n",
      "        [-195.3883, -277.2094]])\n",
      "reward -45.96666681835886\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395,  5.1372,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 6.2107,  6.4395,  6.5373,  6.9140,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.7128, 18.8469,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-29.3088],\n",
      "        [-71.0280],\n",
      "        [ -6.7672],\n",
      "        [-25.4330],\n",
      "        [-50.3626]])\n",
      "action_batch tensor([[4, 7],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [2, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-128.3127, -129.9335, -128.8434, -132.3234, -123.5517, -203.1071,\n",
      "         -188.3676, -176.3690, -207.9227],\n",
      "        [-354.4142, -358.1028, -355.9503, -365.3114, -341.0129, -560.2430,\n",
      "         -519.7305, -486.8646, -574.7314],\n",
      "        [ -41.0173,  -41.8442,  -40.9029,  -42.0700,  -39.2773,  -64.6589,\n",
      "          -60.1110,  -56.3214,  -66.0238],\n",
      "        [-111.3791, -113.0850, -112.1211, -115.4682, -107.8458, -176.9150,\n",
      "         -163.8450, -153.4658, -181.2386],\n",
      "        [-280.8176, -283.3535, -281.3705, -288.4296, -269.1177, -442.7956,\n",
      "         -411.1556, -385.1248, -453.9604]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-123.5517, -176.3690],\n",
      "        [-355.9503, -519.7305],\n",
      "        [ -41.8442,  -60.1110],\n",
      "        [-112.1211, -153.4658],\n",
      "        [-283.3535, -411.1556]], grad_fn=<GatherBackward>)\n",
      "A [[-128.49368  -130.22496  -129.2346   -132.97725  -124.15961  -203.82114\n",
      "  -188.85161  -176.86984  -208.77998 ]\n",
      " [-358.1985   -362.2376   -360.0559   -369.74515  -345.21753  -566.7633\n",
      "  -525.5903   -492.40878  -581.6425  ]\n",
      " [ -59.188923  -60.093987  -59.061817  -60.629326  -56.58088   -93.19477\n",
      "   -86.645035  -81.169075  -95.273285]\n",
      " [-110.86119  -112.530556 -111.65209  -115.1027   -107.47823  -176.2313\n",
      "  -163.141    -152.82341  -180.55415 ]\n",
      " [-292.86057  -295.68625  -293.6242   -301.1032   -280.98703  -462.09195\n",
      "  -428.9663   -401.84088  -473.89594 ]]\n",
      "J tensor([-124.1596, -345.2175,  -56.5809, -107.4782, -280.9870])\n",
      "P tensor([-176.8698, -492.4088,  -81.1691, -152.8234, -401.8409])\n",
      "expected_state_action_values  tensor([[-141.0525, -188.4917],\n",
      "        [-381.7238, -514.1959],\n",
      "        [ -57.6900,  -79.8194],\n",
      "        [-122.1635, -162.9741],\n",
      "        [-303.2509, -412.0194]])\n",
      "5\n",
      "reward -43.74061144979981\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  5.8472,  5.9286,  9.1005,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  6.9588,  7.0048,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 6.0554,  5.7395,  4.8381, 41.4549,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  9.3189, 16.0605,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  6.4395,  5.1372,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  5.8472,  5.9286,  9.1005,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.0140, 38.1314,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 6.8490,  6.4395,  8.2194, 10.9147,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-28.5828],\n",
      "        [-23.4633],\n",
      "        [-60.0879],\n",
      "        [-36.3057],\n",
      "        [-47.6074]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [0, 7],\n",
      "        [4, 6],\n",
      "        [4, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-138.1241, -145.0381, -143.6935, -142.8441, -140.2314, -216.2454,\n",
      "         -195.6471, -195.5441, -220.0571],\n",
      "        [-131.6163, -138.3419, -136.9608, -136.1654, -133.6868, -206.0905,\n",
      "         -186.4576, -186.3989, -209.7598],\n",
      "        [-173.1715, -184.3394, -182.8456, -183.9787, -180.4301, -274.7256,\n",
      "         -246.7655, -248.5654, -281.2009],\n",
      "        [-170.9383, -179.5269, -177.8163, -176.4754, -173.3720, -267.3745,\n",
      "         -242.0593, -241.8842, -272.2328],\n",
      "        [-245.8791, -257.4737, -255.2634, -252.9220, -248.5786, -383.8494,\n",
      "         -347.7533, -347.1619, -391.0464]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-138.1241, -195.5441],\n",
      "        [-131.6163, -186.3989],\n",
      "        [-180.4301, -246.7655],\n",
      "        [-173.3720, -242.0593],\n",
      "        [-255.2634, -347.7533]], grad_fn=<GatherBackward>)\n",
      "A [[-131.61633  -138.34186  -136.96083  -136.16544  -133.68677  -206.09055\n",
      "  -186.45758  -186.39888  -209.75977 ]\n",
      " [-118.75317  -124.86239  -123.41346  -122.54489  -120.415184 -185.71928\n",
      "  -168.16698  -168.0121   -189.09303 ]\n",
      " [-173.79176  -185.20811  -183.92882  -185.24904  -181.73793  -276.37048\n",
      "  -247.93346  -249.91159  -282.81454 ]\n",
      " [-171.46109  -180.5888   -178.76059  -177.90215  -174.67085  -268.7168\n",
      "  -243.01968  -243.21994  -274.02548 ]\n",
      " [-263.3771   -275.95062  -273.53333  -271.02655  -266.37305  -411.22333\n",
      "  -372.52936  -371.98016  -419.015   ]]\n",
      "J tensor([-131.6163, -118.7532, -173.7918, -171.4611, -263.3771])\n",
      "P tensor([-186.3989, -168.0121, -247.9335, -243.0197, -371.9802])\n",
      "expected_state_action_values  tensor([[-147.0375, -196.3418],\n",
      "        [-130.3411, -174.6742],\n",
      "        [-216.5005, -283.2280],\n",
      "        [-190.6207, -255.0235],\n",
      "        [-284.6468, -382.3896]])\n",
      "1\n",
      "reward -35.7023527778277\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-46.9426],\n",
      "        [-73.4551],\n",
      "        [-41.6451],\n",
      "        [-45.6004],\n",
      "        [-35.2009]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-296.1201, -305.0291, -314.0273, -300.0358, -313.2104, -455.0849,\n",
      "         -422.6671, -400.7051, -463.4532],\n",
      "        [-444.3070, -458.0226, -471.7616, -451.0232, -470.7242, -683.2711,\n",
      "         -634.3882, -601.7977, -696.3289],\n",
      "        [-273.6834, -281.7595, -289.9622, -276.6834, -288.9754, -420.2223,\n",
      "         -390.5081, -369.9945, -427.7547],\n",
      "        [-331.8121, -341.5375, -351.4309, -335.2361, -350.2200, -509.2243,\n",
      "         -473.3199, -448.4199, -518.7350],\n",
      "        [-201.2058, -207.4253, -212.9738, -202.9047, -212.1106, -308.6337,\n",
      "         -287.0346, -271.8442, -314.1001]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-296.1201, -400.7051],\n",
      "        [-444.3070, -601.7977],\n",
      "        [-281.7595, -390.5081],\n",
      "        [-341.5375, -473.3199],\n",
      "        [-212.9738, -287.0346]], grad_fn=<GatherBackward>)\n",
      "A [[-286.0621  -294.86795 -303.39368 -289.94702 -302.7106  -439.6454\n",
      "  -408.3352  -387.18427 -447.9677 ]\n",
      " [-419.07956 -432.0103  -444.88638 -425.4316  -443.97244 -644.45935\n",
      "  -598.303   -567.58844 -656.88196]\n",
      " [-263.7851  -271.77927 -279.54202 -266.90286 -278.75894 -405.12064\n",
      "  -376.4311  -356.76187 -412.661  ]\n",
      " [-332.14975 -341.40186 -351.17972 -334.26883 -349.49658 -508.8916\n",
      "  -473.45657 -448.07693 -518.1437 ]\n",
      " [-171.98322 -178.06357 -182.80537 -175.08044 -182.69478 -264.8871\n",
      "  -245.81807 -233.41032 -269.89258]]\n",
      "J tensor([-286.0621, -419.0796, -263.7851, -332.1497, -171.9832])\n",
      "P tensor([-387.1843, -567.5884, -356.7619, -448.0769, -233.4103])\n",
      "expected_state_action_values  tensor([[-304.3985, -395.4084],\n",
      "        [-450.6267, -584.2847],\n",
      "        [-279.0516, -362.7307],\n",
      "        [-344.5352, -448.8697],\n",
      "        [-189.9858, -245.2702]])\n",
      "reward -36.70137480047493\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  6.3968,  6.6019,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  8.2138, 18.7039,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.4549,  4.3198,  7.2281,  8.3536,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  4.7618, 18.3990, 31.2316,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.2177,  6.4395,  9.2441, 18.7039,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-22.4983],\n",
      "        [ -4.2900],\n",
      "        [ -4.6880],\n",
      "        [-77.0748],\n",
      "        [-37.8440]])\n",
      "action_batch tensor([[4, 7],\n",
      "        [4, 6],\n",
      "        [4, 6],\n",
      "        [2, 6],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-134.4014, -137.7660, -131.2861, -130.3973, -136.1136, -197.5128,\n",
      "         -178.7971, -169.2358, -201.0881],\n",
      "        [ -49.1815,  -50.6267,  -47.7392,  -47.3371,  -49.5689,  -72.0905,\n",
      "          -65.4458,  -61.7774,  -73.2495],\n",
      "        [ -41.9809,  -43.2939,  -40.7552,  -40.4993,  -42.3947,  -61.6140,\n",
      "          -55.9086,  -52.7930,  -62.5711],\n",
      "        [-418.1630, -427.9183, -409.1944, -406.8551, -424.3519, -614.8400,\n",
      "         -556.1249, -526.8852, -627.4017],\n",
      "        [-198.0671, -203.3008, -194.0739, -193.1976, -201.4461, -291.6767,\n",
      "         -263.6851, -250.0090, -297.3062]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-136.1136, -169.2358],\n",
      "        [ -49.5689,  -65.4458],\n",
      "        [ -42.3947,  -55.9086],\n",
      "        [-409.1944, -556.1249],\n",
      "        [-201.4461, -250.0090]], grad_fn=<GatherBackward>)\n",
      "A [[-121.405075 -124.43467  -118.514084 -117.68921  -122.88081  -178.3533\n",
      "  -161.4974   -152.8121   -181.56833 ]\n",
      " [ -46.20012   -47.590614  -44.84748   -44.505928  -46.598446  -67.75273\n",
      "   -61.496952  -58.057453  -68.82816 ]\n",
      " [ -49.181496  -50.626728  -47.739193  -47.33712   -49.56891   -72.09047\n",
      "   -65.44582   -61.777435  -73.24954 ]\n",
      " [-408.98724  -418.62238  -400.3631   -398.31125  -415.37576  -601.57947\n",
      "  -544.02356  -515.52905  -613.9789  ]\n",
      " [-179.37697  -184.3677   -175.90253  -175.3923   -182.82744  -264.39157\n",
      "  -238.90532  -226.66988  -269.72226 ]]\n",
      "J tensor([-117.6892,  -44.5059,  -47.3371, -398.3112, -175.3923])\n",
      "P tensor([-152.8121,  -58.0575,  -61.7774, -515.5291, -226.6699])\n",
      "expected_state_action_values  tensor([[-128.4186, -160.0292],\n",
      "        [ -44.3454,  -56.5417],\n",
      "        [ -47.2914,  -60.2877],\n",
      "        [-435.5549, -541.0509],\n",
      "        [-195.6971, -241.8469]])\n",
      "reward -39.073133285670224\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  5.8472,  5.9286,  9.1005,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.1372,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-42.7328],\n",
      "        [-28.5828],\n",
      "        [-73.4551],\n",
      "        [-10.7610],\n",
      "        [-19.2270]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 7],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-258.9746, -264.2859, -261.8302, -249.9314, -245.9553, -379.4578,\n",
      "         -335.2281, -315.9918, -386.8637],\n",
      "        [-138.9439, -142.2842, -140.8676, -135.0950, -132.6998, -204.3488,\n",
      "         -180.1409, -170.1331, -208.0815],\n",
      "        [-419.4380, -428.8599, -425.3693, -407.3395, -400.3246, -616.1021,\n",
      "         -543.3321, -513.1980, -628.8445],\n",
      "        [ -76.7743,  -78.5599,  -77.4056,  -73.9254,  -72.7820, -112.5088,\n",
      "          -99.4625,  -93.6399, -114.3218],\n",
      "        [ -87.0284,  -88.9572,  -87.7771,  -83.8663,  -82.5427, -127.5547,\n",
      "         -112.7290, -106.1488, -129.6788]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-264.2859, -335.2281],\n",
      "        [-138.9439, -170.1331],\n",
      "        [-419.4380, -513.1980],\n",
      "        [ -78.5599,  -99.4625],\n",
      "        [ -88.9572, -112.7290]], grad_fn=<GatherBackward>)\n",
      "A [[-249.04596  -254.34276  -251.98152  -240.82175  -236.83568  -365.23618\n",
      "  -322.43127  -304.13452  -372.35513 ]\n",
      " [-132.29324  -135.60986  -134.16148  -128.6786   -126.408875 -194.59874\n",
      "  -171.54372  -162.05197  -198.1888  ]\n",
      " [-395.9628   -404.85046  -401.47406  -384.56082  -377.89862  -581.60583\n",
      "  -512.8624   -484.44052  -593.74493 ]\n",
      " [ -77.20515   -78.9987    -77.83895   -74.33451   -73.18614  -113.135414\n",
      "  -100.0186    -94.1624   -114.96099 ]\n",
      " [-117.80095  -120.28096  -118.93381  -113.399826 -111.6298   -172.58737\n",
      "  -152.56644  -143.64708  -175.361   ]]\n",
      "J tensor([-236.8357, -126.4089, -377.8986,  -73.1861, -111.6298])\n",
      "P tensor([-304.1345, -162.0520, -484.4405,  -94.1624, -143.6471])\n",
      "expected_state_action_values  tensor([[-255.8849, -316.4539],\n",
      "        [-142.3508, -174.4296],\n",
      "        [-413.5639, -509.4516],\n",
      "        [ -76.6285,  -95.5071],\n",
      "        [-119.6939, -148.5094]])\n",
      "1\n",
      "reward -37.841257769607495\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.8575, 29.0381,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  3.4794, 36.3310,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.1096, 28.4325,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 6.0554,  5.7395,  4.8381, 41.4549,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.2009],\n",
      "        [ -4.6880],\n",
      "        [-53.5095],\n",
      "        [-50.0415],\n",
      "        [-52.1633]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [4, 6],\n",
      "        [2, 6],\n",
      "        [4, 7],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-173.6871, -179.1821, -183.8024, -175.4535, -172.7127, -266.3896,\n",
      "         -229.2273, -215.5805, -271.6936],\n",
      "        [ -37.4029,  -38.9559,  -39.4121,  -37.8178,  -37.2744,  -57.4616,\n",
      "          -49.4963,  -46.5108,  -58.4346],\n",
      "        [-300.5271, -309.5720, -317.9216, -303.1893, -298.5334, -460.5046,\n",
      "         -396.3869, -372.6896, -470.1425],\n",
      "        [-130.2680, -136.2316, -140.1268, -136.2139, -133.4612, -203.0148,\n",
      "         -173.1072, -164.3525, -207.7946],\n",
      "        [-151.2086, -159.1327, -163.8819, -159.8117, -156.7777, -237.4052,\n",
      "         -201.6483, -191.9890, -243.1642]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-183.8024, -229.2273],\n",
      "        [ -37.2744,  -49.4963],\n",
      "        [-317.9216, -396.3869],\n",
      "        [-133.4612, -164.3525],\n",
      "        [-156.7777, -191.9890]], grad_fn=<GatherBackward>)\n",
      "A [[-145.34932  -150.71275  -154.57144  -148.45981  -145.86504  -223.998\n",
      "  -192.21846  -181.36609  -228.75024 ]\n",
      " [ -43.698627  -45.42357   -46.03941   -44.074963  -43.45696   -67.04345\n",
      "   -57.77811   -54.271786  -68.21646 ]\n",
      " [-302.52     -311.33054  -319.94666  -305.04904  -300.32346  -463.5182\n",
      "  -398.9839   -375.02295  -472.92657 ]\n",
      " [-147.61327  -153.95993  -158.40329  -153.55428  -150.41435  -229.44025\n",
      "  -195.91873  -185.77155  -234.51569 ]\n",
      " [-140.2989   -147.44519  -151.81017  -148.1159   -145.17363  -219.9962\n",
      "  -186.98862  -177.96825  -225.33113 ]]\n",
      "J tensor([-145.3493,  -43.4570, -300.3235, -147.6133, -140.2989])\n",
      "P tensor([-181.3661,  -54.2718, -375.0229, -185.7715, -177.9682])\n",
      "expected_state_action_values  tensor([[-166.0153, -198.4304],\n",
      "        [ -43.7993,  -53.5326],\n",
      "        [-323.8007, -391.0302],\n",
      "        [-182.8935, -217.2359],\n",
      "        [-178.4323, -212.3347]])\n",
      "5\n",
      "reward -41.35981474891603\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  7.5863, 10.1950,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2688,  0.0000, 14.4961, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.9144,  5.0590,  7.1990,  9.5668,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3717,  6.4395,  5.6897,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  3.2197,  7.4044,  6.5020,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.9144,  5.0590,  7.1990,  9.5668,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.1448, 10.2561,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.3428,  6.4395,  5.9774, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-36.9724],\n",
      "        [-31.9273],\n",
      "        [-43.0366],\n",
      "        [-29.7391],\n",
      "        [-24.2843]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-128.6982, -135.4673, -143.0504, -133.2595, -138.6952, -201.0535,\n",
      "         -167.4683, -168.5747, -202.3970],\n",
      "        [-140.7545, -148.4730, -156.7945, -146.4439, -152.2115, -220.4344,\n",
      "         -183.3191, -184.8921, -221.8677],\n",
      "        [-248.8571, -261.8965, -277.0333, -258.4503, -268.6983, -389.2307,\n",
      "         -323.8238, -326.4549, -392.2871],\n",
      "        [-130.2001, -137.6857, -145.2877, -135.9873, -141.2631, -204.3569,\n",
      "         -169.8059, -171.5117, -205.5938],\n",
      "        [-134.0925, -141.7693, -149.7248, -140.1507, -145.5322, -210.5412,\n",
      "         -174.8790, -176.6561, -211.7208]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-133.2595, -168.5747],\n",
      "        [-156.7945, -183.3191],\n",
      "        [-248.8571, -326.4549],\n",
      "        [-137.6857, -169.8059],\n",
      "        [-145.5322, -176.6561]], grad_fn=<GatherBackward>)\n",
      "A [[-167.06529 -176.05324 -186.22665 -173.8916  -180.73007 -261.6902\n",
      "  -217.62054 -219.46327 -263.3336 ]\n",
      " [-144.86313 -152.75761 -161.37027 -150.66896 -156.59984 -226.82405\n",
      "  -188.63518 -190.22755 -228.28458]\n",
      " [-248.10231 -261.05472 -275.99765 -257.34097 -267.65277 -387.7864\n",
      "  -322.76346 -325.2777  -390.98196]\n",
      " [-140.75447 -148.47298 -156.79453 -146.44392 -152.21146 -220.43443\n",
      "  -183.31912 -184.89206 -221.8677 ]\n",
      " [-122.41621 -128.59442 -135.87167 -126.51902 -131.63676 -191.0098\n",
      "  -159.09155 -160.01169 -192.3247 ]]\n",
      "J tensor([-167.0653, -144.8631, -248.1023, -140.7545, -122.4162])\n",
      "P tensor([-217.6205, -188.6352, -322.7635, -183.3191, -159.0916])\n",
      "expected_state_action_values  tensor([[-187.3311, -232.8309],\n",
      "        [-162.3041, -201.6989],\n",
      "        [-266.3287, -333.5237],\n",
      "        [-156.4182, -194.7263],\n",
      "        [-134.4589, -167.4667]])\n",
      "reward -33.37645382572675\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  0.0000, 18.4464, 14.4922,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.8575, 29.0381,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  6.8479, 12.3229, 14.8634,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.1096, 28.4325,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-49.9555],\n",
      "        [-45.6004],\n",
      "        [-44.9251],\n",
      "        [ -3.5828],\n",
      "        [-50.0415]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-266.3763, -283.7722, -296.7795, -283.2389, -256.4335, -400.4142,\n",
      "         -344.5847, -351.9300, -394.3881],\n",
      "        [-319.7841, -339.9301, -355.4602, -338.4843, -306.8667, -479.5726,\n",
      "         -413.1351, -421.3114, -472.9005],\n",
      "        [-273.1796, -291.0813, -304.1355, -289.8149, -262.6274, -410.1748,\n",
      "         -353.2946, -360.6163, -404.1888],\n",
      "        [ -24.0073,  -25.9716,  -26.5836,  -25.4967,  -23.2000,  -36.2036,\n",
      "          -31.2913,  -31.8343,  -35.2711],\n",
      "        [-153.5248, -166.5866, -173.6488, -168.0751, -150.8405, -234.1008,\n",
      "         -200.5107, -207.0129, -229.3901]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-283.7722, -344.5847],\n",
      "        [-339.9301, -413.1351],\n",
      "        [-304.1355, -353.2946],\n",
      "        [ -24.0073,  -31.8343],\n",
      "        [-150.8405, -207.0129]], grad_fn=<GatherBackward>)\n",
      "A [[-285.55737  -303.90985  -317.90652  -303.22958  -274.64368  -428.94934\n",
      "  -369.22336  -376.90475  -422.67096 ]\n",
      " [-320.22488  -339.49048  -355.0585   -337.2676   -306.25687  -479.10187\n",
      "  -413.10773  -420.50473  -472.7674  ]\n",
      " [-264.34598  -281.9122   -294.307    -280.4368   -254.12419  -396.8795\n",
      "  -341.93607  -349.08774  -391.1471  ]\n",
      " [ -39.81664   -42.797832  -44.07389   -42.142017  -38.273647  -59.82091\n",
      "   -51.623993  -52.594807  -58.693382]\n",
      " [-173.23473  -187.28383  -195.41777  -188.5569   -169.54051  -263.42767\n",
      "  -225.80243  -232.62015  -258.33884 ]]\n",
      "J tensor([-274.6437, -306.2569, -254.1242,  -38.2736, -169.5405])\n",
      "P tensor([-369.2234, -413.1077, -341.9361,  -51.6240, -225.8024])\n",
      "expected_state_action_values  tensor([[-297.1348, -382.2566],\n",
      "        [-321.2316, -417.3974],\n",
      "        [-273.6368, -352.6675],\n",
      "        [ -38.0291,  -50.0444],\n",
      "        [-202.6280, -253.2637]])\n",
      "5\n",
      "reward -39.06861866652845\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  6.4395,  4.1460, 11.5169,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2276,  6.4395,  7.2726, 25.0270,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 10.1104,  9.0302,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.0690,  6.4395,  6.5522, 21.2992,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.1372,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  6.4395,  3.2406, 12.5768,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.7182, 26.0510,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.3431,  6.4395,  7.1075, 17.9512,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 6.2107,  6.4395,  6.5373,  6.9140,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-28.3822],\n",
      "        [-45.9667],\n",
      "        [-40.4197],\n",
      "        [-41.3598],\n",
      "        [-29.3088]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [4, 7],\n",
      "        [1, 6],\n",
      "        [0, 6],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-149.2440, -147.0004, -152.6718, -154.0173, -147.8630, -215.9075,\n",
      "         -192.7569, -198.8266, -209.0181],\n",
      "        [-185.4897, -183.9930, -190.7246, -193.8456, -185.3788, -269.7767,\n",
      "         -240.5859, -249.5149, -260.8273],\n",
      "        [-253.6333, -248.5013, -258.4542, -259.2759, -249.5582, -365.1515,\n",
      "         -326.5410, -335.7329, -354.5269],\n",
      "        [-166.6609, -164.5640, -170.7441, -172.5592, -165.4791, -241.4235,\n",
      "         -215.4715, -222.6345, -233.6778],\n",
      "        [-157.4432, -154.2629, -160.3260, -160.7840, -154.7997, -226.6615,\n",
      "         -202.6123, -208.1792, -219.8859]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-147.0004, -192.7569],\n",
      "        [-185.3788, -249.5149],\n",
      "        [-248.5013, -326.5410],\n",
      "        [-166.6609, -215.4715],\n",
      "        [-154.7997, -208.1792]], grad_fn=<GatherBackward>)\n",
      "A [[-136.22356 -134.09877 -139.27403 -140.33554 -134.82677 -196.94002\n",
      "  -175.86995 -181.25833 -190.64972]\n",
      " [-190.55952 -188.64696 -195.72023 -198.46246 -189.98303 -276.75082\n",
      "  -246.82825 -255.62796 -267.67285]\n",
      " [-257.4658  -251.66682 -262.05386 -262.4137  -252.78252 -370.2106\n",
      "  -331.07123 -339.92477 -359.55893]\n",
      " [-170.96297 -169.03357 -175.46454 -177.62715 -170.20381 -248.0647\n",
      "  -221.33553 -228.94879 -239.88307]\n",
      " [-158.7225  -155.69795 -161.84682 -162.66873 -156.44064 -228.87134\n",
      "  -204.47964 -210.36177 -221.89485]]\n",
      "J tensor([-134.0988, -188.6470, -251.6668, -169.0336, -155.6980])\n",
      "P tensor([-175.8699, -246.8282, -331.0712, -221.3355, -204.4796])\n",
      "expected_state_action_values  tensor([[-149.0711, -186.6652],\n",
      "        [-215.7489, -268.1121],\n",
      "        [-266.9198, -338.3838],\n",
      "        [-193.4900, -240.5618],\n",
      "        [-169.4370, -213.3405]])\n",
      "reward -40.98607597613607\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  6.8479, 12.3229, 14.8634,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.1372, 19.6389,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  6.4395, 14.3632, 15.1425,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  7.2726, 22.3220,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-50.3005],\n",
      "        [-73.1039],\n",
      "        [-35.7024],\n",
      "        [-45.6004],\n",
      "        [-28.1379]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [1, 7],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-345.0777, -338.9599, -333.1769, -336.4879, -343.5339, -471.8770,\n",
      "         -438.5575, -453.8231, -451.6885],\n",
      "        [-483.2453, -474.7055, -466.7123, -471.6035, -481.3275, -660.9270,\n",
      "         -614.2367, -635.8885, -632.8149],\n",
      "        [-208.6893, -206.5970, -202.5099, -206.2236, -209.8413, -287.0213,\n",
      "         -266.4117, -277.0970, -273.8379],\n",
      "        [-390.3262, -382.8181, -376.3548, -379.5878, -387.7827, -533.0701,\n",
      "         -495.6196, -512.3796, -510.7417],\n",
      "        [-208.3248, -204.6301, -201.3392, -203.6658, -207.7908, -285.3530,\n",
      "         -264.9336, -274.2069, -272.7150]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-345.0777, -453.8231],\n",
      "        [-474.7055, -635.8885],\n",
      "        [-208.6893, -277.0970],\n",
      "        [-382.8181, -495.6196],\n",
      "        [-201.3392, -264.9336]], grad_fn=<GatherBackward>)\n",
      "A [[-329.0952  -323.29007 -317.79318 -321.2373  -327.82632 -450.21945\n",
      "  -418.31424 -433.0188  -430.90094]\n",
      " [-475.2355  -467.16418 -459.24698 -464.5761  -473.91907 -650.42694\n",
      "  -604.3793  -626.08386 -622.506  ]\n",
      " [-181.21936 -179.73691 -175.88339 -179.59514 -182.57034 -249.49117\n",
      "  -231.56831 -241.17783 -237.95062]\n",
      " [-388.7823  -380.5525  -374.32663 -376.5701  -385.1329  -530.1054\n",
      "  -493.0477  -508.9058  -508.28064]\n",
      " [-186.54036 -183.47235 -180.19936 -182.51035 -186.19151 -255.51605\n",
      "  -237.41643 -245.86658 -244.19025]]\n",
      "J tensor([-317.7932, -459.2470, -175.8834, -374.3266, -180.1994])\n",
      "P tensor([-418.3142, -604.3793, -231.5683, -493.0477, -237.4164])\n",
      "expected_state_action_values  tensor([[-336.3144, -426.7834],\n",
      "        [-486.4262, -617.0453],\n",
      "        [-193.9974, -244.1138],\n",
      "        [-382.4944, -489.3434],\n",
      "        [-190.3174, -241.8127]])\n",
      "reward -42.42868271212684\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 10.1302, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  8.2268,  9.3189, 28.8972,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  7.5863, 10.1950,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  8.2138, 18.7039,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-40.3589],\n",
      "        [-19.2270],\n",
      "        [-36.8639],\n",
      "        [-54.1495],\n",
      "        [-46.5218]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [4, 6],\n",
      "        [4, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-242.0251, -259.3496, -234.6215, -246.4733, -252.8486, -347.4674,\n",
      "         -311.3295, -320.0260, -336.6410],\n",
      "        [ -97.4314, -104.3658,  -94.1753,  -98.8521, -101.5334, -139.7332,\n",
      "         -125.2530, -128.4987, -135.2846],\n",
      "        [-179.1733, -192.7826, -173.9511, -183.3309, -187.8487, -257.8169,\n",
      "         -230.8350, -237.7540, -249.5262],\n",
      "        [-192.8315, -208.8674, -187.8236, -199.1514, -203.6268, -278.5821,\n",
      "         -249.2991, -257.7742, -269.2198],\n",
      "        [-343.1691, -366.9657, -332.2463, -348.3643, -357.5820, -491.8555,\n",
      "         -440.8604, -452.7428, -477.1047]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-259.3496, -311.3295],\n",
      "        [-104.3658, -125.2530],\n",
      "        [-187.8487, -230.8350],\n",
      "        [-203.6268, -257.7742],\n",
      "        [-366.9657, -440.8604]], grad_fn=<GatherBackward>)\n",
      "A [[-254.20729 -273.11987 -246.6599  -259.715   -266.23276 -365.42703\n",
      "  -327.4005  -337.04456 -353.9334 ]\n",
      " [-131.648   -140.76949 -127.37426 -133.37155 -137.03947 -188.67705\n",
      "  -169.11636 -173.41495 -182.72333]\n",
      " [-214.21562 -230.25609 -207.88037 -218.85359 -224.32562 -307.9865\n",
      "  -275.83282 -283.97278 -298.25238]\n",
      " [-233.5765  -253.50201 -228.02708 -242.35204 -247.4647  -338.17206\n",
      "  -302.31097 -313.13766 -326.54654]\n",
      " [-337.72525 -361.66962 -327.11557 -343.49057 -352.42224 -484.4226\n",
      "  -434.19016 -446.26056 -469.79688]]\n",
      "J tensor([-246.6599, -127.3743, -207.8804, -228.0271, -327.1156])\n",
      "P tensor([-327.4005, -169.1164, -275.8328, -302.3110, -434.1902])\n",
      "expected_state_action_values  tensor([[-262.3528, -335.0193],\n",
      "        [-133.8639, -171.4318],\n",
      "        [-223.9562, -285.1134],\n",
      "        [-259.3739, -326.2294],\n",
      "        [-340.9258, -437.2929]])\n",
      "reward -42.04852182894588\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 6.0554,  5.7395,  4.8381, 41.4549,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.8575, 29.0381,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  3.2197,  6.0140, 38.1314,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.1096, 28.4325,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-60.0879],\n",
      "        [-45.6004],\n",
      "        [-41.6451],\n",
      "        [-50.0415],\n",
      "        [-51.4207]])\n",
      "action_batch tensor([[4, 6],\n",
      "        [1, 6],\n",
      "        [1, 6],\n",
      "        [4, 7],\n",
      "        [1, 7]])\n",
      "policy_net1(state_batch) tensor([[-218.7157, -240.2784, -214.3857, -231.6668, -249.6733, -319.8095,\n",
      "         -296.1026, -309.6344, -304.9014],\n",
      "        [-369.9129, -397.6923, -358.3382, -379.1371, -411.5100, -532.7814,\n",
      "         -494.5448, -510.8269, -511.3447],\n",
      "        [-306.5204, -329.5285, -297.0202, -314.2916, -341.0714, -441.6366,\n",
      "         -409.8120, -423.2932, -423.6644],\n",
      "        [-185.0379, -202.6271, -181.0640, -195.0947, -210.4889, -270.0399,\n",
      "         -250.1039, -261.0044, -257.5628],\n",
      "        [-387.3842, -415.6992, -374.9006, -395.4587, -429.6444, -556.9773,\n",
      "         -517.1360, -533.4260, -534.9260]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-249.6733, -296.1026],\n",
      "        [-397.6923, -494.5448],\n",
      "        [-329.5285, -409.8120],\n",
      "        [-210.4889, -261.0044],\n",
      "        [-415.6992, -533.4260]], grad_fn=<GatherBackward>)\n",
      "A [[-221.92973 -243.95969 -217.81552 -235.63324 -253.87196 -324.95804\n",
      "  -300.67804 -314.613   -309.65   ]\n",
      " [-368.57007 -395.25113 -356.50516 -376.1171  -408.6493  -529.8159\n",
      "  -491.98083 -507.3575  -508.8896 ]\n",
      " [-294.745   -317.2806  -285.65778 -302.5766  -328.31113 -424.85724\n",
      "  -394.30887 -407.51297 -407.57062]\n",
      " [-208.6113  -227.68694 -203.88278 -218.89693 -236.3914  -303.79636\n",
      "  -281.3824  -293.1224  -290.01917]\n",
      " [-377.02826 -404.75528 -364.78723 -384.72217 -418.0744  -541.94867\n",
      "  -503.35046 -519.1993  -520.5384 ]]\n",
      "J tensor([-217.8155, -356.5052, -285.6578, -203.8828, -364.7872])\n",
      "P tensor([-300.6780, -491.9808, -394.3089, -281.3824, -503.3505])\n",
      "expected_state_action_values  tensor([[-256.1219, -330.6981],\n",
      "        [-366.4551, -488.3832],\n",
      "        [-298.7370, -396.5230],\n",
      "        [-233.5360, -303.2857],\n",
      "        [-379.7292, -504.4361]])\n",
      "5\n",
      "reward -33.529663407559624\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.0140, 38.1314,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  4.7618, 18.3990, 31.2316,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  7.5781, 35.2749,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 10.5880, 11.7574,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  4.7618, 18.3990, 31.2316,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 18.1602, 26.5328,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-18.9634],\n",
      "        [-55.0717],\n",
      "        [-37.9897],\n",
      "        [-77.0748],\n",
      "        [-77.7920]])\n",
      "action_batch tensor([[3, 6],\n",
      "        [4, 7],\n",
      "        [4, 7],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-122.7014, -125.9420, -119.1414, -125.9096, -144.3583, -176.8322,\n",
      "         -158.4807, -163.3598, -170.8846],\n",
      "        [-244.0498, -253.0408, -238.9384, -255.4009, -291.6030, -354.7117,\n",
      "         -317.1668, -329.5587, -341.8585],\n",
      "        [-251.0015, -256.8368, -243.6982, -256.8712, -294.6397, -361.1264,\n",
      "         -323.8028, -333.5157, -349.4764],\n",
      "        [-459.5727, -471.3197, -447.0812, -472.7113, -541.5236, -662.5491,\n",
      "         -593.6519, -612.8159, -641.0179],\n",
      "        [-443.0692, -453.9959, -430.7280, -454.9754, -521.4030, -638.2668,\n",
      "         -572.0462, -590.1349, -617.7128]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-125.9096, -158.4807],\n",
      "        [-291.6030, -329.5587],\n",
      "        [-294.6397, -333.5157],\n",
      "        [-447.0812, -593.6519],\n",
      "        [-430.7280, -572.0462]], grad_fn=<GatherBackward>)\n",
      "A [[-118.38372 -121.2928  -114.87984 -121.2044  -139.00584 -170.46475\n",
      "  -152.76343 -157.29683 -164.74844]\n",
      " [-217.90846 -227.10738 -213.8832  -229.85948 -262.07367 -317.8397\n",
      "  -284.0258  -296.05457 -305.91077]\n",
      " [-245.4389  -250.53952 -238.06824 -250.32707 -287.29074 -352.63406\n",
      "  -316.21765 -325.222   -341.3817 ]\n",
      " [-450.84158 -462.59827 -438.72742 -464.24033 -531.7186  -650.27216\n",
      "  -582.61426 -601.64777 -629.02527]\n",
      " [-459.57266 -471.31967 -447.08124 -472.71127 -541.52356 -662.5491\n",
      "  -593.6519  -612.8159  -641.0179 ]]\n",
      "J tensor([-114.8798, -213.8832, -238.0682, -438.7274, -447.0812])\n",
      "P tensor([-152.7634, -284.0258, -316.2177, -582.6143, -593.6519])\n",
      "expected_state_action_values  tensor([[-122.3552, -156.4504],\n",
      "        [-247.5665, -310.6949],\n",
      "        [-252.2511, -322.5856],\n",
      "        [-471.9294, -601.4276],\n",
      "        [-480.1651, -612.0786]])\n",
      "reward -39.66840369274708\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  1.7539,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  3.2197, 21.8606, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-52.6400],\n",
      "        [ -4.6880],\n",
      "        [-25.6968],\n",
      "        [ -7.8685],\n",
      "        [-57.2807]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [4, 6],\n",
      "        [3, 6],\n",
      "        [3, 7],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-354.5305, -358.5314, -358.8005, -335.0152, -390.2508, -506.0844,\n",
      "         -467.3630, -449.7062, -494.1902],\n",
      "        [ -45.5454,  -46.5325,  -45.9330,  -43.0969,  -50.2900,  -65.1767,\n",
      "          -60.2646,  -57.9612,  -63.3352],\n",
      "        [-134.5280, -136.7896, -136.5825, -128.4619, -149.3228, -193.0754,\n",
      "         -178.0564, -171.8101, -187.9290],\n",
      "        [ -67.3431,  -68.3238,  -67.9253,  -63.3960,  -74.0341,  -96.0944,\n",
      "          -88.8877,  -85.3237,  -93.5418],\n",
      "        [-365.0114, -368.0863, -368.8710, -344.0079, -400.8272, -520.3928,\n",
      "         -480.5958, -461.8861, -508.4192]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-358.8005, -449.7062],\n",
      "        [ -50.2900,  -60.2646],\n",
      "        [-128.4619, -178.0564],\n",
      "        [ -63.3960,  -85.3237],\n",
      "        [-368.8710, -461.8861]], grad_fn=<GatherBackward>)\n",
      "A [[-381.06616  -384.97913  -385.38358  -359.45184  -418.85745  -543.50323\n",
      "  -502.00757  -482.759    -530.93    ]\n",
      " [ -53.252666  -54.28106   -53.692665  -50.265575  -58.67077   -76.09528\n",
      "   -70.369354  -67.64161   -74.036316]\n",
      " [-143.59836  -145.81401  -145.74478  -136.81874  -159.07101  -205.89716\n",
      "  -189.84259  -183.04732  -200.45369 ]\n",
      " [ -65.889534  -66.75447   -66.463486  -62.000427  -72.3795    -94.01885\n",
      "   -86.91201   -83.386055  -91.48531 ]\n",
      " [-409.38666  -412.8413   -414.07343  -386.4629   -450.03006  -584.12616\n",
      "  -539.1702   -518.39124  -570.4594  ]]\n",
      "J tensor([-359.4518,  -50.2656, -136.8187,  -62.0004, -386.4629])\n",
      "P tensor([-482.7590,  -67.6416, -183.0473,  -83.3861, -518.3912])\n",
      "expected_state_action_values  tensor([[-376.1466, -487.1230],\n",
      "        [ -49.9270,  -65.5654],\n",
      "        [-148.8336, -190.4394],\n",
      "        [ -63.6689,  -82.9159],\n",
      "        [-405.0973, -523.8328]])\n",
      "1\n",
      "reward -37.17033698461408\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  8.2268,  9.3189, 28.8972,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.2640,  6.4395, 12.5595,  9.9073,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  7.2726, 22.3220,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  8.2138, 18.7039,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197, 11.7271, 17.0151,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2276,  6.4395,  7.2726, 25.0270,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-35.5628],\n",
      "        [-54.1495],\n",
      "        [-10.7610],\n",
      "        [-37.1703],\n",
      "        [-43.7406]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [4, 7],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [0, 6]])\n",
      "policy_net1(state_batch) tensor([[-194.3099, -198.5923, -205.9803, -197.3744, -207.7853, -279.2646,\n",
      "         -266.0282, -257.6790, -269.8618],\n",
      "        [-216.2207, -221.8150, -229.8900, -220.8277, -232.2722, -311.5143,\n",
      "         -296.7143, -287.9595, -300.7739],\n",
      "        [ -91.7972,  -93.4474,  -96.7567,  -92.3086,  -97.4144, -131.4240,\n",
      "         -125.2324, -120.8373, -127.0282],\n",
      "        [-236.0539, -241.5288, -250.4835, -239.9619, -252.6497, -339.3548,\n",
      "         -323.3422, -313.3296, -327.9502],\n",
      "        [-216.3965, -222.5801, -230.7122, -222.3434, -233.5097, -312.6521,\n",
      "         -297.5366, -289.3042, -301.5190]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-194.3099, -266.0282],\n",
      "        [-232.2722, -287.9595],\n",
      "        [ -93.4474, -125.2324],\n",
      "        [-236.0539, -313.3296],\n",
      "        [-216.3965, -297.5366]], grad_fn=<GatherBackward>)\n",
      "A [[-205.3142   -209.51828  -217.67358  -208.42567  -219.43361  -295.0509\n",
      "  -280.96918  -272.01062  -285.024   ]\n",
      " [-264.17377  -271.3626   -281.45337  -270.78412  -284.5411   -381.24844\n",
      "  -362.87967  -352.59048  -367.88614 ]\n",
      " [ -92.299995  -93.956276  -97.285164  -92.80771   -97.94245  -132.13867\n",
      "  -125.91476  -121.49417  -127.722855]\n",
      " [-243.1772   -247.73601  -257.19296  -245.49878  -258.7618   -348.51132\n",
      "  -332.0457   -321.0436   -337.2459  ]\n",
      " [-208.58064  -214.19214  -222.16953  -213.7654   -224.65498  -301.03888\n",
      "  -286.55225  -278.33893  -290.34363 ]]\n",
      "J tensor([-205.3142, -264.1738,  -92.3000, -243.1772, -208.5806])\n",
      "P tensor([-272.0106, -352.5905, -121.4942, -321.0436, -278.3389])\n",
      "expected_state_action_values  tensor([[-220.3456, -280.3724],\n",
      "        [-291.9059, -371.4810],\n",
      "        [ -93.8310, -120.1057],\n",
      "        [-256.0298, -326.1096],\n",
      "        [-231.4632, -294.2456]])\n",
      "5\n",
      "reward -44.544329631235826\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 9.9864,  9.6592,  4.3602, 10.4849,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  6.4395, 23.7921, 22.3220,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  7.2726, 22.3220,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.0296, 12.1538,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2276,  6.4395,  7.2726, 25.0270,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-36.4907],\n",
      "        [-46.9426],\n",
      "        [-47.6074],\n",
      "        [-74.5264],\n",
      "        [-43.7406]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [0, 6]])\n",
      "policy_net1(state_batch) tensor([[-199.9031, -202.5089, -201.3262, -192.9236, -214.2326, -273.0829,\n",
      "         -253.4368, -260.5361, -261.9467],\n",
      "        [-381.6328, -386.2647, -384.1903, -368.0476, -408.6537, -520.9323,\n",
      "         -483.5282, -497.1782, -500.3965],\n",
      "        [-336.2748, -340.1384, -338.2635, -323.6925, -359.5686, -458.6772,\n",
      "         -425.8330, -437.5410, -440.7090],\n",
      "        [-543.5485, -550.7755, -547.6827, -525.1434, -582.8973, -742.3864,\n",
      "         -689.0861, -709.1780, -713.0908],\n",
      "        [-241.8655, -246.9680, -244.9058, -236.3938, -261.8211, -332.0660,\n",
      "         -307.8476, -318.2175, -317.9889]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-201.3262, -253.4368],\n",
      "        [-381.6328, -497.1782],\n",
      "        [-338.2635, -425.8330],\n",
      "        [-543.5485, -709.1780],\n",
      "        [-241.8655, -307.8476]], grad_fn=<GatherBackward>)\n",
      "A [[-226.69528 -229.54434 -228.46985 -219.11307 -243.16083 -309.90027\n",
      "  -287.4364  -295.59723 -297.19867]\n",
      " [-368.76593 -373.56906 -371.28857 -355.8158  -395.09317 -503.426\n",
      "  -467.38403 -480.74905 -483.57394]\n",
      " [-361.36328 -365.69968 -363.60388 -347.94916 -386.51065 -492.94528\n",
      "  -457.6592  -470.35107 -473.64252]\n",
      " [-561.6895  -568.6553  -565.5052  -541.44794 -601.3039  -766.4456\n",
      "  -711.58777 -731.74866 -736.525  ]\n",
      " [-233.06    -237.64062 -235.82013 -227.27875 -251.87715 -319.7101\n",
      "  -296.4786  -306.1375  -306.1834 ]]\n",
      "J tensor([-219.1131, -355.8158, -347.9492, -541.4479, -227.2787])\n",
      "P tensor([-287.4364, -467.3840, -457.6592, -711.5878, -296.4786])\n",
      "expected_state_action_values  tensor([[-233.6924, -295.1834],\n",
      "        [-367.1768, -467.5883],\n",
      "        [-360.7617, -459.5007],\n",
      "        [-561.8295, -714.9553],\n",
      "        [-248.2915, -310.5714]])\n",
      "reward -34.78890666841803\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 1.4266,  0.0000,  3.3406,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  3.2406, 11.1610,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  8.8056, 17.9032,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 0.0000,  0.0000,  2.2900,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  8.5990,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 16.5628, 10.7409,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 18.4777,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -6.7672],\n",
      "        [-32.2255],\n",
      "        [-27.4673],\n",
      "        [-48.1451],\n",
      "        [-57.2807]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [2, 6],\n",
      "        [4, 6],\n",
      "        [1, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[ -62.5927,  -61.0015,  -62.3310,  -57.4120,  -63.8962,  -81.4252,\n",
      "          -78.1427,  -80.3938,  -77.4766],\n",
      "        [-184.3953, -179.2830, -184.6773, -170.5588, -189.1936, -240.4549,\n",
      "         -230.2937, -237.8065, -229.0410],\n",
      "        [-175.1633, -170.2601, -175.4681, -162.0764, -179.7863, -228.4762,\n",
      "         -218.8229, -225.9352, -217.5238],\n",
      "        [-457.4702, -442.3082, -457.0051, -420.0829, -466.5443, -594.7045,\n",
      "         -569.7848, -586.7759, -567.7718],\n",
      "        [-459.3600, -443.6539, -458.3126, -420.6449, -467.4999, -596.4688,\n",
      "         -571.7142, -588.1773, -569.8555]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -61.0015,  -78.1427],\n",
      "        [-184.6773, -230.2937],\n",
      "        [-179.7863, -218.8229],\n",
      "        [-442.3082, -569.7848],\n",
      "        [-458.3126, -588.1773]], grad_fn=<GatherBackward>)\n",
      "A [[ -90.43659  -87.82714  -90.09596  -82.86724  -92.18893 -117.54694\n",
      "  -112.76758 -115.99522 -112.01978]\n",
      " [-232.43147 -225.64377 -232.75609 -214.74004 -238.2051  -302.97412\n",
      "  -290.08032 -299.38184 -288.72183]\n",
      " [-186.57933 -181.39005 -187.04044 -172.76668 -191.5869  -243.47876\n",
      "  -233.07442 -240.70154 -231.77199]\n",
      " [-449.57376 -434.2714  -448.78622 -412.16644 -457.90625 -584.0906\n",
      "  -559.65826 -575.95416 -557.847  ]\n",
      " [-518.8826  -501.13074 -518.05615 -475.78644 -528.5321  -674.18646\n",
      "  -645.90857 -664.7446  -643.91034]]\n",
      "J tensor([ -82.8672, -214.7400, -172.7667, -412.1664, -475.7864])\n",
      "P tensor([-112.0198, -288.7218, -231.7720, -557.8470, -643.9103])\n",
      "expected_state_action_values  tensor([[ -81.3478, -107.5851],\n",
      "        [-225.4915, -292.0752],\n",
      "        [-182.9573, -236.0621],\n",
      "        [-419.0948, -550.2073],\n",
      "        [-485.4885, -636.7999]])\n",
      "reward -35.64468602209828\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-53.5095],\n",
      "        [-10.7610],\n",
      "        [-18.9634],\n",
      "        [-35.2038],\n",
      "        [-33.5341]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [1, 6],\n",
      "        [4, 6],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-504.6178, -468.3592, -524.4775, -463.2067, -541.7388, -655.6642,\n",
      "         -614.0026, -668.5127, -622.5869],\n",
      "        [-115.8345, -107.6497, -120.2022, -106.2962, -124.3401, -150.6452,\n",
      "         -141.0438, -153.3384, -142.7460],\n",
      "        [-168.2154, -156.7191, -175.2174, -155.3455, -181.5007, -219.2146,\n",
      "         -205.1811, -223.7718, -207.5770],\n",
      "        [-215.7789, -200.8006, -224.8052, -199.2917, -232.8064, -281.1656,\n",
      "         -263.1521, -287.0160, -266.2938],\n",
      "        [-244.0206, -226.8345, -254.1767, -225.2081, -263.0550, -317.9116,\n",
      "         -297.4323, -324.2565, -301.1702]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-524.4775, -614.0026],\n",
      "        [-107.6497, -141.0438],\n",
      "        [-181.5007, -205.1811],\n",
      "        [-224.8052, -263.1521],\n",
      "        [-254.1767, -297.4323]], grad_fn=<GatherBackward>)\n",
      "A [[-507.85294  -470.97937  -527.70996  -465.97244  -544.9362   -659.83\n",
      "  -617.7703   -672.3602   -626.5478  ]\n",
      " [-116.463585 -108.23193  -120.85355  -106.867516 -125.00999  -151.45891\n",
      "  -141.8069   -154.16628  -143.521   ]\n",
      " [-162.10443  -150.86807  -168.74542  -149.4931   -174.68852  -211.17769\n",
      "  -197.62091  -215.34796  -199.9879  ]\n",
      " [-242.10837  -225.5854   -252.71553  -224.44261  -261.92908  -315.9532\n",
      "  -295.4965   -322.7713   -299.04944 ]\n",
      " [-237.69412  -221.3077   -247.9242   -219.99814  -256.8279   -310.00974\n",
      "  -289.97543  -316.51868  -293.50775 ]]\n",
      "J tensor([-465.9724, -106.8675, -149.4931, -224.4426, -219.9981])\n",
      "P tensor([-617.7703, -141.8069, -197.6209, -295.4965, -289.9754])\n",
      "expected_state_action_values  tensor([[-472.8847, -609.5028],\n",
      "        [-106.9418, -138.3872],\n",
      "        [-153.5072, -196.8222],\n",
      "        [-237.2021, -301.1506],\n",
      "        [-231.5324, -294.5120]])\n",
      "5\n",
      "reward -38.063557156369185\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  6.8575, 29.0381,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.1096, 28.4325,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-33.5341],\n",
      "        [-36.9989],\n",
      "        [-50.0415],\n",
      "        [-53.5095],\n",
      "        [ -3.5828]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 7],\n",
      "        [4, 7],\n",
      "        [2, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-232.4260, -206.0424, -232.7022, -214.2167, -237.9868, -302.7153,\n",
      "         -274.3643, -307.2886, -288.4016],\n",
      "        [-301.8670, -267.2616, -301.8554, -277.3631, -308.2746, -392.6477,\n",
      "         -355.8948, -398.1710, -374.5691],\n",
      "        [-248.9160, -222.4480, -250.9328, -233.1048, -258.1509, -326.0693,\n",
      "         -295.1473, -333.1351, -309.7573],\n",
      "        [-482.6746, -427.2524, -482.1878, -442.3469, -492.0746, -626.8824,\n",
      "         -568.7817, -636.1108, -598.7101],\n",
      "        [ -33.1844,  -29.8234,  -33.0857,  -30.5679,  -34.1014,  -43.3239,\n",
      "          -39.4175,  -43.9967,  -40.9314]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-232.7022, -274.3643],\n",
      "        [-301.8670, -398.1710],\n",
      "        [-258.1509, -333.1351],\n",
      "        [-482.1878, -568.7817],\n",
      "        [ -33.1844,  -43.9967]], grad_fn=<GatherBackward>)\n",
      "A [[-225.90648  -200.5808   -226.47871  -208.83774  -231.87009  -294.56564\n",
      "  -266.89407  -299.33798  -280.45734 ]\n",
      " [-301.1543   -266.85364  -301.12863  -276.67465  -307.58545  -391.6326\n",
      "  -355.11517  -397.40848  -373.6594  ]\n",
      " [-278.61865  -248.59138  -280.56952  -260.10355  -288.2053   -364.57584\n",
      "  -330.01788  -371.9392   -346.58185 ]\n",
      " [-485.65436  -429.53616  -485.06082  -444.87296  -494.86038  -630.7144\n",
      "  -572.14667  -639.5994   -602.3741  ]\n",
      " [ -55.57119   -49.67387   -55.392803  -51.060787  -56.8663    -72.35271\n",
      "   -65.73421   -73.48632   -68.76172 ]]\n",
      "J tensor([-200.5808, -266.8536, -248.5914, -429.5362,  -49.6739])\n",
      "P tensor([-266.8941, -355.1152, -330.0179, -572.1467,  -65.7342])\n",
      "expected_state_action_values  tensor([[-214.0568, -273.7388],\n",
      "        [-277.1671, -356.6025],\n",
      "        [-273.7738, -347.0576],\n",
      "        [-440.0921, -568.4415],\n",
      "        [ -48.2893,  -62.7436]])\n",
      "5\n",
      "reward -42.43790871713207\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 4.2799,  3.2197,  6.3968,  6.6019,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  3.2406, 11.1610,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  7.5781, 35.2749,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  8.2268,  9.3189, 28.8972,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.4549,  4.3198,  7.2281,  8.3536,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  8.5990,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.4535,  6.4395,  8.8024, 28.5155,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  8.2138, 18.7039,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-22.4983],\n",
      "        [-27.4673],\n",
      "        [-58.4256],\n",
      "        [-54.1495],\n",
      "        [-32.6959]])\n",
      "action_batch tensor([[4, 7],\n",
      "        [4, 6],\n",
      "        [1, 7],\n",
      "        [4, 7],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-180.3121, -167.7587, -182.0193, -174.2264, -203.3244, -246.3148,\n",
      "         -216.3638, -245.0676, -235.8810],\n",
      "        [-162.6345, -151.5202, -164.4293, -157.8000, -184.0247, -222.5687,\n",
      "         -195.4290, -221.7241, -212.8994],\n",
      "        [-303.8251, -283.7928, -308.2045, -296.5229, -345.3670, -416.6760,\n",
      "         -365.5194, -416.1038, -398.5102],\n",
      "        [-247.8744, -231.2843, -251.0034, -241.0593, -281.0162, -339.4351,\n",
      "         -297.9851, -338.6744, -324.8110],\n",
      "        [-277.4186, -257.4209, -279.6891, -267.0258, -311.8098, -378.2949,\n",
      "         -332.3914, -375.8691, -362.8077]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-203.3244, -245.0676],\n",
      "        [-184.0247, -195.4290],\n",
      "        [-283.7928, -416.1038],\n",
      "        [-281.0162, -338.6744],\n",
      "        [-279.6891, -375.8691]], grad_fn=<GatherBackward>)\n",
      "A [[-161.7509  -150.49097 -163.20543 -156.20282 -182.32645 -220.91028\n",
      "  -194.09132 -219.7772  -211.54132]\n",
      " [-173.22858 -161.42839 -175.27612 -168.20259 -196.0916  -237.17534\n",
      "  -208.14832 -236.2011  -226.83463]\n",
      " [-310.5603  -290.66745 -315.6706  -304.52148 -354.35257 -426.74277\n",
      "  -374.1292  -426.85364 -407.72598]\n",
      " [-306.45038 -286.1831  -310.84274 -298.93484 -348.19327 -420.21344\n",
      "  -368.60065 -419.48648 -401.91858]\n",
      " [-259.36957 -240.6387  -261.28198 -249.19319 -291.14136 -353.3961\n",
      "  -310.65424 -351.04443 -339.05286]]\n",
      "J tensor([-150.4910, -161.4284, -290.6674, -286.1831, -240.6387])\n",
      "P tensor([-194.0913, -208.1483, -374.1292, -368.6006, -310.6542])\n",
      "expected_state_action_values  tensor([[-157.9402, -197.1805],\n",
      "        [-172.7529, -214.8008],\n",
      "        [-320.0263, -395.1418],\n",
      "        [-311.7143, -385.8901],\n",
      "        [-249.2707, -312.2847]])\n",
      "reward -47.356294965389786\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.0690,  6.4395,  6.5522, 21.2992,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2688,  0.0000, 14.4961, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.3431,  6.4395,  7.1075, 17.9512,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-53.5095],\n",
      "        [-41.3598],\n",
      "        [-43.0366],\n",
      "        [-47.9592],\n",
      "        [-21.4316]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [0, 6],\n",
      "        [0, 7],\n",
      "        [0, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-425.6086, -410.9842, -411.6756, -407.5299, -454.0109, -578.8412,\n",
      "         -523.2520, -556.5476, -558.9915],\n",
      "        [-205.1642, -199.6596, -199.5856, -199.3626, -221.4293, -280.8712,\n",
      "         -253.4169, -271.1360, -270.1757],\n",
      "        [-358.6682, -346.9842, -347.5858, -345.4246, -384.3033, -489.0025,\n",
      "         -441.7047, -470.8465, -471.6230],\n",
      "        [-345.3334, -333.4966, -334.2845, -331.4547, -368.9859, -470.2216,\n",
      "         -424.7897, -452.0842, -453.7451],\n",
      "        [-136.2531, -131.9910, -131.9841, -131.2643, -146.0406, -185.9219,\n",
      "         -167.8772, -178.8301, -179.0243]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-411.6756, -523.2520],\n",
      "        [-205.1642, -253.4169],\n",
      "        [-358.6682, -470.8465],\n",
      "        [-345.3334, -424.7897],\n",
      "        [-136.2531, -178.8301]], grad_fn=<GatherBackward>)\n",
      "A [[-427.29834 -412.2003  -413.22446 -408.90903 -455.52206 -581.05884\n",
      "  -525.1431  -558.28394 -561.1382 ]\n",
      " [-213.40967 -207.8603  -207.85966 -207.86578 -230.7986  -292.45972\n",
      "  -263.83994 -282.53384 -281.1337 ]\n",
      " [-354.96182 -343.3843  -343.82828 -341.56882 -380.12585 -483.731\n",
      "  -437.09418 -465.83514 -466.65668]\n",
      " [-350.35638 -338.4609  -339.23438 -336.2269  -374.38162 -477.04575\n",
      "  -431.026   -458.70322 -460.2798 ]\n",
      " [-153.122   -148.63081 -148.57327 -148.2071  -164.7402  -209.2961\n",
      "  -188.9404  -201.72273 -201.42395]]\n",
      "J tensor([-408.9090, -207.8597, -341.5688, -336.2269, -148.2071])\n",
      "P tensor([-525.1431, -263.8399, -437.0942, -431.0260, -188.9404])\n",
      "expected_state_action_values  tensor([[-421.5277, -526.1384],\n",
      "        [-228.4335, -278.8157],\n",
      "        [-350.4485, -436.4214],\n",
      "        [-350.5634, -435.8826],\n",
      "        [-154.8180, -191.4779]])\n",
      "reward -43.78575281877071\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  9.6986, 18.5841,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [18.5462,  3.2197, 18.7128, 14.9720,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  8.1389, 20.1278,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  7.7877, 12.1854,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-25.4330],\n",
      "        [-60.7464],\n",
      "        [-42.4287],\n",
      "        [-51.4207],\n",
      "        [-33.9160]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [1, 7],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-181.1266, -167.9565, -174.4150, -167.1419, -185.8043, -236.1528,\n",
      "         -219.7440, -221.9621, -226.1242],\n",
      "        [-498.5408, -460.9309, -479.4054, -457.9676, -509.6328, -648.4199,\n",
      "         -603.9633, -609.2913, -622.2051],\n",
      "        [-252.2581, -234.5600, -243.5283, -233.8647, -259.8036, -329.3893,\n",
      "         -306.5170, -310.3482, -315.2201],\n",
      "        [-468.5844, -432.5789, -449.9699, -428.7635, -477.5094, -608.6030,\n",
      "         -566.9528, -571.0099, -584.4811],\n",
      "        [-270.1284, -250.1423, -260.0859, -248.9090, -276.8164, -351.9057,\n",
      "         -327.5846, -330.7400, -337.2001]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-174.4150, -221.9621],\n",
      "        [-460.9309, -603.9633],\n",
      "        [-243.5283, -306.5170],\n",
      "        [-432.5789, -571.0099],\n",
      "        [-270.1284, -330.7400]], grad_fn=<GatherBackward>)\n",
      "A [[-180.7771  -167.61736 -174.12498 -166.99718 -185.58298 -235.79802\n",
      "  -219.37523 -221.65887 -225.727  ]\n",
      " [-527.36127 -487.55292 -507.13943 -484.4901  -539.1262  -685.92065\n",
      "  -638.8815  -644.5463  -658.20197]\n",
      " [-278.34784 -258.4111  -268.4197  -257.34647 -286.04086 -363.05438\n",
      "  -337.92123 -341.75568 -347.68106]\n",
      " [-456.82043 -421.86505 -438.58765 -417.82416 -465.43225 -593.1682\n",
      "  -552.7449  -556.6996  -569.70056]\n",
      " [-274.217   -253.52393 -263.5372  -251.58345 -280.02432 -356.61597\n",
      "  -332.07507 -334.729   -342.10776]]\n",
      "J tensor([-166.9972, -484.4901, -257.3465, -417.8242, -251.5835])\n",
      "P tensor([-219.3752, -638.8815, -337.9212, -552.7449, -332.0751])\n",
      "expected_state_action_values  tensor([[-175.7305, -222.8707],\n",
      "        [-496.7875, -635.7397],\n",
      "        [-274.0405, -346.5578],\n",
      "        [-427.4624, -548.8911],\n",
      "        [-260.3411, -332.7836]])\n",
      "reward -42.64195529735776\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[18.5462,  3.2197, 18.7128, 14.9720,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.6949,  3.2197, 16.1138,  6.6162,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  3.2406, 11.1610,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[19.9728,  6.4395, 19.5042,  8.9866,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 13.9035,  8.5325,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  8.5990,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-57.4508],\n",
      "        [-35.6447],\n",
      "        [-27.4673],\n",
      "        [-25.6968],\n",
      "        [ -3.7268]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [3, 6],\n",
      "        [4, 6],\n",
      "        [3, 6],\n",
      "        [3, 7]])\n",
      "policy_net1(state_batch) tensor([[-508.9507, -513.9009, -532.3065, -489.1245, -544.4649, -693.1938,\n",
      "         -665.1099, -636.2450, -662.1174],\n",
      "        [-295.3209, -298.9286, -309.0401, -284.2432, -316.3928, -402.4945,\n",
      "         -386.2641, -369.7955, -384.2557],\n",
      "        [-171.7215, -174.5046, -180.2617, -166.6024, -185.1679, -234.8988,\n",
      "         -225.3107, -216.2507, -223.6531],\n",
      "        [-168.6624, -171.3081, -176.9646, -163.5963, -181.7936, -230.6824,\n",
      "         -221.2253, -212.3077, -219.6995],\n",
      "        [ -47.8899,  -48.7916,  -49.9384,  -45.9759,  -51.3376,  -65.3079,\n",
      "          -62.8304,  -60.0144,  -62.0011]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-532.3065, -665.1099],\n",
      "        [-284.2432, -386.2641],\n",
      "        [-185.1679, -225.3107],\n",
      "        [-163.5963, -221.2253],\n",
      "        [ -45.9759,  -60.0144]], grad_fn=<GatherBackward>)\n",
      "A [[-497.42813  -503.24316  -520.9266   -479.4637   -533.4518   -678.2695\n",
      "  -650.82855  -623.36444  -647.524   ]\n",
      " [-315.91476  -319.4211   -330.29767  -303.4236   -337.88754  -430.17767\n",
      "  -412.88583  -394.97446  -410.88348 ]\n",
      " [-183.58923  -186.59581  -192.85246  -178.23874  -198.04245  -251.24002\n",
      "  -240.8629   -231.22491  -239.18506 ]\n",
      " [-181.52866  -184.17937  -190.38939  -175.75087  -195.3473   -248.12653\n",
      "  -237.87231  -228.11206  -236.40347 ]\n",
      " [ -49.270016  -50.181206  -51.37573   -47.286057  -52.800503  -67.17738\n",
      "   -64.62682   -61.726753  -63.792957]]\n",
      "J tensor([-479.4637, -303.4236, -178.2387, -175.7509,  -47.2861])\n",
      "P tensor([-623.3644, -394.9745, -231.2249, -228.1121,  -61.7268])\n",
      "expected_state_action_values  tensor([[-488.9681, -618.4787],\n",
      "        [-308.7259, -391.1217],\n",
      "        [-187.8822, -235.5697],\n",
      "        [-183.8726, -230.9976],\n",
      "        [ -46.2843,  -59.2809]])\n",
      "5\n",
      "reward -49.61302567484107\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 11.4544, 12.2717,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.4549,  4.3198,  7.2281,  8.3536,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  4.8914,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  9.3189, 16.0605,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[11.5238,  6.1826,  9.8826,  8.4006,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  5.0432,  7.1056,  5.8941,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.3138,  9.1767,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.8490,  6.4395,  8.2194, 10.9147,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197,  9.2441,  8.4561,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-40.3589],\n",
      "        [-26.3564],\n",
      "        [-24.1008],\n",
      "        [-36.3057],\n",
      "        [-39.9305]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 7],\n",
      "        [2, 6],\n",
      "        [4, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-313.5034, -317.8390, -316.5126, -322.8213, -354.2863, -428.6359,\n",
      "         -422.0546, -383.1608, -408.5533],\n",
      "        [-212.3183, -215.5385, -214.3157, -218.6981, -240.0097, -290.3571,\n",
      "         -285.9031, -259.6180, -276.6640],\n",
      "        [-186.4594, -189.7804, -188.6190, -193.0311, -211.6152, -255.5664,\n",
      "         -251.5448, -228.7983, -243.1873],\n",
      "        [-260.2589, -264.5326, -263.1012, -268.7461, -294.8123, -356.2696,\n",
      "         -350.7997, -318.8392, -339.3149],\n",
      "        [-296.9198, -301.2675, -299.8416, -306.0629, -335.8529, -406.0899,\n",
      "         -399.9584, -363.2502, -386.9939]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-317.8390, -422.0546],\n",
      "        [-215.5385, -259.6180],\n",
      "        [-188.6190, -251.5448],\n",
      "        [-294.8123, -350.7997],\n",
      "        [-299.8416, -399.9584]], grad_fn=<GatherBackward>)\n",
      "A [[-333.0853  -338.27887 -336.58585 -343.7725  -377.15073 -455.7547\n",
      "  -448.89053 -407.92157 -434.28104]\n",
      " [-192.82724 -196.19022 -194.89214 -199.21762 -218.52293 -264.0239\n",
      "  -260.00732 -236.37152 -251.35387]\n",
      " [-183.86227 -187.40686 -186.05638 -190.61943 -208.94829 -252.09514\n",
      "  -248.25986 -225.96448 -239.8547 ]\n",
      " [-267.6095  -272.8041  -270.96976 -277.44403 -304.1655  -366.85162\n",
      "  -361.35754 -328.9723  -349.1774 ]\n",
      " [-315.13867 -319.82352 -318.43625 -325.15924 -356.70322 -431.22733\n",
      "  -424.57553 -385.72153 -410.85168]]\n",
      "J tensor([-333.0853, -192.8272, -183.8623, -267.6095, -315.1387])\n",
      "P tensor([-407.9216, -236.3715, -225.9645, -328.9723, -385.7215])\n",
      "expected_state_action_values  tensor([[-340.1357, -407.4883],\n",
      "        [-199.9009, -239.0908],\n",
      "        [-189.5769, -227.4688],\n",
      "        [-277.1543, -332.3808],\n",
      "        [-323.5553, -387.0799]])\n",
      "1\n",
      "reward -47.82624306378484\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.1096, 28.4325,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  3.2406, 12.5768,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.7183,  6.5877, 26.1786,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880, 13.7230,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  3.2197, 12.8733, 10.7124,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-48.4121],\n",
      "        [-44.4684],\n",
      "        [-29.9633],\n",
      "        [-43.9882],\n",
      "        [-80.0695]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [4, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-380.9684, -400.7453, -398.8406, -390.6872, -409.3267, -519.9976,\n",
      "         -497.5233, -448.9692, -498.4095],\n",
      "        [-252.4971, -268.5399, -266.4999, -263.7122, -275.3044, -347.2570,\n",
      "         -332.0512, -301.6285, -331.4036],\n",
      "        [-194.6691, -206.2595, -204.8780, -202.1467, -211.2552, -267.1797,\n",
      "         -255.4225, -231.4773, -255.1725],\n",
      "        [-347.3311, -365.9920, -363.9813, -357.3293, -374.0919, -474.6414,\n",
      "         -454.1143, -410.2868, -454.7102],\n",
      "        [-598.5840, -631.8050, -628.6555, -618.3477, -646.7725, -819.3166,\n",
      "         -783.6687, -709.0610, -784.4224]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-398.8406, -497.5233],\n",
      "        [-275.3044, -301.6285],\n",
      "        [-206.2595, -255.4225],\n",
      "        [-363.9813, -454.1143],\n",
      "        [-631.8050, -783.6687]], grad_fn=<GatherBackward>)\n",
      "A [[-378.42175 -399.0089  -396.74567 -389.38837 -407.72076 -517.1625\n",
      "  -494.88538 -447.18906 -495.40268]\n",
      " [-242.77582 -258.76566 -256.63486 -254.50392 -265.49857 -334.37637\n",
      "  -319.7342  -290.83252 -318.86703]\n",
      " [-195.554   -207.02402 -205.66617 -202.74252 -211.94937 -268.2188\n",
      "  -256.43628 -232.26558 -256.2517 ]\n",
      " [-362.06708 -381.10693 -379.11545 -371.6423  -389.26703 -494.3477\n",
      "  -472.9696  -426.99142 -473.80475]\n",
      " [-597.8165  -630.5767  -627.47754 -616.69324 -645.2411  -817.7995\n",
      "  -782.2795  -707.46454 -783.198  ]]\n",
      "J tensor([-378.4218, -242.7758, -195.5540, -362.0671, -597.8165])\n",
      "P tensor([-447.1891, -290.8325, -232.2656, -426.9914, -707.4645])\n",
      "expected_state_action_values  tensor([[-388.9916, -450.8822],\n",
      "        [-262.9666, -306.2176],\n",
      "        [-205.9619, -239.0023],\n",
      "        [-369.8485, -428.2804],\n",
      "        [-618.1043, -716.7875]])\n",
      "1\n",
      "reward -48.09447619049699\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 13.2003, 10.0980,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  5.0623, 10.1720,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[12.8397,  6.4395, 10.1104,  9.0302,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.1757,  6.4395, 14.7205, 12.2717,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.1460, 11.5169,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.7648,  6.4395, 10.3665,  7.3454,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-40.9402],\n",
      "        [ -4.6880],\n",
      "        [-46.0041],\n",
      "        [-24.7339],\n",
      "        [-38.6147]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [4, 6],\n",
      "        [1, 7],\n",
      "        [0, 7],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[-304.7461, -307.3741, -308.1602, -312.5995, -311.9633, -416.0978,\n",
      "         -386.5259, -369.1103, -400.4864],\n",
      "        [ -51.6557,  -52.5483,  -52.0374,  -52.9336,  -52.9244,  -70.5579,\n",
      "          -65.6729,  -62.7031,  -67.6598],\n",
      "        [-355.2525, -358.6347, -359.2997, -364.5400, -363.7545, -485.0528,\n",
      "         -450.6135, -430.5130, -467.0036],\n",
      "        [-169.1633, -171.5490, -171.6514, -174.8689, -174.2856, -231.8039,\n",
      "         -215.2041, -206.1005, -222.5163],\n",
      "        [-253.0040, -255.3238, -255.7149, -259.2686, -258.8027, -345.3217,\n",
      "         -320.7940, -306.2945, -332.4280]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-307.3741, -386.5259],\n",
      "        [ -52.9244,  -65.6729],\n",
      "        [-358.6347, -430.5130],\n",
      "        [-169.1633, -206.1005],\n",
      "        [-253.0040, -306.2945]], grad_fn=<GatherBackward>)\n",
      "A [[-338.0734   -340.87302  -341.6197   -346.28732  -345.71085  -461.2409\n",
      "  -428.6437   -409.17627  -444.18497 ]\n",
      " [ -60.131348  -61.060326  -60.560802  -61.48932   -61.49626   -82.04714\n",
      "   -76.36827   -72.86801   -78.77136 ]\n",
      " [-344.32013  -347.34375  -348.13574  -353.1676   -352.40997  -470.0597\n",
      "  -436.66452  -417.05966  -452.5425  ]\n",
      " [-169.97537  -172.49266  -172.34987  -175.66956  -175.07407  -232.80783\n",
      "  -216.22919  -207.15338  -223.62885 ]\n",
      " [-269.36432  -271.7318   -272.34488  -276.15295  -275.59943  -367.7662\n",
      "  -341.51617  -326.08755  -353.97604 ]]\n",
      "J tensor([-338.0734,  -60.1313, -344.3201, -169.9754, -269.3643])\n",
      "P tensor([-409.1763,  -72.8680, -417.0597, -207.1534, -326.0876])\n",
      "expected_state_action_values  tensor([[-345.2063, -409.1989],\n",
      "        [ -58.8062,  -70.2692],\n",
      "        [-355.8922, -421.3578],\n",
      "        [-177.7117, -211.1719],\n",
      "        [-281.0425, -332.0934]])\n",
      "1\n",
      "reward -43.04258914766287\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.0554,  5.7395,  4.8381, 41.4549,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.2276,  6.4395,  7.2726, 25.0270,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.5753,  6.4395,  5.9862,  8.4561,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.0140, 38.1314,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.7182, 26.0510,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.4118, 14.2193,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.2270],\n",
      "        [-60.0879],\n",
      "        [-45.9667],\n",
      "        [-30.9730],\n",
      "        [-30.4570]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [4, 6],\n",
      "        [4, 7],\n",
      "        [2, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-122.2816, -113.3158, -117.7882, -119.5731, -125.2410, -159.3447,\n",
      "         -152.3865, -145.8911, -152.5357],\n",
      "        [-306.2060, -287.1699, -298.3180, -306.8077, -319.6870, -402.3965,\n",
      "         -384.6489, -372.1911, -383.7221],\n",
      "        [-267.1202, -249.7803, -259.5580, -266.1311, -277.6102, -350.2973,\n",
      "         -334.8810, -323.2722, -334.4493],\n",
      "        [-267.0591, -248.3651, -258.3339, -263.1637, -275.1664, -348.7795,\n",
      "         -333.5845, -320.5800, -333.7915],\n",
      "        [-229.3130, -213.3443, -222.1330, -226.5995, -236.7908, -299.9215,\n",
      "         -286.6886, -275.6818, -286.6585]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-113.3158, -152.3865],\n",
      "        [-319.6870, -384.6489],\n",
      "        [-277.6102, -323.2722],\n",
      "        [-258.3339, -333.5845],\n",
      "        [-222.1330, -286.6886]], grad_fn=<GatherBackward>)\n",
      "A [[-167.69415 -155.2146  -161.64513 -163.80301 -171.61028 -218.44824\n",
      "  -208.86577 -199.87357 -209.20363]\n",
      " [-313.82574 -294.31442 -305.95453 -314.88153 -327.97482 -412.69266\n",
      "  -394.34564 -381.70462 -393.37866]\n",
      " [-272.78867 -254.76016 -264.90268 -271.19855 -283.00797 -357.50745\n",
      "  -341.6903  -329.52655 -341.453  ]\n",
      " [-234.76959 -218.29066 -227.06264 -231.35541 -241.89609 -306.64078\n",
      "  -293.25626 -281.79163 -293.40045]\n",
      " [-217.82614 -202.1214  -210.55276 -214.1401  -224.02017 -284.36438\n",
      "  -271.86368 -260.85742 -272.0608 ]]\n",
      "J tensor([-155.2146, -294.3144, -254.7602, -218.2907, -202.1214])\n",
      "P tensor([-199.8736, -381.7046, -329.5266, -281.7916, -260.8574])\n",
      "expected_state_action_values  tensor([[-158.9202, -199.1132],\n",
      "        [-324.9709, -403.6220],\n",
      "        [-275.2508, -342.5406],\n",
      "        [-227.4346, -284.5855],\n",
      "        [-212.3662, -265.2286]])\n",
      "reward -45.198434714897886\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  4.7183,  6.5877, 26.1786,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.1998,  6.4395,  6.1390,  9.1287,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 16.8623, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.6855,  6.4395,  6.1674, 31.5733,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  6.7200, 11.8436,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-32.6959],\n",
      "        [-45.1912],\n",
      "        [-31.9069],\n",
      "        [-50.3626],\n",
      "        [-81.8207]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [4, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [0, 6]])\n",
      "policy_net1(state_batch) tensor([[-291.4066, -282.3045, -270.7090, -286.4619, -313.5034, -380.1080,\n",
      "         -354.0121, -359.6893, -363.5799],\n",
      "        [-299.4695, -292.6553, -280.0862, -299.5450, -326.6577, -393.1855,\n",
      "         -365.8985, -374.5575, -374.4530],\n",
      "        [-246.3526, -239.0441, -229.2816, -243.3712, -266.0652, -322.0056,\n",
      "         -299.7892, -305.1149, -307.4497],\n",
      "        [-424.9551, -411.0703, -394.5465, -417.4663, -456.8841, -554.0217,\n",
      "         -516.0825, -524.2004, -530.2575],\n",
      "        [-585.9138, -568.9997, -545.7027, -580.0131, -633.8065, -765.9963,\n",
      "         -713.3295, -727.0344, -731.9102]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-270.7090, -359.6893],\n",
      "        [-326.6577, -365.8985],\n",
      "        [-229.2816, -299.7892],\n",
      "        [-411.0703, -516.0825],\n",
      "        [-585.9138, -713.3295]], grad_fn=<GatherBackward>)\n",
      "A [[-271.55774 -263.00885 -252.08739 -266.47632 -291.77798 -353.95447\n",
      "  -329.7643  -334.8424  -338.70425]\n",
      " [-265.5434  -259.39075 -248.17639 -265.175   -289.3043  -348.41815\n",
      "  -324.3253  -331.77667 -331.9089 ]\n",
      " [-225.83545 -218.93837 -210.04951 -222.74092 -243.59253 -295.03354\n",
      "  -274.6856  -279.33813 -281.75427]\n",
      " [-447.30167 -432.92383 -415.44342 -439.76797 -481.22305 -583.3132\n",
      "  -543.36475 -552.13477 -558.21326]\n",
      " [-621.5939  -603.01245 -578.63446 -614.30365 -671.4787  -812.19275\n",
      "  -756.2972  -770.2161  -776.33496]]\n",
      "J tensor([-252.0874, -248.1764, -210.0495, -415.4434, -578.6345])\n",
      "P tensor([-329.7643, -324.3253, -274.6856, -543.3647, -756.2972])\n",
      "expected_state_action_values  tensor([[-259.5745, -329.4837],\n",
      "        [-268.5499, -337.0840],\n",
      "        [-220.9515, -279.1239],\n",
      "        [-424.2617, -539.3909],\n",
      "        [-602.5917, -762.4881]])\n",
      "reward -37.13945513671985\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[13.2982,  3.2197, 24.6231,  8.2797,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [17.1196,  3.2197, 15.5834,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [17.1196,  6.4395, 15.8138,  6.7723,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-51.4207],\n",
      "        [-46.5218],\n",
      "        [ -3.7268],\n",
      "        [-21.4316],\n",
      "        [ -3.7268]])\n",
      "action_batch tensor([[1, 7],\n",
      "        [1, 6],\n",
      "        [3, 7],\n",
      "        [0, 7],\n",
      "        [4, 6]])\n",
      "policy_net1(state_batch) tensor([[-497.5251, -477.4749, -422.3788, -463.6077, -484.6166, -618.0718,\n",
      "         -591.8301, -567.9794, -591.2007],\n",
      "        [-460.1796, -441.4859, -390.7003, -429.2130, -448.5024, -571.9079,\n",
      "         -547.5061, -525.5333, -546.8680],\n",
      "        [ -51.3637,  -49.7048,  -43.4804,  -47.8692,  -50.1566,  -63.9117,\n",
      "          -61.3334,  -58.7992,  -60.7491],\n",
      "        [-160.2140, -154.2828, -136.1531, -150.1296, -156.7460, -199.5577,\n",
      "         -190.9657, -183.5973, -190.3436],\n",
      "        [ -58.2972,  -56.4506,  -49.3444,  -54.4727,  -56.9826,  -72.5637,\n",
      "          -69.5826,  -66.8337,  -69.0633]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-477.4749, -567.9794],\n",
      "        [-441.4859, -547.5061],\n",
      "        [ -47.8692,  -58.7992],\n",
      "        [-160.2140, -183.5973],\n",
      "        [ -56.9826,  -69.5826]], grad_fn=<GatherBackward>)\n",
      "A [[-484.52835  -465.14734  -411.25687  -451.3297   -471.86557  -601.7689\n",
      "  -576.3908   -553.1607   -575.6547  ]\n",
      " [-456.20715  -438.0636   -387.40887  -426.05664  -445.09708  -567.143\n",
      "  -543.04425  -521.5889   -542.1853  ]\n",
      " [ -52.8289    -51.107544  -44.721355  -49.220154  -51.57289   -65.72428\n",
      "   -63.071312  -60.461433  -62.488773]\n",
      " [-181.0498   -174.64635  -154.01276  -170.31595  -177.6605   -225.74835\n",
      "  -216.05708  -208.09091  -215.1961  ]\n",
      " [ -59.762386  -57.85333   -50.585354  -55.823605  -58.398895  -74.37625\n",
      "   -71.32055   -68.495895  -70.803   ]]\n",
      "J tensor([-411.2569, -387.4089,  -44.7214, -154.0128,  -50.5854])\n",
      "P tensor([-553.1607, -521.5889,  -60.4614, -208.0909,  -68.4959])\n",
      "expected_state_action_values  tensor([[-421.5519, -549.2653],\n",
      "        [-395.1898, -515.9518],\n",
      "        [ -43.9760,  -58.1421],\n",
      "        [-160.0431, -208.7134],\n",
      "        [ -49.2536,  -65.3731]])\n",
      "reward -36.73798629963237\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  6.9588,  7.0048,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  5.0623, 10.1720,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 13.9035,  8.5325,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  5.8472,  5.9286,  9.1005,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  4.1460, 11.5169,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395, 14.1121, 13.4330,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.8037,  5.2162, 15.6465,  9.3219,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-80.0695],\n",
      "        [-23.4633],\n",
      "        [-24.7339],\n",
      "        [-34.7889],\n",
      "        [-45.2304]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [0, 7],\n",
      "        [0, 7],\n",
      "        [4, 6],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-605.7814, -583.3411, -537.2162, -558.1900, -589.4913, -785.7888,\n",
      "         -730.8415, -701.0231, -753.6649],\n",
      "        [-192.9167, -185.9296, -170.9837, -177.6578, -187.6516, -250.3231,\n",
      "         -232.7541, -223.1363, -239.8095],\n",
      "        [-177.9373, -171.7643, -157.9395, -164.3546, -173.5359, -231.1910,\n",
      "         -214.9253, -206.2559, -221.2456],\n",
      "        [-333.4128, -320.8889, -295.3261, -306.1596, -323.5988, -432.0127,\n",
      "         -401.8607, -384.9296, -414.5397],\n",
      "        [-366.7116, -352.3392, -324.4821, -335.8835, -355.1855, -474.6759,\n",
      "         -441.6361, -422.5561, -455.8065]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-583.3411, -730.8415],\n",
      "        [-192.9167, -223.1363],\n",
      "        [-177.9373, -206.2559],\n",
      "        [-323.5988, -401.8607],\n",
      "        [-352.3392, -441.6361]], grad_fn=<GatherBackward>)\n",
      "A [[-605.2118  -582.4702  -536.4849  -556.93005 -588.3361  -784.66547\n",
      "  -729.8677  -699.7112  -752.8234 ]\n",
      " [-172.18967 -166.00293 -152.47968 -158.29851 -167.30618 -223.25041\n",
      "  -207.70471 -199.03288 -213.9481 ]\n",
      " [-179.9662  -173.82732 -159.59079 -166.16356 -175.45216 -233.69823\n",
      "  -217.3528  -208.6577  -223.79431]\n",
      " [-289.37076 -278.35797 -256.051   -265.12393 -280.39774 -374.5979\n",
      "  -348.60602 -333.64764 -359.62402]\n",
      " [-369.77475 -355.24075 -327.25653 -338.87265 -358.27545 -478.76535\n",
      "  -445.36124 -426.16956 -459.6582 ]]\n",
      "J tensor([-536.4849, -152.4797, -159.5908, -256.0510, -327.2565])\n",
      "P tensor([-699.7112, -199.0329, -208.6577, -333.6476, -426.1696])\n",
      "expected_state_action_values  tensor([[-562.9059, -709.8095],\n",
      "        [-160.6950, -202.5929],\n",
      "        [-168.3656, -212.5258],\n",
      "        [-265.2348, -335.0718],\n",
      "        [-339.7613, -428.7830]])\n",
      "reward -37.79103524890038\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.0690,  6.4395,  6.5522, 21.2992,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 22.2047,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 4.6263,  6.4395,  4.0320,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 16.9802, 14.7991,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.3431,  6.4395,  7.1075, 17.9512,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.3440,  3.2197, 24.5665, 10.5098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197, 11.0652, 14.6361,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  7.8173,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.4285,  6.4395, 17.0893,  9.5668,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-41.3598],\n",
      "        [-53.5095],\n",
      "        [-35.7833],\n",
      "        [-25.6968],\n",
      "        [-48.4121]])\n",
      "action_batch tensor([[0, 6],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [3, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-200.4783, -194.7425, -186.4784, -194.2716, -195.2995, -272.7727,\n",
      "         -246.2395, -238.8598, -262.1562],\n",
      "        [-416.4693, -401.4539, -385.3010, -397.8002, -401.0800, -563.1110,\n",
      "         -509.1227, -490.9938, -543.5779],\n",
      "        [-216.2771, -209.7135, -200.7470, -208.4911, -209.8584, -293.5744,\n",
      "         -265.2635, -256.8314, -282.5715],\n",
      "        [-150.1918, -145.4919, -139.3435, -144.9819, -145.8510, -204.0399,\n",
      "         -184.2567, -178.3768, -196.2322],\n",
      "        [-351.2214, -338.4118, -325.0902, -335.7675, -338.4586, -475.1692,\n",
      "         -429.4544, -414.1382, -458.3992]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-200.4783, -246.2395],\n",
      "        [-385.3010, -509.1227],\n",
      "        [-200.7470, -265.2635],\n",
      "        [-144.9819, -184.2567],\n",
      "        [-325.0902, -429.4544]], grad_fn=<GatherBackward>)\n",
      "A [[-208.40329 -202.6188  -194.07936 -202.42729 -203.4345  -283.83832\n",
      "  -256.21365 -248.74147 -272.58768]\n",
      " [-417.01172 -401.57108 -385.73828 -398.09537 -401.36307 -563.7876\n",
      "  -509.6044  -491.22418 -544.2436 ]\n",
      " [-233.35452 -225.94414 -216.49101 -224.77487 -226.24425 -316.64993\n",
      "  -286.08713 -276.84085 -304.86752]\n",
      " [-159.84853 -154.69557 -148.29713 -153.99956 -154.97636 -217.03531\n",
      "  -195.9389  -189.50935 -208.79955]\n",
      " [-349.62836 -337.6491  -323.98727 -335.39273 -337.88586 -473.5946\n",
      "  -428.04932 -413.4792  -456.58157]]\n",
      "J tensor([-194.0794, -385.7383, -216.4910, -148.2971, -323.9873])\n",
      "P tensor([-248.7415, -491.2242, -276.8409, -189.5094, -413.4792])\n",
      "expected_state_action_values  tensor([[-216.0312, -265.2271],\n",
      "        [-400.6740, -495.6113],\n",
      "        [-230.6252, -284.9400],\n",
      "        [-159.1642, -196.2552],\n",
      "        [-340.0006, -420.5433]])\n",
      "reward -43.13746606879342\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[19.9728,  6.4395, 23.7921, 22.3220,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 10.5880, 11.7574,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  3.2197, 21.3259,  9.4774,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 16.1990,  9.5117,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  7.2726, 22.3220,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[21.3995,  5.5751, 19.4536, 25.0270,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  9.0045, 11.0735,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 20.5345,  9.3674,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.6949,  3.2197, 16.1138,  6.6162,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.2276,  6.4395,  7.2726, 25.0270,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-74.5264],\n",
      "        [-38.9782],\n",
      "        [-47.4361],\n",
      "        [-38.0636],\n",
      "        [-43.7406]])\n",
      "action_batch tensor([[0, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [1, 6],\n",
      "        [0, 6]])\n",
      "policy_net1(state_batch) tensor([[-568.6024, -525.6404, -524.9395, -553.6793, -526.4161, -737.4865,\n",
      "         -648.2197, -645.0388, -709.2905],\n",
      "        [-302.1276, -279.0751, -278.7806, -293.6989, -279.3964, -391.7797,\n",
      "         -344.3649, -342.2480, -376.6552],\n",
      "        [-426.4780, -393.3623, -392.8775, -412.6216, -392.9309, -551.9225,\n",
      "         -485.4155, -481.5995, -531.5412],\n",
      "        [-308.6428, -284.9140, -284.2496, -298.5499, -284.3651, -399.3840,\n",
      "         -351.3616, -348.6102, -384.5863],\n",
      "        [-259.2751, -241.2859, -240.3235, -255.1123, -242.0462, -337.6843,\n",
      "         -296.5287, -296.5256, -323.6290]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-568.6024, -645.0388],\n",
      "        [-278.7806, -344.3649],\n",
      "        [-392.8775, -485.4155],\n",
      "        [-284.9140, -351.3616],\n",
      "        [-259.2751, -296.5287]], grad_fn=<GatherBackward>)\n",
      "A [[-585.2695  -540.6725  -539.98175 -568.70526 -540.98663 -758.5223\n",
      "  -666.8572  -662.97437 -729.90265]\n",
      " [-311.49588 -287.91824 -287.4111  -302.90854 -288.1581  -403.89108\n",
      "  -355.11627 -353.0723  -388.34134]\n",
      " [-410.12326 -378.50574 -377.79242 -396.86606 -377.94604 -530.7191\n",
      "  -466.87863 -463.3277  -511.13956]\n",
      " [-313.60654 -289.79105 -289.0225  -303.89337 -289.3438  -406.06198\n",
      "  -357.18997 -354.71124 -390.8506 ]\n",
      " [-248.43407 -230.92702 -230.16174 -243.96669 -231.6098  -323.36978\n",
      "  -284.03024 -283.70157 -309.9618 ]]\n",
      "J tensor([-539.9818, -287.4111, -377.7924, -289.0225, -230.1617])\n",
      "P tensor([-662.9744, -353.0723, -463.3277, -354.7112, -283.7016])\n",
      "expected_state_action_values  tensor([[-560.5099, -671.2033],\n",
      "        [-297.6482, -356.7432],\n",
      "        [-387.4493, -464.4310],\n",
      "        [-298.1838, -357.3037],\n",
      "        [-250.8862, -299.0720]])\n",
      "reward -43.030881390104554\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  3.9703,  1.1107,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  5.5108,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 18.1602, 26.5328,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  3.4794, 36.3310,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  6.3968,  6.6019,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.1834, 25.0816,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 6.0554,  5.7395,  4.8381, 41.4549,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-19.2270],\n",
      "        [-25.0361],\n",
      "        [-71.3122],\n",
      "        [-30.9730],\n",
      "        [-52.1633]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [4, 7],\n",
      "        [1, 6],\n",
      "        [2, 6],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-115.6861, -116.7504, -107.1042, -117.2689, -111.8131, -156.7250,\n",
      "         -142.0922, -141.4342, -150.1077],\n",
      "        [-164.0985, -166.2264, -152.3493, -167.2770, -159.3076, -222.7116,\n",
      "         -201.9180, -201.6061, -213.1504],\n",
      "        [-542.2292, -548.5085, -503.9150, -553.4487, -526.7461, -735.9733,\n",
      "         -667.0653, -666.5042, -705.0612],\n",
      "        [-255.0908, -258.5537, -237.0829, -260.6169, -248.0472, -346.4117,\n",
      "         -314.0295, -313.9232, -331.6100],\n",
      "        [-299.3448, -306.3365, -280.2599, -311.5073, -295.3481, -409.5095,\n",
      "         -370.7493, -373.5993, -390.2282]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-116.7504, -142.0922],\n",
      "        [-159.3076, -201.6061],\n",
      "        [-548.5085, -667.0653],\n",
      "        [-237.0829, -314.0295],\n",
      "        [-295.3481, -373.5993]], grad_fn=<GatherBackward>)\n",
      "A [[-158.89014 -160.16493 -147.21951 -160.88107 -153.43567 -215.17075\n",
      "  -195.05058 -194.05505 -206.16544]\n",
      " [-179.29855 -181.75569 -166.60893 -183.25052 -174.40201 -243.5792\n",
      "  -220.77957 -220.67589 -233.0167 ]\n",
      " [-552.3866  -559.1083  -513.42175 -564.22974 -536.9443  -749.90173\n",
      "  -679.75916 -679.4663  -718.3945 ]\n",
      " [-223.97972 -226.97002 -208.13794 -228.8454  -217.799   -304.1965\n",
      "  -275.73773 -275.60913 -291.1391 ]\n",
      " [-273.3189  -279.6589  -255.80812 -284.41553 -269.66486 -373.8768\n",
      "  -338.53952 -341.12146 -356.2731 ]]\n",
      "J tensor([-147.2195, -166.6089, -513.4218, -208.1379, -255.8081])\n",
      "P tensor([-194.0551, -220.6759, -679.4663, -275.6091, -338.5395])\n",
      "expected_state_action_values  tensor([[-151.7246, -193.8766],\n",
      "        [-174.9841, -223.6444],\n",
      "        [-533.3918, -682.8319],\n",
      "        [-218.2972, -279.0212],\n",
      "        [-282.3906, -356.8488]])\n",
      "reward -36.1995328091042\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  6.1448, 10.2561,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.3509, 14.1056,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.2538,  6.4395, 21.5328, 27.8434,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  6.4395, 21.3190, 12.4147,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  4.8983,  9.9410,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [15.6929,  6.4395, 12.2431, 13.5800,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [14.2663,  8.9138, 22.0666,  5.8941,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-38.6657],\n",
      "        [-27.3272],\n",
      "        [-46.9426],\n",
      "        [-80.0695],\n",
      "        [-56.4395]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [1, 6],\n",
      "        [2, 7]])\n",
      "policy_net1(state_batch) tensor([[-256.5900, -248.1592, -228.3817, -260.5389, -235.9444, -347.8363,\n",
      "         -323.7358, -304.5044, -333.5488],\n",
      "        [-209.2385, -202.9476, -186.4488, -213.2630, -192.9724, -284.0854,\n",
      "         -264.3145, -249.0499, -272.1864],\n",
      "        [-381.3894, -369.1259, -339.5254, -387.6055, -350.8782, -517.0349,\n",
      "         -481.2483, -452.9990, -496.0583],\n",
      "        [-580.0834, -562.3564, -517.0712, -591.3470, -534.9800, -787.2306,\n",
      "         -732.7482, -690.6866, -754.9791],\n",
      "        [-420.0281, -405.3131, -373.1600, -423.8224, -384.2946, -567.8293,\n",
      "         -528.7755, -496.3490, -545.5963]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-228.3817, -323.7358],\n",
      "        [-186.4488, -264.3145],\n",
      "        [-381.3894, -452.9990],\n",
      "        [-562.3564, -732.7482],\n",
      "        [-373.1600, -496.3490]], grad_fn=<GatherBackward>)\n",
      "A [[-290.65836 -281.16595 -258.7411  -295.2454  -267.341   -394.04242\n",
      "  -366.75348 -345.052   -377.90036]\n",
      " [-198.01923 -192.35776 -176.44952 -201.98903 -182.77168 -268.8755\n",
      "  -250.28616 -235.98067 -257.60522]\n",
      " [-369.654   -358.06326 -329.0841  -375.86798 -340.2454  -501.15768\n",
      "  -466.59323 -439.36807 -480.81967]\n",
      " [-579.6091  -561.5603  -516.41656 -590.0396  -533.9686  -786.1527\n",
      "  -731.8228  -689.4358  -754.16986]\n",
      " [-429.82162 -415.63348 -382.2861  -435.2964  -394.39365 -581.84045\n",
      "  -541.80206 -509.39407 -558.68536]]\n",
      "J tensor([-258.7411, -176.4495, -329.0841, -516.4166, -382.2861])\n",
      "P tensor([-345.0520, -235.9807, -439.3681, -689.4358, -509.3941])\n",
      "expected_state_action_values  tensor([[-271.5327, -349.2125],\n",
      "        [-186.1318, -239.7098],\n",
      "        [-343.1183, -442.3739],\n",
      "        [-544.8444, -700.5616],\n",
      "        [-400.4970, -514.8942]])\n",
      "reward -33.20634054239278\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.8490,  6.4395,  8.2194, 10.9147,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 13.9035,  8.5325,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.8952,  7.8252, 21.0683,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 9.9864,  3.2197,  4.3327,  8.5990,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 10.1302, 11.1610,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395, 14.1121, 13.4330,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  6.4812, 17.3957,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.6897, 12.2717,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-27.4673],\n",
      "        [-34.4226],\n",
      "        [-34.7889],\n",
      "        [-39.0686],\n",
      "        [-27.9424]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [4, 6],\n",
      "        [4, 6],\n",
      "        [1, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-185.6945, -180.7329, -180.6235, -198.0642, -179.5070, -263.9603,\n",
      "         -238.7740, -237.1084, -253.8712],\n",
      "        [-237.4476, -231.0537, -230.7992, -252.4862, -228.9602, -337.0569,\n",
      "         -304.9005, -302.5639, -324.6149],\n",
      "        [-291.4944, -283.5489, -283.2778, -309.7221, -280.9015, -413.5918,\n",
      "         -374.1985, -371.2686, -398.4961],\n",
      "        [-172.8462, -169.3842, -168.7620, -185.8631, -168.2174, -246.5272,\n",
      "         -222.9508, -222.2597, -236.6966],\n",
      "        [-186.3338, -181.6340, -181.4662, -199.0354, -180.3434, -265.0818,\n",
      "         -239.6929, -238.1902, -254.8305]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-180.7329, -238.7740],\n",
      "        [-228.9602, -304.9005],\n",
      "        [-280.9015, -374.1985],\n",
      "        [-169.3842, -222.9508],\n",
      "        [-181.4662, -239.6929]], grad_fn=<GatherBackward>)\n",
      "A [[-213.03265 -206.96625 -206.83131 -226.26607 -205.2168  -302.2308\n",
      "  -273.45837 -271.16714 -291.13522]\n",
      " [-218.1851  -212.42816 -212.18033 -232.17937 -210.54398 -309.8448\n",
      "  -280.2915  -278.20312 -298.25626]\n",
      " [-253.51627 -246.45235 -246.07274 -268.70322 -243.86116 -359.3003\n",
      "  -325.2478  -322.4084  -346.35526]\n",
      " [-208.77228 -204.52908 -203.83917 -224.41725 -203.11061 -297.69485\n",
      "  -269.2044  -268.36786 -285.99094]\n",
      " [-172.77722 -168.39888 -168.12474 -184.35078 -167.09787 -245.64038\n",
      "  -222.2341  -220.76167 -236.20415]]\n",
      "J tensor([-205.2168, -210.5440, -243.8612, -203.1106, -167.0979])\n",
      "P tensor([-271.1671, -278.2031, -322.4084, -268.3679, -220.7617])\n",
      "expected_state_action_values  tensor([[-212.1624, -271.5177],\n",
      "        [-223.9122, -284.8054],\n",
      "        [-254.2639, -324.9565],\n",
      "        [-221.8682, -280.5997],\n",
      "        [-178.3305, -226.6279]])\n",
      "reward -37.14699828258604\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  6.4395,  2.6880, 13.7230,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [22.8261,  6.4395, 19.7430, 30.8121,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  7.5863, 10.1950,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  3.8170, 16.3308,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  7.6612, 10.9589,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.6263,  6.4395,  3.2406, 11.1610,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [21.3995,  3.2197, 17.9118, 32.5438,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.9144,  5.0590,  7.1990,  9.5668,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  4.5685, 16.2850,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197, 10.0480, 13.3824,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-30.5570],\n",
      "        [-81.8207],\n",
      "        [-31.9273],\n",
      "        [-37.1470],\n",
      "        [-30.9730]])\n",
      "action_batch tensor([[4, 6],\n",
      "        [0, 6],\n",
      "        [2, 6],\n",
      "        [4, 6],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-147.8951, -150.2000, -138.4317, -158.1439, -135.9254, -210.5249,\n",
      "         -184.8683, -188.8195, -202.9814],\n",
      "        [-474.5024, -480.7990, -443.7918, -506.6866, -435.4075, -674.4778,\n",
      "         -592.5728, -605.2258, -651.7277],\n",
      "        [-184.1469, -186.4522, -171.9554, -195.8688, -168.5036, -261.4489,\n",
      "         -229.7342, -234.1876, -252.5792],\n",
      "        [-179.2025, -182.4238, -167.9319, -192.4677, -165.2423, -255.4507,\n",
      "         -224.3470, -229.6575, -246.1982],\n",
      "        [-213.9875, -216.9625, -199.9531, -228.0474, -196.0969, -303.9905,\n",
      "         -267.1580, -272.6367, -293.6254]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-135.9254, -184.8683],\n",
      "        [-474.5024, -592.5728],\n",
      "        [-171.9554, -229.7342],\n",
      "        [-165.2423, -224.3470],\n",
      "        [-199.9531, -267.1580]], grad_fn=<GatherBackward>)\n",
      "A [[-161.21956 -163.8966  -151.04134 -172.86214 -148.47067 -229.73439\n",
      "  -201.67981 -206.23503 -221.42531]\n",
      " [-504.13806 -510.145   -471.23083 -537.2438  -461.8559  -716.04913\n",
      "  -629.1058  -641.9296  -692.16205]\n",
      " [-189.6522  -191.97894 -177.11732 -201.66862 -173.50113 -269.25397\n",
      "  -236.5598  -241.10825 -260.1259 ]\n",
      " [-193.47159 -196.56529 -181.25304 -207.47078 -178.15585 -275.66467\n",
      "  -242.00552 -247.49832 -265.74966]\n",
      " [-188.30899 -190.8771  -175.92912 -200.69417 -172.57169 -267.54242\n",
      "  -235.10806 -239.89342 -258.35696]]\n",
      "J tensor([-148.4707, -461.8559, -173.5011, -178.1559, -172.5717])\n",
      "P tensor([-201.6798, -629.1058, -236.5598, -242.0055, -235.1081])\n",
      "expected_state_action_values  tensor([[-164.1806, -212.0688],\n",
      "        [-497.4910, -648.0159],\n",
      "        [-188.0783, -244.8311],\n",
      "        [-197.4873, -254.9520],\n",
      "        [-186.2876, -242.5703]])\n",
      "reward -47.04790025031489\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 0.0000,  0.0000,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  9.6986, 18.5841,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  5.6897,  8.8634,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.5005,  6.4395,  8.1867,  7.2458,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.5828,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 0.0000,  0.0000,  1.7268,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  6.4395,  8.1389, 20.1278,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [10.0293,  3.2197,  6.4812,  5.7371,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  7.1502,  6.6508,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[ -4.6880],\n",
      "        [-42.4287],\n",
      "        [-32.9790],\n",
      "        [-28.3724],\n",
      "        [ -3.5828]])\n",
      "action_batch tensor([[3, 7],\n",
      "        [2, 6],\n",
      "        [0, 7],\n",
      "        [0, 6],\n",
      "        [0, 7]])\n",
      "policy_net1(state_batch) tensor([[ -45.3375,  -44.1632,  -38.6695,  -45.8126,  -41.6943,  -61.4431,\n",
      "          -55.7845,  -55.1744,  -58.8541],\n",
      "        [-224.7129, -218.7034, -193.0580, -230.2964, -208.3279, -305.8506,\n",
      "         -276.5424, -275.5450, -293.7425],\n",
      "        [-223.1439, -215.7610, -190.9016, -226.5571, -205.2981, -302.5424,\n",
      "         -273.7493, -271.5966, -291.2505],\n",
      "        [-186.3060, -180.4587, -159.4444, -189.2544, -171.5175, -252.6649,\n",
      "         -228.6516, -226.9498, -243.1497],\n",
      "        [ -30.4692,  -29.8572,  -25.9955,  -30.9713,  -28.1887,  -41.4328,\n",
      "          -37.6381,  -37.2725,  -39.4886]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -45.8126,  -55.1744],\n",
      "        [-193.0580, -276.5424],\n",
      "        [-223.1439, -271.5966],\n",
      "        [-186.3060, -228.6516],\n",
      "        [ -30.4692,  -37.2725]], grad_fn=<GatherBackward>)\n",
      "A [[ -53.61352   -52.126175  -45.724155  -54.07359   -49.211765  -72.58128\n",
      "   -65.88516   -65.138985  -69.63347 ]\n",
      " [-248.21536  -241.126    -213.03085  -253.62914  -229.571    -337.4221\n",
      "  -305.1603   -303.6717   -324.32556 ]\n",
      " [-233.163    -225.59042  -199.68341  -237.2049   -214.8334   -316.43378\n",
      "  -286.15927  -284.11676  -304.46317 ]\n",
      " [-186.11693  -180.07285  -159.3399   -188.67201  -171.11064  -252.29114\n",
      "  -228.28896  -226.32433  -242.7766  ]\n",
      " [ -49.672085  -48.44355   -42.356365  -50.36043   -45.74468   -67.36995\n",
      "   -61.10144   -60.57105   -64.60552 ]]\n",
      "J tensor([ -45.7242, -213.0309, -199.6834, -159.3399,  -42.3564])\n",
      "P tensor([ -65.1390, -303.6717, -284.1168, -226.3243,  -60.5710])\n",
      "expected_state_action_values  tensor([[ -45.8397,  -63.3131],\n",
      "        [-234.1564, -315.7332],\n",
      "        [-212.6941, -288.6841],\n",
      "        [-171.7783, -232.0643],\n",
      "        [ -41.7036,  -58.0968]])\n",
      "reward -46.21262083160008\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 6.0554,  5.7395,  4.8381, 41.4549,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [20.5676,  6.4395, 19.5013, 22.5196,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 1.4266,  0.0000,  3.3406,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [19.9728,  3.2197, 19.6161, 15.9378,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  8.2268,  9.3189, 28.8972,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  3.2197,  6.0140, 38.1314,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [21.3995,  6.4395, 18.7128, 18.8469,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.2900,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [18.5462,  3.2197, 18.7128, 14.9720,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  8.2138, 18.7039,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-60.0879],\n",
      "        [-71.0280],\n",
      "        [ -6.7672],\n",
      "        [-60.7464],\n",
      "        [-54.1495]])\n",
      "action_batch tensor([[4, 6],\n",
      "        [2, 6],\n",
      "        [3, 7],\n",
      "        [1, 6],\n",
      "        [4, 7]])\n",
      "policy_net1(state_batch) tensor([[-268.4329, -275.9756, -252.8521, -292.8228, -263.1489, -384.5351,\n",
      "         -357.6345, -359.5068, -366.9575],\n",
      "        [-498.0885, -504.9438, -465.3012, -531.6172, -479.8341, -706.9738,\n",
      "         -658.1149, -655.5353, -678.0981],\n",
      "        [ -50.1628,  -51.1461,  -46.6657,  -53.1934,  -48.2241,  -71.1045,\n",
      "          -66.3931,  -65.9054,  -67.9023],\n",
      "        [-447.8162, -453.9499, -418.1410, -477.4908, -431.1100, -635.3163,\n",
      "         -591.5794, -589.0811, -609.4893],\n",
      "        [-235.8291, -240.6773, -220.9924, -253.7913, -228.7617, -335.9347,\n",
      "         -312.6895, -312.5661, -321.4540]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-263.1489, -357.6345],\n",
      "        [-465.3012, -658.1149],\n",
      "        [ -53.1934,  -65.9054],\n",
      "        [-453.9499, -591.5794],\n",
      "        [-228.7617, -312.5661]], grad_fn=<GatherBackward>)\n",
      "A [[-273.12405  -280.84253  -257.4808   -298.4295   -268.0781   -391.58426\n",
      "  -364.0331   -366.09717  -373.51352 ]\n",
      " [-508.27158  -515.6777   -475.03403  -543.0902   -490.08057  -721.7517\n",
      "  -671.84784  -669.5538   -692.1319  ]\n",
      " [ -74.090294  -75.27197   -68.95217   -78.54704   -71.136925 -104.95404\n",
      "   -97.91753   -97.2141   -100.44145 ]\n",
      " [-473.66632  -480.13654  -442.2933   -505.10892  -456.02457  -672.0113\n",
      "  -625.7372   -623.1215   -644.70404 ]\n",
      " [-290.84442  -297.15448  -272.98538  -313.93713  -282.7495   -414.87448\n",
      "  -385.90945  -386.20712  -396.79315 ]]\n",
      "J tensor([-257.4808, -475.0340,  -68.9522, -442.2933, -272.9854])\n",
      "P tensor([-364.0331, -669.5538,  -97.2141, -623.1215, -385.9095])\n",
      "expected_state_action_values  tensor([[-291.8206, -387.7177],\n",
      "        [-498.5586, -673.6264],\n",
      "        [ -68.8242,  -94.2599],\n",
      "        [-458.8104, -621.5557],\n",
      "        [-299.8364, -401.4680]])\n",
      "reward -49.98455025703707\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.7065,  4.6340, 13.3505,  7.0048,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  5.8472,  5.9286,  9.1005,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  5.0432,  7.1056,  5.8941,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  4.0320,  5.2239,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  2.6880,  8.5990,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.1372,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.3717,  6.4395,  5.6897,  4.7834,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  6.4395,  2.6880,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-32.6959],\n",
      "        [-28.5828],\n",
      "        [-25.7494],\n",
      "        [-23.4019],\n",
      "        [-22.2133]])\n",
      "action_batch tensor([[2, 7],\n",
      "        [0, 7],\n",
      "        [2, 6],\n",
      "        [2, 6],\n",
      "        [4, 6]])\n",
      "policy_net1(state_batch) tensor([[-273.1370, -288.4739, -265.4834, -307.6633, -276.4748, -387.6733,\n",
      "         -370.9871, -370.4133, -370.3987],\n",
      "        [-194.5545, -205.8230, -189.3747, -220.1176, -197.6021, -276.7256,\n",
      "         -264.6800, -264.6294, -263.9623],\n",
      "        [-179.8644, -189.9042, -174.7762, -202.5897, -182.0696, -255.3721,\n",
      "         -244.3034, -243.8346, -243.8255],\n",
      "        [-170.2273, -180.3060, -165.8172, -193.0059, -173.1945, -242.3692,\n",
      "         -231.7644, -231.8921, -231.0641],\n",
      "        [-142.5542, -150.8428, -138.6607, -160.9713, -144.6387, -202.6364,\n",
      "         -193.9052, -193.7052, -193.2446]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-265.4834, -370.4133],\n",
      "        [-194.5545, -264.6294],\n",
      "        [-174.7762, -244.3034],\n",
      "        [-165.8172, -231.7644],\n",
      "        [-144.6387, -193.9052]], grad_fn=<GatherBackward>)\n",
      "A [[-255.09566 -269.33096 -247.76265 -286.83755 -257.88666 -361.77603\n",
      "  -346.31964 -345.5876  -345.78317]\n",
      " [-186.37129 -197.32657 -181.44118 -210.92366 -189.35402 -265.1115\n",
      "  -253.60745 -253.62009 -252.86382]\n",
      " [-196.44803 -207.55899 -190.92847 -221.41977 -198.96425 -278.942\n",
      "  -266.92392 -266.5286  -266.35855]\n",
      " [-170.25702 -179.87573 -165.53682 -192.1108  -172.5789  -241.92688\n",
      "  -231.39818 -231.08876 -230.8726 ]\n",
      " [-163.3356  -173.2878  -159.1675  -185.41681 -166.38754 -232.6259\n",
      "  -222.60397 -222.86578 -221.69174]]\n",
      "J tensor([-247.7626, -181.4412, -190.9285, -165.5368, -159.1675])\n",
      "P tensor([-345.5876, -252.8638, -266.3586, -230.8726, -221.6917])\n",
      "expected_state_action_values  tensor([[-255.6822, -343.7247],\n",
      "        [-191.8798, -256.1602],\n",
      "        [-197.5850, -265.4720],\n",
      "        [-172.3850, -231.1872],\n",
      "        [-165.4641, -221.7359]])\n",
      "1\n",
      "reward -49.15241722445189\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.2506,  6.4395,  6.7159,  4.9156,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  9.6592,  7.9782, 14.8337,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 5.7065,  4.7183,  6.5877, 26.1786,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  3.2197,  3.3192, 31.4592,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395,  7.8801,  9.5065,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 5.7065,  6.4395,  6.2006, 11.8789,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395, 10.0508, 16.0874,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 6.6855,  6.4395,  6.1674, 31.5733,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 9.9864,  6.4395,  2.4993, 25.2874,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [11.4130,  6.4395,  6.4915,  7.7724,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-25.3215],\n",
      "        [-43.0309],\n",
      "        [-45.1912],\n",
      "        [-49.9846],\n",
      "        [-38.6657]])\n",
      "action_batch tensor([[2, 6],\n",
      "        [2, 7],\n",
      "        [4, 6],\n",
      "        [2, 8],\n",
      "        [2, 6]])\n",
      "policy_net1(state_batch) tensor([[-195.9657, -217.4835, -208.4525, -232.7673, -219.7305, -292.5522,\n",
      "         -286.9337, -271.7238, -280.1819],\n",
      "        [-257.0800, -285.2422, -273.4865, -305.3395, -288.2157, -383.7075,\n",
      "         -376.4025, -356.4788, -367.5947],\n",
      "        [-250.4894, -280.1387, -268.1361, -301.4997, -283.8540, -376.1031,\n",
      "         -368.8308, -350.9591, -359.2997],\n",
      "        [-239.9801, -267.8573, -256.5903, -288.3351, -271.5131, -360.0535,\n",
      "         -353.0466, -335.6626, -344.0513],\n",
      "        [-237.9917, -262.9846, -252.6028, -281.2943, -265.7949, -354.5115,\n",
      "         -347.7519, -328.6306, -339.8443]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-208.4525, -286.9337],\n",
      "        [-273.4865, -356.4788],\n",
      "        [-283.8540, -368.8308],\n",
      "        [-256.5903, -344.0513],\n",
      "        [-252.6028, -347.7519]], grad_fn=<GatherBackward>)\n",
      "A [[-173.65872 -191.93736 -184.12222 -204.7776  -193.61159 -258.4504\n",
      "  -253.52354 -239.4423  -247.81596]\n",
      " [-243.74763 -270.20218 -259.2806  -289.36176 -273.14087 -363.80887\n",
      "  -356.72275 -337.7176  -348.4892 ]\n",
      " [-222.9917  -249.22487 -238.48033 -267.8978  -252.342   -334.52863\n",
      "  -328.14822 -312.04858 -319.66138]\n",
      " [-258.51727 -289.34378 -276.8353  -311.76385 -293.36713 -388.41873\n",
      "  -380.98404 -362.7775  -370.9814 ]\n",
      " [-269.21347 -297.56354 -285.80405 -318.33563 -300.75647 -401.062\n",
      "  -393.43347 -371.88657 -384.51025]]\n",
      "J tensor([-173.6587, -243.7476, -222.9917, -258.5173, -269.2135])\n",
      "P tensor([-239.4423, -337.7176, -312.0486, -362.7775, -371.8866])\n",
      "expected_state_action_values  tensor([[-181.6143, -240.8196],\n",
      "        [-262.4037, -346.9767],\n",
      "        [-245.8837, -326.0349],\n",
      "        [-282.6501, -376.4843],\n",
      "        [-280.9578, -373.3636]])\n",
      "1\n",
      "reward -55.36872263287702\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 6.6855,  6.4395,  6.1674, 31.5733,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [11.4130,  6.4395, 16.4276, 11.6791,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197, 11.7271, 17.0151,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 7.1332,  3.2197,  3.4794, 36.3310,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  1.7268,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 14.2415,  9.7098,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  9.5578, 14.4722,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch tensor([[-52.8656],\n",
      "        [ -4.6880],\n",
      "        [-47.9592],\n",
      "        [-39.6684],\n",
      "        [-10.7610]])\n",
      "action_batch tensor([[4, 7],\n",
      "        [4, 6],\n",
      "        [0, 6],\n",
      "        [3, 7],\n",
      "        [1, 6]])\n",
      "policy_net1(state_batch) tensor([[-228.5921, -256.2471, -254.2437, -276.9641, -248.4790, -344.1438,\n",
      "         -328.2658, -312.0689, -363.3276],\n",
      "        [ -47.5813,  -52.8766,  -52.0674,  -56.0406,  -50.6593,  -70.7987,\n",
      "          -67.7412,  -63.7323,  -74.7209],\n",
      "        [-316.1126, -348.2433, -346.9740, -372.2413, -335.8338, -469.9137,\n",
      "         -448.5721, -422.0267, -497.9484],\n",
      "        [-196.9913, -218.4201, -217.0362, -233.7532, -210.6740, -293.8568,\n",
      "         -280.5948, -264.7599, -310.9459],\n",
      "        [ -85.5420,  -94.1621,  -93.6133, -100.2192,  -90.6230, -127.0048,\n",
      "         -121.3157, -113.8277, -134.2545]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[-248.4790, -312.0689],\n",
      "        [ -50.6593,  -67.7412],\n",
      "        [-316.1126, -448.5721],\n",
      "        [-233.7532, -264.7599],\n",
      "        [ -94.1621, -121.3157]], grad_fn=<GatherBackward>)\n",
      "A [[-238.55928  -266.25583  -264.5119   -286.92957  -257.80865  -358.03302\n",
      "  -341.51184  -323.78278  -378.25677 ]\n",
      " [ -55.202877  -61.2271    -60.38374   -64.87174   -58.656956  -82.041756\n",
      "   -78.49304   -73.80016   -86.67373 ]\n",
      " [-320.74628  -353.46652  -352.14896  -377.64566  -340.78223  -476.78604\n",
      "  -455.20856  -428.25363  -505.1874  ]\n",
      " [-235.73389  -261.25775  -259.69894  -279.6532   -252.02542  -351.57388\n",
      "  -335.70093  -316.7435   -372.1362  ]\n",
      " [ -86.01849   -94.684135  -94.13321  -100.771355  -91.12299  -127.707726\n",
      "  -121.98788  -114.45711  -135.0018  ]]\n",
      "J tensor([-238.5593,  -55.2029, -320.7463, -235.7339,  -86.0185])\n",
      "P tensor([-323.7828,  -73.8002, -428.2536, -316.7435, -114.4571])\n",
      "expected_state_action_values  tensor([[-267.5690, -344.2701],\n",
      "        [ -54.3706,  -71.1081],\n",
      "        [-336.6308, -433.3874],\n",
      "        [-251.8289, -324.7375],\n",
      "        [ -88.1776, -113.7724]])\n",
      "5\n",
      "reward -56.10804689910301\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 2.8533,  3.2197,  2.6880,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  2.3823,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395, 12.2458,  9.1805,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  4.7294,  3.1707,  4.7834,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  6.4395,  3.8170, 16.3308,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "state_batch tensor([[ 2.8533,  3.2197,  2.6279,  0.0000,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 2.8533,  3.2197,  2.6880,  0.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 8.5598,  9.6592,  9.7966,  8.5990,  0.0000,  1.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 4.2799,  3.2197,  3.7932,  6.0371,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  4.5685, 16.2850,  0.0000,  0.0000,  1.0000,  0.0000,\n",
      "          1.0000,  0.0000]])\n",
      "reward_batch tensor([[-10.7610],\n",
      "        [-11.8820],\n",
      "        [-36.9989],\n",
      "        [-18.9634],\n",
      "        [-37.1470]])\n",
      "action_batch tensor([[1, 6],\n",
      "        [1, 6],\n",
      "        [0, 7],\n",
      "        [4, 6],\n",
      "        [4, 6]])\n",
      "policy_net1(state_batch) tensor([[ -93.0330,  -94.1855,  -97.2316, -109.6712,  -98.6230, -131.9371,\n",
      "         -122.9442, -121.9179, -139.1449],\n",
      "        [ -98.2020,  -99.5102, -102.6425, -115.9398, -104.1765, -139.3197,\n",
      "         -129.7810, -128.8279, -147.0119],\n",
      "        [-265.0785, -268.4907, -277.8458, -314.0168, -281.7666, -376.4260,\n",
      "         -350.3828, -348.3988, -397.6774],\n",
      "        [-145.2961, -147.7970, -152.5052, -172.8784, -155.0583, -206.7121,\n",
      "         -192.5385, -191.7769, -218.0933],\n",
      "        [-203.4547, -207.7086, -214.3537, -244.1007, -218.4733, -290.3789,\n",
      "         -270.3392, -270.2051, -306.2145]], grad_fn=<AddmmBackward>)\n",
      "state_action_values tensor([[ -94.1855, -122.9442],\n",
      "        [ -99.5102, -129.7810],\n",
      "        [-265.0785, -348.3988],\n",
      "        [-155.0583, -192.5385],\n",
      "        [-218.4733, -270.3392]], grad_fn=<GatherBackward>)\n",
      "A [[ -93.55215   -94.708855  -97.772804 -110.277     -99.16844  -132.66895\n",
      "  -123.62697  -122.593605 -139.921   ]\n",
      " [-110.87495  -112.28496  -115.92875  -131.01682  -117.67519  -157.33746\n",
      "  -146.53961  -145.5062   -166.05893 ]\n",
      " [-265.03137  -268.70227  -277.78485  -313.9864   -281.75992  -376.28674\n",
      "  -350.41064  -348.52197  -397.62943 ]\n",
      " [-139.42183  -141.6281   -146.2449   -165.59325  -148.5826   -198.25348\n",
      "  -184.61461  -183.72061  -209.15048 ]\n",
      " [-220.25099  -224.46469  -231.94374  -263.85687  -236.2054   -314.2208\n",
      "  -292.41193  -292.03476  -331.3688  ]]\n",
      "J tensor([ -93.5521, -110.8749, -265.0314, -139.4218, -220.2510])\n",
      "P tensor([-122.5936, -145.5062, -348.5220, -183.7206, -292.0348])\n",
      "expected_state_action_values  tensor([[ -94.9579, -121.0952],\n",
      "        [-111.6694, -142.8375],\n",
      "        [-275.5271, -350.6686],\n",
      "        [-144.4430, -184.3119],\n",
      "        [-235.3729, -299.9783]])\n",
      "reward -49.55452894917043\n",
      "non_final_mask tensor([[True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True],\n",
      "        [True, True]])\n",
      "non_final_next_states tensor([[ 5.0690,  6.4395,  6.5522, 21.2992,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  6.4395,  5.1372,  8.5990,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2197,  6.0075,  8.4625,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [14.2663,  3.2197, 13.5898,  7.8644,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 5.7065,  3.2197,  9.3189, 16.0605,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "state_batch tensor([[ 4.3431,  6.4395,  7.1075, 17.9512,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 6.2107,  6.4395,  6.5373,  6.9140,  0.0000,  1.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [ 7.1332,  3.2766,  5.8229,  9.7098,  1.0000,  0.0000,  0.0000,  0.0000,\n",
      "          1.0000,  0.0000],\n",
      "        [12.8397,  6.4395, 10.1104,  9.0302,  1.0000,  0.0000,  0.0000,  1.0000,\n",
      "          0.0000,  0.0000],\n",
      "        [ 6.8490,  6.4395,  8.2194, 10.9147,  0.0000,  0.0000,  1.0000,  1.0000,\n",
      "          0.0000,  0.0000]])\n",
      "reward_batch "
     ]
    }
   ],
   "source": [
    "# async def run():\n",
    "for episode in range(100):\n",
    "    rewards = 0\n",
    "    count = 0\n",
    "    reset()\n",
    "    traci.simulationStep()\n",
    "    previous_phase = [traci.trafficlight.getPhase(id[key]) for key in ['virtual','KasemRat']]\n",
    "#     print(previous_phase)\n",
    "    for seconds in range(100): \n",
    "#         traci.simulationStep()\n",
    "#             if seconds >= 300: ??\n",
    "#         if seconds%time_step == 0:\n",
    "#             if done(count) >= 50:\n",
    "#                 print('done')\n",
    "#                 break\n",
    "        current_phase = [traci.trafficlight.getPhase(id[key]) for key in ['virtual','KasemRat']]\n",
    "#             print('current_phase', current_phase)\n",
    "        current_state1 = get_state()\n",
    "#             print('current_state1', current_state1)\n",
    "        junction, action = get_action(current_state1)\n",
    "#         print('junction', junction)\n",
    "#         print('action', action)\n",
    "        reward, next_state1 = take_action(action, junction) \n",
    "        if previous_state1 is not None:\n",
    "            memory1.push(torch.tensor([current_state1]), torch.tensor([[junction, action]]), torch.tensor([next_state1]), torch.tensor([[reward]]))\n",
    "        optimize_model1()\n",
    "        previous_state1 = current_state1\n",
    "        rewards += reward\n",
    "    reward_memory.append(rewards)\n",
    "    plot_durations()\n",
    "\n",
    "    if episode % TARGET_UPDATE == 0:\n",
    "        target_net1.load_state_dict(policy_net1.state_dict())\n",
    "    traci.close()\n",
    "\n",
    "print('Complete')\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
